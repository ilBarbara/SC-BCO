{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1521,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1522,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1523,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1821,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home2/wsdm/gyy/sjh_project/pku-lab/pytorch-a2c-ppo-acktr-gail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home2/wsdm/yy/workplace/rllab',\n",
       " '/home2/wsdm/anaconda3/envs/mbpo/lib/python36.zip',\n",
       " '/home2/wsdm/anaconda3/envs/mbpo/lib/python3.6',\n",
       " '/home2/wsdm/anaconda3/envs/mbpo/lib/python3.6/lib-dynload',\n",
       " '',\n",
       " '/home2/wsdm/.local/lib/python3.6/site-packages',\n",
       " '/home2/wsdm/anaconda3/envs/mbpo/lib/python3.6/site-packages',\n",
       " '/home2/wsdm/gyy/sjh_project/pku-lab/mbpo/viskit',\n",
       " '/home2/wsdm/gyy/sjh_project/pku-lab/mbpo',\n",
       " '/home2/wsdm/gyy/sjh_project/pku-lab/baselines',\n",
       " '/home2/wsdm/anaconda3/envs/mbpo/lib/python3.6/site-packages/IPython/extensions',\n",
       " '/home2/wsdm/.ipython',\n",
       " 'a2c_ppo_acktr',\n",
       " 'a2c_ppo_acktr',\n",
       " 'a2c_ppo_acktr',\n",
       " '/home2/wsdm/gyy/sjh_project/pku-lab/a2c_ppo_acktr',\n",
       " '/home2/wsdm/gyy/sjh_project/pku-lab/a2c_ppo_acktr',\n",
       " '/home2/wsdm/gyy/sjh_project/pku-lab/pytorch-a2c-ppo-acktr-gail/a2c_ppo_acktr',\n",
       " '/home2/wsdm/gyy/sjh_project/pku-lab/pytorch-a2c-ppo-acktr-gail',\n",
       " '/home2/wsdm/gyy/sjh_project/pku-lab/pytorch-a2c-ppo-acktr-gail',\n",
       " '/home2/wsdm/gyy/sjh_project/pku-lab/pytorch-a2c-ppo-acktr-gail']"
      ]
     },
     "execution_count": 1526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2219,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_names = ['HalfCheetah-v2', 'Hopper-v2', 'Ant-v2', 'Reacher-v2']\n",
    "env_id = 0\n",
    "env_name = env_names[env_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/wsdm/anaconda3/envs/mbpo/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17,) (6,)\n",
      "17 6\n",
      "Ep 1: -328.07\n",
      "Ep 2: -241.17\n",
      "Ep 3: -201.04\n",
      "Ep 4: -178.75\n",
      "Ep 5: -312.14\n",
      "Ep 6: -282.04\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(env_name)\n",
    "print(env.observation_space.shape, env.action_space.shape)\n",
    "dX = env.observation_space.shape[0]\n",
    "dU = env.action_space.shape[0]\n",
    "print(dX, dU)\n",
    "\n",
    "# env.reset()\n",
    "# for _ in range(1000):\n",
    "#    env.render()\n",
    "#    env.step(env.action_space.sample()) # take a random action\n",
    "\n",
    "for i in range(6):\n",
    "    env.reset()\n",
    "    rew = 0\n",
    "    \n",
    "    while True:\n",
    "        _, r, done, _ = env.step(env.action_space.sample())\n",
    "        \n",
    "        rew += r\n",
    "        \n",
    "        if done==True:\n",
    "            print('Ep %d: %.2f' % (i+1, rew))\n",
    "            \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 1000\n",
      "[-0.05514779  0.01413077  0.05431647 -0.07718844 -0.00049597 -0.09359713\n",
      "  0.0420625   0.0338486  -0.04637919 -0.07595596  0.0203414   0.03111221\n",
      "  0.09077952 -0.21187306  0.09768369  0.06454459 -0.02387658]\n",
      "[ 4.6757808e-01  1.4283210e-03  6.5947473e-01  1.8217321e-01\n",
      "  1.4451610e+00 -5.1731670e-01]\n",
      "[-0.07202693 -0.01772098  0.15221099 -0.03565283  0.23121198 -0.01841621\n",
      "  0.33170059 -0.1662426   0.49294502 -0.50951452 -0.75886748  2.57497647\n",
      "  1.62760945  5.13046556  2.27734184  7.41693052 -5.22591411]\n",
      "[ 0.54515547  0.33950102  0.29747805  0.6972709   1.1975483  -0.50355154]\n",
      "[-0.11176169 -0.08448181  0.31225425  0.17700579  0.2379468   0.2849727\n",
      "  0.44529777 -0.45882151  0.72204616 -1.03220343 -1.31037863  3.22701093\n",
      "  4.05066736 -1.43649638  7.04318367  1.28284355 -3.77430341]\n",
      "[ 0.8011819   0.23011546  0.5274805   1.0458543   1.4034928  -0.64960414]\n",
      "[-0.1772751  -0.12200063  0.46834402  0.21548037  0.24988779  0.66666613\n",
      "  0.49098657 -0.48806973  0.62554882 -1.45787828 -0.33301604  2.0607669\n",
      " -0.8075364   1.00980744  6.80945023  1.34424167  1.43217154]\n",
      "[ 0.8073275   0.15338953  0.5052876   1.289279    1.6756067  -0.8328984 ]\n",
      "MjSimState(time=0.0, qpos=array([ 0.0665112 , -0.05514779,  0.01413077,  0.05431647, -0.07718844,\n",
      "       -0.00049597, -0.09359713,  0.0420625 ,  0.0338486 ]), qvel=array([-0.04637919, -0.07595596,  0.0203414 ,  0.03111221,  0.09077952,\n",
      "       -0.21187306,  0.09768369,  0.06454459, -0.02387658]), act=None, udd_state={})\n",
      "MjSimState(time=0.0, qpos=array([ 0.05409377,  0.09952785,  0.0057446 ,  0.0850555 ,  0.04826949,\n",
      "       -0.08728959, -0.04303616,  0.04039512,  0.01200828]), qvel=array([-0.07563419, -0.12442083,  0.00247144,  0.03528212,  0.0364408 ,\n",
      "       -0.0101428 , -0.20868451, -0.08987289, -0.07427616]), act=None, udd_state={})\n",
      "number of demo observations:  100000\n",
      "number of demo actions:  100000\n",
      "mean of demo observations:  [-5.02106020e-01  3.29980820e+00  1.72823195e-01 -1.88362994e-01\n",
      "  1.81453094e-01  4.51563638e-02  6.17097223e-02  5.30601372e-02\n",
      "  2.05173531e+00  6.57970809e-02  1.68806817e-02  1.78584147e-01\n",
      "  9.65510415e-04 -4.47575959e-02  2.66880975e-01 -1.33557539e-01\n",
      " -2.45116294e-02]\n",
      "std of demo observations:  [ 0.07621656  0.3628234   0.3926324   0.49064338  0.22567464  0.62442118\n",
      "  0.4013012   0.20958786  1.00389716  0.87679126  1.48988729  7.87042007\n",
      "  9.68387957  6.17618375 11.21141343  9.19465053  5.50954252]\n",
      "mean of demo actions:  [ 0.369078   -0.29493484  0.28046194  0.08302145 -0.07246893  0.03800287]\n",
      "std of demo actions:  [0.5636468  0.8660695  0.35193706 0.78787154 0.5578178  0.300753  ]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "trajs_demo, demo_st_info, demo_rew_info = pickle.load(open('Demo/my_demo_'+env_name+'.pkl', 'rb'))\n",
    "print(len(trajs_demo), len(trajs_demo[0]))\n",
    "# print(trajs_demo[0][0][0].dtype)\n",
    "print(trajs_demo[0][0][0])\n",
    "print(trajs_demo[0][0][1])\n",
    "print(trajs_demo[0][1][0])\n",
    "print(trajs_demo[0][1][1])\n",
    "print(trajs_demo[0][2][0])\n",
    "print(trajs_demo[0][2][1])\n",
    "print(trajs_demo[0][3][0])\n",
    "print(trajs_demo[0][3][1])\n",
    "print(demo_st_info[0][0])\n",
    "print(demo_st_info[1][0])\n",
    "\n",
    "demo_observations = []\n",
    "demo_actions = []\n",
    "for traj in trajs_demo:\n",
    "    for dat in traj:\n",
    "        obs, act, new_obs, _ = dat\n",
    "        demo_observations.append(obs)\n",
    "        demo_actions.append(act)\n",
    "        \n",
    "print('number of demo observations: ', len(demo_observations))\n",
    "print('number of demo actions: ', len(demo_actions))    \n",
    "\n",
    "demo_O_mean = np.mean(demo_observations, axis=0)\n",
    "demo_O_std = np.std(demo_observations, axis=0)\n",
    "demo_A_mean = np.mean(demo_actions, axis=0)\n",
    "demo_A_std = np.std(demo_actions, axis=0)\n",
    "print('mean of demo observations: ',demo_O_mean)\n",
    "print('std of demo observations: ', demo_O_std)\n",
    "print('mean of demo actions: ',demo_A_mean)\n",
    "print('std of demo actions: ', demo_A_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def O_normalizing(x):\n",
    "    a = x - demo_O_mean\n",
    "    b = demo_O_std\n",
    "    return np.divide(a, b, out=np.zeros_like(a), where=b!=0)\n",
    "def A_normalizing(x):\n",
    "    a = x - demo_A_mean\n",
    "    b = demo_A_std\n",
    "    return np.divide(a, b, out=np.zeros_like(a), where=b!=0)\n",
    "def O_recovering(x):\n",
    "    return np.nan_to_num(x * demo_O_std + demo_O_mean)\n",
    "def A_recovering(x):\n",
    "    return np.nan_to_num(x * demo_A_std + demo_A_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of demo observations after norm:  [ 3.37679340e-14 -1.09075972e-13 -3.42399442e-15 -2.29559038e-15\n",
      " -6.01547223e-15 -9.72555370e-17 -3.19091420e-15 -4.28386215e-16\n",
      " -2.69595035e-14  5.42956791e-16 -6.20193740e-17 -1.95872207e-16\n",
      " -1.85667454e-17  2.67730282e-17 -3.14201443e-16  7.09970971e-17\n",
      " -1.04518616e-16]\n",
      "std of demo observations after norm:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "mean of demo actions after norm:  [ 1.2536609e-06  4.3133409e-06 -1.3447824e-06  1.4005958e-07\n",
      "  3.1584801e-08 -1.3115019e-07]\n",
      "std of demo actions after norm:  [0.9999979 1.0000002 1.0000023 1.0000002 1.000008  0.9999979]\n"
     ]
    }
   ],
   "source": [
    "demo_observations_norm = []\n",
    "demo_actions_norm = []\n",
    "for o in demo_observations:\n",
    "    demo_observations_norm.append(O_normalizing(o))\n",
    "for a in demo_actions:\n",
    "    demo_actions_norm.append(A_normalizing(a))\n",
    "    \n",
    "demo_O_norm_mean = np.mean(demo_observations_norm, axis=0)\n",
    "demo_O_norm_std = np.std(demo_observations_norm, axis=0)\n",
    "demo_A_norm_mean = np.mean(demo_actions_norm, axis=0)\n",
    "demo_A_norm_std = np.std(demo_actions_norm, axis=0)\n",
    "print('mean of demo observations after norm: ',demo_O_norm_mean)\n",
    "print('std of demo observations after norm: ', demo_O_norm_std)\n",
    "print('mean of demo actions after norm: ',demo_A_norm_mean)\n",
    "print('std of demo actions after norm: ', demo_A_norm_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 5.86431900e+00, -9.05585876e+00, -3.01826134e-01,  2.26589325e-01,\n",
      "       -8.06245074e-01, -2.22211375e-01, -4.89587978e-02, -9.16634164e-02,\n",
      "       -2.08996955e+00, -1.61672505e-01,  2.32280342e-03, -1.87374925e-02,\n",
      "        9.27458995e-03, -2.70580454e-02, -1.50915217e-02,  2.15453684e-02,\n",
      "        1.15263660e-04]), array([ 5.64285616, -9.14364736, -0.05249746,  0.31124472,  0.22048949,\n",
      "       -0.1018104 ,  0.67278857, -1.04635227, -1.55273901, -0.65615573,\n",
      "       -0.52067574,  0.30448087,  0.16797441,  0.83793219,  0.17932269,\n",
      "        0.82118271, -0.94407157]), array([ 5.12151586, -9.327651  ,  0.35511856,  0.74467282,  0.25033256,\n",
      "        0.38406181,  0.9558607 , -2.44232486, -1.32452725, -1.25229409,\n",
      "       -0.89084545,  0.38732708,  0.41819003, -0.2253396 ,  0.6044111 ,\n",
      "        0.15404621, -0.68059948])]\n",
      "[array([ 0.17475496,  0.3421933 ,  1.0769335 ,  0.12584761,  2.720655  ,\n",
      "       -1.8464307 ], dtype=float32), array([ 0.3123897 ,  0.73254615,  0.04834988,  0.77963144,  2.2767596 ,\n",
      "       -1.8006617 ], dtype=float32), array([ 0.76662177,  0.606245  ,  0.7018827 ,  1.2220683 ,  2.6459565 ,\n",
      "       -2.2862847 ], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(demo_observations_norm[0:3])\n",
    "print(demo_actions_norm[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2225,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DS_Inv(Dataset):\n",
    "    def __init__(self, trajs):\n",
    "        self.dat = []\n",
    "        \n",
    "        for traj in trajs:\n",
    "            for dat in traj:\n",
    "                obs, act, new_obs, triple_index = dat\n",
    "                \n",
    "                self.dat.append(np.array([O_normalizing(obs), O_normalizing(new_obs), A_normalizing(act), triple_index]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dat)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        obs, new_obs, act, triple_index = self.dat[idx]\n",
    "        \n",
    "        return obs, new_obs, act, triple_index\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.97410303,  1.048857  , -0.23440407,  0.23690283, -1.9266481 ,\n",
      "       -0.8393571 , -0.10077251,  0.25461867,  2.1475408 ,  1.2477115 ,\n",
      "        0.        ], dtype=float32), array([ 0.97435443,  1.04895295, -0.23308636,  0.23601929, -1.92664801,\n",
      "       -0.83935707, -0.03027357,  0.21901877,  2.14756814,  1.24829732,\n",
      "        0.        ]), array([ 0.6625731, -0.3265949], dtype=float32), array([0, 0]))\n",
      "--------------------------------------------------------------------------------\n",
      "[array([-0.05514779,  0.01413077,  0.05431647, -0.07718844, -0.00049597,\n",
      "       -0.09359713,  0.0420625 ,  0.0338486 , -0.04637919, -0.07595596,\n",
      "        0.0203414 ,  0.03111221,  0.09077952, -0.21187306,  0.09768369,\n",
      "        0.06454459, -0.02387658]), array([ 4.6757808e-01,  1.4283210e-03,  6.5947473e-01,  1.8217321e-01,\n",
      "        1.4451610e+00, -5.1731670e-01], dtype=float32), array([-0.07202693, -0.01772098,  0.15221099, -0.03565283,  0.23121198,\n",
      "       -0.01841621,  0.33170059, -0.1662426 ,  0.49294502, -0.50951452,\n",
      "       -0.75886748,  2.57497647,  1.62760945,  5.13046556,  2.27734184,\n",
      "        7.41693052, -5.22591411]), array([0, 0])]\n"
     ]
    }
   ],
   "source": [
    "print(demo_DS_Inv.__getitem__(0))\n",
    "print(\"-\" * 80)\n",
    "print(trajs_demo[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BCO(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step1, generate inverse samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2259,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 100000\n",
    "trajs_inv = []\n",
    "inv_st_info = []\n",
    "inv_rew_info = []\n",
    "cnt = 0\n",
    "epn = 0\n",
    "rews = 0\n",
    "\n",
    "while True:\n",
    "    traj = []\n",
    "    traj_st = []\n",
    "    traj_rew = []\n",
    "    rew = 0\n",
    "\n",
    "    obs = env.reset()\n",
    "    st = env.sim.get_state()\n",
    "    traj_st.append(st)\n",
    "    while True:\n",
    "        # act = env.action_space.sample()\n",
    "        act = np.random.normal(demo_A_mean, demo_A_std, dU).astype(np.float32)\n",
    "        # act = np.zeros(2)\n",
    "        \n",
    "        new_obs, r, done, _ = env.step(act)\n",
    "\n",
    "        traj.append([obs, act, new_obs])\n",
    "        traj_rew.append(r)\n",
    "        \n",
    "        obs = new_obs\n",
    "        rew += r\n",
    "        cnt += 1\n",
    "        st = env.sim.get_state()\n",
    "        traj_st.append(st)\n",
    "\n",
    "        if done==True:\n",
    "            rews += rew\n",
    "            trajs_inv.append(traj)\n",
    "            inv_st_info.append(traj_st)\n",
    "            inv_rew_info.append(traj_rew)\n",
    "            epn += 1\n",
    "            break\n",
    "\n",
    "    if cnt >= M:\n",
    "        break\n",
    "\n",
    "rews /= epn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 1000\n",
      "average reward of 100 random trajs: -330.44\n"
     ]
    }
   ],
   "source": [
    "print(len(trajs_inv), len(trajs_inv[0]))\n",
    "print('average reward of %d random trajs: %.2f' % (epn, rews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2261,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_dev = 100000\n",
    "trajs_inv_dev = []\n",
    "inv_dev_st_info = []\n",
    "inv_dev_rew_info = []\n",
    "cnt = 0\n",
    "epn = 0\n",
    "rews = 0\n",
    "\n",
    "while True:\n",
    "    traj = []\n",
    "    traj_st = []\n",
    "    traj_rew = []\n",
    "    rew = 0\n",
    "\n",
    "    obs = env.reset()\n",
    "    st = env.sim.get_state()\n",
    "    traj_st.append(st)\n",
    "    while True:\n",
    "        # act = env.action_space.sample()\n",
    "        act = np.random.normal(demo_A_mean, demo_A_std, dU).astype(np.float32)\n",
    "        # act = np.zeros(2)\n",
    "        \n",
    "        new_obs, r, done, _ = env.step(act)\n",
    "\n",
    "        traj.append([obs, act, new_obs])\n",
    "        traj_rew.append(r)\n",
    "        \n",
    "        obs = new_obs\n",
    "        rew += r\n",
    "        cnt += 1\n",
    "        st = env.sim.get_state()\n",
    "        traj_st.append(st)\n",
    "\n",
    "        if done==True:\n",
    "            rews += rew\n",
    "            trajs_inv_dev.append(traj)\n",
    "            inv_dev_st_info.append(traj_st)\n",
    "            inv_dev_rew_info.append(traj_rew)\n",
    "            epn += 1\n",
    "            break\n",
    "\n",
    "    if cnt >= M_dev:\n",
    "        break\n",
    "\n",
    "rews /= epn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 1000\n",
      "average reward of 100 random trajs: -347.27\n"
     ]
    }
   ],
   "source": [
    "print(len(trajs_inv_dev), len(trajs_inv_dev[0]))\n",
    "print('average reward of %d random trajs: %.2f' % (epn, rews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add inv triple index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2263,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_index = 0\n",
    "for traj in trajs_inv:\n",
    "    triple_index = 0\n",
    "    for triple in traj:\n",
    "        triple.append(np.array([traj_index, triple_index]))\n",
    "        triple_index += 1\n",
    "    traj_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2264,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1001\n",
      "1000\n",
      "[array([ 0.02271028,  0.02434279,  0.06393868,  0.08352046,  0.00723075,\n",
      "        0.04267395, -0.00691647,  0.07282949, -0.12695392, -0.05766077,\n",
      "       -0.01388619,  0.08162828, -0.07372188, -0.12594663,  0.01230115,\n",
      "       -0.0222567 ,  0.11633374]), array([ 0.48530665,  0.49718362, -0.6969199 , -0.01073017, -0.40378693,\n",
      "       -0.619034  ], dtype=float32), array([-2.28832822e-03,  1.18772089e-02,  1.59714321e-01,  1.79076991e-01,\n",
      "       -2.90564410e-01,  6.52260377e-02, -1.05005232e-01, -1.21612482e-01,\n",
      "       -4.44108354e-02, -7.48339369e-01, -3.19780203e-01,  2.59860536e+00,\n",
      "        2.51627380e+00, -6.74905722e+00,  2.16517048e-01, -2.57997897e+00,\n",
      "       -5.50991448e+00]), array([0, 0])]\n",
      "[ 0.02271028  0.02434279  0.06393868  0.08352046  0.00723075  0.04267395\n",
      " -0.00691647  0.07282949 -0.12695392 -0.05766077 -0.01388619  0.08162828\n",
      " -0.07372188 -0.12594663  0.01230115 -0.0222567   0.11633374]\n",
      "[ 0.48530665  0.49718362 -0.6969199  -0.01073017 -0.40378693 -0.619034  ]\n",
      "[-2.28832822e-03  1.18772089e-02  1.59714321e-01  1.79076991e-01\n",
      " -2.90564410e-01  6.52260377e-02 -1.05005232e-01 -1.21612482e-01\n",
      " -4.44108354e-02 -7.48339369e-01 -3.19780203e-01  2.59860536e+00\n",
      "  2.51627380e+00 -6.74905722e+00  2.16517048e-01 -2.57997897e+00\n",
      " -5.50991448e+00]\n",
      "[-0.05082729 -1.9801918   0.20610875 -0.23023407 -0.22035667  0.04911779]\n",
      "[ -0.02563433   0.07640288   0.16996644  -0.26241667  -0.06274857\n",
      "  -0.11423438  -0.17038691  -0.19262031  -0.50395474  -0.5182592\n",
      "   1.77035518  -1.16832227 -11.68184353   8.49486948  -4.91341701\n",
      "  -0.62522275   1.00066324]\n",
      "[ 0.77422434  1.0606307   0.69214696  0.0974275   0.6084902  -0.2924789 ]\n",
      "MjSimState(time=0.0, qpos=array([-0.09539053,  0.02271028,  0.02434279,  0.06393868,  0.08352046,\n",
      "        0.00723075,  0.04267395, -0.00691647,  0.07282949]), qvel=array([-0.12695392, -0.05766077, -0.01388619,  0.08162828, -0.07372188,\n",
      "       -0.12594663,  0.01230115, -0.0222567 ,  0.11633374]), act=None, udd_state={})\n",
      "MjSimState(time=0.05, qpos=array([-0.09796078, -0.00228833,  0.01187721,  0.15971432,  0.17907699,\n",
      "       -0.29056441,  0.06522604, -0.10500523, -0.12161248]), qvel=array([-0.04441084, -0.74833937, -0.3197802 ,  2.59860536,  2.5162738 ,\n",
      "       -6.74905722,  0.21651705, -2.57997897, -5.50991448]), act=None, udd_state={})\n"
     ]
    }
   ],
   "source": [
    "print(len(trajs_inv[0]))\n",
    "print(len(inv_st_info[0]))\n",
    "print(len(inv_rew_info[0]))\n",
    "print(trajs_inv[0][0])\n",
    "print(trajs_inv[0][0][0])\n",
    "print(trajs_inv[0][0][1])\n",
    "print(trajs_inv[0][1][0])\n",
    "print(trajs_inv[0][1][1])\n",
    "print(trajs_inv[0][2][0])\n",
    "print(trajs_inv[0][2][1])\n",
    "print(inv_st_info[0][0])\n",
    "print(inv_st_info[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2265,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_index = 0\n",
    "for traj in trajs_inv_dev:\n",
    "    triple_index = 0\n",
    "    for triple in traj:\n",
    "        triple.append(np.array([traj_index, triple_index]))\n",
    "        triple_index += 1\n",
    "    traj_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2266,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1001\n",
      "1000\n",
      "[array([-0.09168263, -0.06981019, -0.02401561, -0.06010482,  0.09466317,\n",
      "        0.08499827, -0.09488915, -0.06999623, -0.08160894,  0.05106071,\n",
      "       -0.02579562, -0.09508993, -0.09688674, -0.00534476,  0.01905766,\n",
      "        0.03989724,  0.11229962]), array([ 0.35008454,  0.28010136, -0.2377199 ,  0.3578608 ,  0.50038534,\n",
      "       -0.15050298], dtype=float32), array([-0.078975  , -0.05222383,  0.05284531,  0.1230475 , -0.18982119,\n",
      "        0.12659877,  0.1144691 , -0.09827096,  0.55986574,  0.08387347,\n",
      "        0.19847602,  3.29805509,  2.8055943 , -3.83540746,  1.55129628,\n",
      "        5.53045415, -0.6005152 ]), array([0, 0])]\n",
      "[-0.09168263 -0.06981019 -0.02401561 -0.06010482  0.09466317  0.08499827\n",
      " -0.09488915 -0.06999623 -0.08160894  0.05106071 -0.02579562 -0.09508993\n",
      " -0.09688674 -0.00534476  0.01905766  0.03989724  0.11229962]\n",
      "[ 0.35008454  0.28010136 -0.2377199   0.3578608   0.50038534 -0.15050298]\n",
      "[-0.078975   -0.05222383  0.05284531  0.1230475  -0.18982119  0.12659877\n",
      "  0.1144691  -0.09827096  0.55986574  0.08387347  0.19847602  3.29805509\n",
      "  2.8055943  -3.83540746  1.55129628  5.53045415 -0.6005152 ]\n",
      "[ 0.6016146  -1.3428711   0.09508178 -0.12487136 -1.2910564   0.18104656]\n",
      "[ -0.07364229   0.04443956   0.32091423  -0.34690463  -0.06243561\n",
      "   0.15499513  -0.13438287   0.05261732   0.27675322  -0.07920661\n",
      "   2.40155567   4.63519946 -11.81910535   4.87667628  -0.74598397\n",
      "  -9.00486571   4.24220413]\n",
      "[ 0.5839424  -0.9820968   0.7704732   0.60955733 -0.22003002  0.15197349]\n",
      "MjSimState(time=0.0, qpos=array([-0.03699528, -0.09168263, -0.06981019, -0.02401561, -0.06010482,\n",
      "        0.09466317,  0.08499827, -0.09488915, -0.06999623]), qvel=array([-0.08160894,  0.05106071, -0.02579562, -0.09508993, -0.09688674,\n",
      "       -0.00534476,  0.01905766,  0.03989724,  0.11229962]), act=None, udd_state={})\n",
      "MjSimState(time=0.05, qpos=array([-0.01841432, -0.078975  , -0.05222383,  0.05284531,  0.1230475 ,\n",
      "       -0.18982119,  0.12659877,  0.1144691 , -0.09827096]), qvel=array([ 0.55986574,  0.08387347,  0.19847602,  3.29805509,  2.8055943 ,\n",
      "       -3.83540746,  1.55129628,  5.53045415, -0.6005152 ]), act=None, udd_state={})\n"
     ]
    }
   ],
   "source": [
    "print(len(trajs_inv_dev[0]))\n",
    "print(len(inv_dev_st_info[0]))\n",
    "print(len(inv_dev_rew_info[0]))\n",
    "print(trajs_inv_dev[0][0])\n",
    "print(trajs_inv_dev[0][0][0])\n",
    "print(trajs_inv_dev[0][0][1])\n",
    "print(trajs_inv_dev[0][1][0])\n",
    "print(trajs_inv_dev[0][1][1])\n",
    "print(trajs_inv_dev[0][2][0])\n",
    "print(trajs_inv_dev[0][2][1])\n",
    "print(inv_dev_st_info[0][0])\n",
    "print(inv_dev_st_info[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file_path = 'Inv/my_inv_'+env_name+'.pkl'\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(([trajs_inv, inv_st_info, inv_rew_info],\n",
    "                 [trajs_inv_dev, inv_dev_st_info, inv_dev_rew_info]), f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2268,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of inv observations:  100000\n",
      "number of inv actions:  100000\n",
      "mean of inv observations:  [-0.16819512  0.79516542  0.17869008 -0.07947992  0.14438009  0.09622169\n",
      " -0.03868449  0.00987078 -0.08204005 -0.01197039  0.02296281  0.04140622\n",
      " -0.06264299  0.01067214  0.00817421  0.04321798  0.01346708]\n",
      "std of inv observations:  [0.19892647 1.26831144 0.2324723  0.30311331 0.17707735 0.3549618\n",
      " 0.29982709 0.22895165 0.70318806 0.77609273 1.57779143 5.11761557\n",
      " 7.49479838 4.61795603 6.88494372 6.50357883 4.30980408]\n",
      "mean of inv actions:  [ 0.36644524 -0.29348433  0.28174198  0.08369995 -0.07630549  0.03846281]\n",
      "std of inv actions:  [0.563836   0.86418724 0.3511869  0.78683025 0.5584134  0.30187386]\n"
     ]
    }
   ],
   "source": [
    "inv_observations = []\n",
    "inv_actions = []\n",
    "for traj in trajs_inv:\n",
    "    for dat in traj:\n",
    "        obs, act, new_obs, _ = dat\n",
    "        inv_observations.append(obs)\n",
    "        inv_actions.append(act)\n",
    "        \n",
    "print('number of inv observations: ', len(inv_observations))\n",
    "print('number of inv actions: ', len(inv_actions))    \n",
    "\n",
    "inv_O_mean = np.mean(inv_observations, axis=0)\n",
    "inv_O_std = np.std(inv_observations, axis=0)\n",
    "inv_A_mean = np.mean(inv_actions, axis=0)\n",
    "inv_A_std = np.std(inv_actions, axis=0)\n",
    "print('mean of inv observations: ', inv_O_mean)\n",
    "print('std of inv observations: ', inv_O_std)\n",
    "print('mean of inv actions: ', inv_A_mean)\n",
    "print('std of inv actions: ', inv_A_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2269,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of inv observations after norm:  [ 4.38108062e+00 -6.90320086e+00  1.49424430e-02  2.21918981e-01\n",
      " -1.64276354e-01  8.17802660e-02 -2.50171710e-01 -2.06068013e-01\n",
      " -2.12549197e+00 -8.86955377e-02  4.08227128e-03 -1.74295565e-02\n",
      " -6.56849401e-03  8.97475433e-03 -2.30753032e-02  1.92259091e-02\n",
      "  6.89326095e-03]\n",
      "std of inv observations after norm:  [2.6100164  3.49567161 0.59208638 0.61778743 0.78465772 0.56846535\n",
      " 0.74713729 1.0923898  0.70045826 0.88515108 1.05900054 0.65023411\n",
      " 0.77394585 0.7477038  0.61410131 0.70732202 0.78224355]\n",
      "mean of inv actions after norm:  [-0.00466377  0.00167504  0.00363762  0.00086128 -0.00687796  0.00152986]\n",
      "std of inv actions after norm:  [1.000332   0.997826   0.99787396 0.9986778  1.0010633  1.0037211 ]\n"
     ]
    }
   ],
   "source": [
    "inv_observations_norm = []\n",
    "inv_actions_norm = []\n",
    "for o in inv_observations:\n",
    "    inv_observations_norm.append(O_normalizing(o))\n",
    "for a in inv_actions:\n",
    "    inv_actions_norm.append(A_normalizing(a))\n",
    "    \n",
    "inv_O_norm_mean = np.mean(inv_observations_norm, axis=0)\n",
    "inv_O_norm_std = np.std(inv_observations_norm, axis=0)\n",
    "inv_A_norm_mean = np.mean(inv_actions_norm, axis=0)\n",
    "inv_A_norm_std = np.std(inv_actions_norm, axis=0)\n",
    "print('mean of inv observations after norm: ', inv_O_norm_mean)\n",
    "print('std of inv observations after norm: ', inv_O_norm_std)\n",
    "print('mean of inv actions after norm: ', inv_A_norm_mean)\n",
    "print('std of inv actions after norm: ', inv_A_norm_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step2, update inverse model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_get_obs(env_id):\n",
    "    if env_id == 3:\n",
    "        theta = env.sim.data.qpos.flat[:2]\n",
    "        return np.concatenate([\n",
    "            np.cos(theta),\n",
    "            np.sin(theta),\n",
    "            env.sim.data.qpos.flat[2:],\n",
    "            env.sim.data.qvel.flat[:2],\n",
    "            env.get_body_com(\"fingertip\") - env.get_body_com(\"target\")\n",
    "        ])\n",
    "    elif env_id == 0:\n",
    "        return np.concatenate([\n",
    "            env.sim.data.qpos.flat[1:],\n",
    "            env.sim.data.qvel.flat,\n",
    "        ])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2272,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = DataLoader(DS_Inv(trajs_inv), batch_size=256, shuffle=True)\n",
    "dev_data_gen = DataLoader(DS_Inv(trajs_inv_dev), batch_size=256, shuffle=True)\n",
    "test_data_gen = DataLoader(DS_Inv(trajs_demo), batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 391, 'dev': 391, 'test': 391}\n"
     ]
    }
   ],
   "source": [
    "dataset_sizes = {'train':len(train_data_gen), 'dev':len(dev_data_gen), \n",
    "                 'test':len(test_data_gen)}\n",
    "dataloaders = {'train':train_data_gen, 'dev':dev_data_gen, 'test':test_data_gen}\n",
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 17, 180])\n",
      "MjSimState(time=8.999999999999853, qpos=array([-1.93118024, -0.15860556,  0.09668648,  0.61294191,  0.31699695,\n",
      "        0.16171375, -0.23308764, -0.1732204 , -0.3378226 ]), qvel=array([ 0.07562121, -1.93842807, -1.53974795,  3.30830927, 18.06069465,\n",
      "        2.74182434,  3.50788337,  7.05688625, -3.34793429]), act=None, udd_state={})\n",
      "[-0.15860556  0.09668648  0.61294191  0.31699695  0.16171375 -0.23308764\n",
      " -0.1732204  -0.3378226   0.07562121 -1.93842807 -1.53974795  3.30830927\n",
      " 18.06069465  2.74182434  3.50788337  7.05688625 -3.34793429]\n",
      "[-0.15860556  0.09668648  0.61294191  0.31699695  0.16171375 -0.23308764\n",
      " -0.1732204  -0.3378226   0.07562121 -1.93842807 -1.53974795  3.30830927\n",
      " 18.06069465  2.74182434  3.50788337  7.05688625 -3.34793429]\n",
      "[-0.19967196  0.05572193  0.66267934  0.39859089  0.1829092   0.06819554\n",
      " -0.24756763 -0.56730938  0.58581825 -0.14137396 -0.65225956 -2.03738195\n",
      " -6.37012727 -1.35872427  6.34430757 -4.37517032  1.68508579]\n",
      "[-0.19967196  0.05572193  0.66267934  0.39859089  0.18290919  0.06819554\n",
      " -0.24756763 -0.56730938  0.58581825 -0.14137396 -0.65225956 -2.03738194\n",
      " -6.37012727 -1.3587243   6.34430757 -4.37517031  1.68508576]\n"
     ]
    }
   ],
   "source": [
    "for obs, next_obs, action, triple_index in train_data_gen:\n",
    "    cur_st = inv_st_info[triple_index[0][0]][triple_index[0][1]]\n",
    "    print(triple_index[0])\n",
    "    print(cur_st)\n",
    "    env.sim.set_state(cur_st) #restore\n",
    "    obs_ = sim_get_obs(env_id)\n",
    "    print(O_recovering(obs[0].numpy()))\n",
    "    print(obs_)\n",
    "    ground_next_obs, reward, done, _ = env.step(A_recovering(action[0].numpy()))\n",
    "    print(O_recovering(next_obs[0].numpy()))\n",
    "    print(ground_next_obs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7.3025, -8.4203, -0.3004,  ..., -0.9322, -0.8540, -0.0905],\n",
      "        [ 4.5904, -8.7478, -0.0792,  ...,  0.3125,  1.1957,  0.4449],\n",
      "        [ 7.5394, -6.8294,  0.8961,  ...,  0.0670, -0.4125,  0.6124],\n",
      "        ...,\n",
      "        [ 5.3505, -8.9744, -0.7692,  ..., -0.9817, -0.0587,  0.9655],\n",
      "        [ 3.4057, -9.0920,  0.5162,  ...,  0.6751,  0.2334,  0.3427],\n",
      "        [ 5.1580, -8.7586, -0.2147,  ..., -0.0325, -0.1127, -0.6162]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[ 7.4594, -8.5365, -0.3220,  ...,  0.3496,  0.3098, -1.8508],\n",
      "        [ 5.1751, -8.9813, -0.1673,  ..., -0.6288,  0.7793,  0.4759],\n",
      "        [ 8.1487, -6.7205, -0.5194,  ..., -0.3324, -0.0969,  0.9340],\n",
      "        ...,\n",
      "        [ 5.4358, -8.9920, -0.6969,  ..., -0.0932, -0.0721,  0.7575],\n",
      "        [ 4.1010, -9.2967,  0.7952,  ..., -0.9323,  0.8843,  1.5991],\n",
      "        [ 5.5323, -8.7014, -0.5805,  ..., -0.1792,  0.4645, -0.2746]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[ 0.6334, -0.0144,  0.1471,  0.2484,  1.1881,  0.1587],\n",
      "        [ 0.0804,  0.7803,  0.6030, -0.4042, -0.1635, -0.5177],\n",
      "        [-1.6753,  0.7565,  2.5601, -0.5637,  0.2632,  0.4830],\n",
      "        ...,\n",
      "        [ 0.0197,  0.3488, -0.6005, -0.9780,  0.5631,  1.1453],\n",
      "        [ 0.8222,  0.1992,  0.2748, -1.1292,  0.3661,  0.9720],\n",
      "        [-0.4381, -0.0044,  0.2629,  0.5624,  0.8519, -0.5886]])\n",
      "tensor([[  4, 301],\n",
      "        [  6, 717],\n",
      "        [ 82, 269],\n",
      "        [ 87, 686],\n",
      "        [ 73, 399],\n",
      "        [ 49, 874],\n",
      "        [ 76, 790],\n",
      "        [ 93, 454],\n",
      "        [ 51, 900],\n",
      "        [ 96, 151],\n",
      "        [ 31, 309],\n",
      "        [ 42, 407],\n",
      "        [ 77, 764],\n",
      "        [ 92, 403],\n",
      "        [ 82,  49],\n",
      "        [ 99, 442],\n",
      "        [ 53, 312],\n",
      "        [ 41, 117],\n",
      "        [ 22, 689],\n",
      "        [ 25, 133],\n",
      "        [ 12, 884],\n",
      "        [ 55,  99],\n",
      "        [ 36, 450],\n",
      "        [ 51, 761],\n",
      "        [ 19, 914],\n",
      "        [ 72, 416],\n",
      "        [ 79, 301],\n",
      "        [ 62, 950],\n",
      "        [ 50, 729],\n",
      "        [ 30, 116],\n",
      "        [ 13, 862],\n",
      "        [ 73, 273],\n",
      "        [ 75,  47],\n",
      "        [ 10, 413],\n",
      "        [ 84, 733],\n",
      "        [ 21, 773],\n",
      "        [ 78, 753],\n",
      "        [  1,  10],\n",
      "        [ 88, 686],\n",
      "        [ 26, 346],\n",
      "        [ 54,  62],\n",
      "        [ 71, 318],\n",
      "        [ 29,  85],\n",
      "        [ 64, 585],\n",
      "        [ 15, 349],\n",
      "        [ 64,  35],\n",
      "        [ 83, 278],\n",
      "        [ 28,  63],\n",
      "        [ 70, 537],\n",
      "        [ 50, 876],\n",
      "        [ 12, 245],\n",
      "        [ 17, 472],\n",
      "        [ 70, 903],\n",
      "        [ 18, 352],\n",
      "        [  3, 843],\n",
      "        [ 27,  42],\n",
      "        [ 80, 740],\n",
      "        [ 65, 898],\n",
      "        [ 16, 170],\n",
      "        [ 49, 527],\n",
      "        [  2,  65],\n",
      "        [ 82, 539],\n",
      "        [ 17, 543],\n",
      "        [ 12, 199],\n",
      "        [ 90, 209],\n",
      "        [ 78, 647],\n",
      "        [ 51,  25],\n",
      "        [ 19, 781],\n",
      "        [ 27, 892],\n",
      "        [  0, 215],\n",
      "        [  8, 466],\n",
      "        [ 17, 938],\n",
      "        [ 59, 194],\n",
      "        [  0, 720],\n",
      "        [ 42, 184],\n",
      "        [  6, 231],\n",
      "        [ 73, 985],\n",
      "        [ 13, 839],\n",
      "        [ 17, 447],\n",
      "        [ 72, 360],\n",
      "        [ 46, 902],\n",
      "        [ 39, 929],\n",
      "        [ 92, 560],\n",
      "        [ 53, 768],\n",
      "        [ 28, 434],\n",
      "        [ 23, 890],\n",
      "        [ 84, 121],\n",
      "        [  1, 661],\n",
      "        [ 67, 497],\n",
      "        [ 43, 713],\n",
      "        [ 39, 460],\n",
      "        [ 82, 266],\n",
      "        [  5, 200],\n",
      "        [ 98, 900],\n",
      "        [ 93,  20],\n",
      "        [  7, 705],\n",
      "        [ 91, 237],\n",
      "        [ 46,  87],\n",
      "        [ 21, 392],\n",
      "        [ 90, 760],\n",
      "        [ 71, 272],\n",
      "        [ 68, 683],\n",
      "        [  0, 496],\n",
      "        [ 81, 508],\n",
      "        [ 80, 537],\n",
      "        [ 31, 176],\n",
      "        [ 16, 840],\n",
      "        [ 77, 897],\n",
      "        [  4, 872],\n",
      "        [ 16, 551],\n",
      "        [ 53, 337],\n",
      "        [ 99, 950],\n",
      "        [ 99, 385],\n",
      "        [ 26, 474],\n",
      "        [ 98,  24],\n",
      "        [ 48, 712],\n",
      "        [ 22, 341],\n",
      "        [ 45, 747],\n",
      "        [ 75,  94],\n",
      "        [  7, 675],\n",
      "        [ 30, 539],\n",
      "        [ 56, 827],\n",
      "        [ 58, 194],\n",
      "        [ 40, 455],\n",
      "        [ 50, 820],\n",
      "        [ 48, 644],\n",
      "        [ 87, 439],\n",
      "        [ 35, 193],\n",
      "        [ 77, 213],\n",
      "        [ 32, 641],\n",
      "        [ 62, 198],\n",
      "        [ 36, 135],\n",
      "        [ 97, 184],\n",
      "        [ 52, 242],\n",
      "        [  0, 973],\n",
      "        [ 49,  72],\n",
      "        [ 13, 545],\n",
      "        [ 86, 541],\n",
      "        [ 68, 171],\n",
      "        [ 93, 848],\n",
      "        [ 80,  44],\n",
      "        [ 77,  49],\n",
      "        [ 42, 598],\n",
      "        [ 12, 366],\n",
      "        [ 67, 860],\n",
      "        [ 83, 141],\n",
      "        [ 86, 801],\n",
      "        [ 38, 862],\n",
      "        [  0, 843],\n",
      "        [ 50, 585],\n",
      "        [ 60, 569],\n",
      "        [ 78, 476],\n",
      "        [ 16, 200],\n",
      "        [ 76, 458],\n",
      "        [ 54, 469],\n",
      "        [ 28, 479],\n",
      "        [ 79,  37],\n",
      "        [ 66, 392],\n",
      "        [ 63, 153],\n",
      "        [  4, 491],\n",
      "        [ 41, 737],\n",
      "        [ 41, 442],\n",
      "        [ 25, 471],\n",
      "        [ 90, 777],\n",
      "        [ 19, 239],\n",
      "        [ 99, 334],\n",
      "        [ 35, 138],\n",
      "        [ 66, 448],\n",
      "        [ 89,  25],\n",
      "        [ 48, 534],\n",
      "        [ 44, 497],\n",
      "        [ 27, 882],\n",
      "        [  2, 600],\n",
      "        [ 20, 989],\n",
      "        [ 32, 750],\n",
      "        [ 36, 176],\n",
      "        [  4, 150],\n",
      "        [  4, 983],\n",
      "        [ 98, 221],\n",
      "        [ 66, 526],\n",
      "        [ 43, 764],\n",
      "        [ 15, 969],\n",
      "        [ 78, 525],\n",
      "        [ 52, 652],\n",
      "        [ 14, 131],\n",
      "        [ 90, 799],\n",
      "        [ 48, 241],\n",
      "        [ 21, 909],\n",
      "        [ 31, 720],\n",
      "        [ 29, 569],\n",
      "        [ 68,  82],\n",
      "        [ 35, 758],\n",
      "        [  8, 408],\n",
      "        [ 24,  12],\n",
      "        [ 50, 771],\n",
      "        [ 29, 809],\n",
      "        [ 40, 960],\n",
      "        [ 40,  31],\n",
      "        [  7, 140],\n",
      "        [ 69, 877],\n",
      "        [ 99, 433],\n",
      "        [ 55, 976],\n",
      "        [ 60, 793],\n",
      "        [ 91, 611],\n",
      "        [ 96, 429],\n",
      "        [ 50, 850],\n",
      "        [ 15, 494],\n",
      "        [ 58, 955],\n",
      "        [ 52, 850],\n",
      "        [ 98, 682],\n",
      "        [ 23, 309],\n",
      "        [ 79, 784],\n",
      "        [ 55, 694],\n",
      "        [ 47, 379],\n",
      "        [ 31, 310],\n",
      "        [ 60, 397],\n",
      "        [ 81, 104],\n",
      "        [ 34, 224],\n",
      "        [ 67, 471],\n",
      "        [ 75, 294],\n",
      "        [ 60, 475],\n",
      "        [ 92, 136],\n",
      "        [  0, 372],\n",
      "        [ 45, 242],\n",
      "        [  6, 182],\n",
      "        [ 86, 665],\n",
      "        [  3,  32],\n",
      "        [ 66, 989],\n",
      "        [ 85, 525],\n",
      "        [ 95,  45],\n",
      "        [ 13, 611],\n",
      "        [ 99, 164],\n",
      "        [ 45, 846],\n",
      "        [ 86, 470],\n",
      "        [ 66, 174],\n",
      "        [ 30, 731],\n",
      "        [ 46, 737],\n",
      "        [ 90, 598],\n",
      "        [ 26, 929],\n",
      "        [ 22, 722],\n",
      "        [ 60, 945],\n",
      "        [  5, 289],\n",
      "        [  6, 773],\n",
      "        [ 44, 625],\n",
      "        [ 92, 918],\n",
      "        [ 84,  58],\n",
      "        [ 33, 166],\n",
      "        [ 66, 244],\n",
      "        [ 66, 494],\n",
      "        [ 74, 770],\n",
      "        [ 50, 552],\n",
      "        [ 99, 701],\n",
      "        [ 51, 263],\n",
      "        [ 39, 568],\n",
      "        [ 90, 889],\n",
      "        [ 43, 202]])\n"
     ]
    }
   ],
   "source": [
    "for obs, next_obs, action, triple_index in train_data_gen:\n",
    "    print(obs)\n",
    "    print(next_obs)\n",
    "    print(action)\n",
    "    print(triple_index)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2300,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCO_inv(nn.Module):\n",
    "    def __init__(self, env, policy='mlp'):\n",
    "        super(BCO_inv, self).__init__()\n",
    "        \n",
    "        self.policy = policy\n",
    "        self.act_n = env.action_space.shape[0]\n",
    "        \n",
    "        if self.policy=='mlp':\n",
    "            self.obs_n = env.observation_space.shape[0]\n",
    "            self.inv = nn.Sequential(*[nn.Linear(self.obs_n*2, 256), nn.LeakyReLU(), \n",
    "                                       nn.Linear(256, 256), nn.LeakyReLU(), \n",
    "                                       nn.Linear(256, 256), nn.LeakyReLU(), \n",
    "                                       nn.Linear(256, self.act_n)])\n",
    "        \n",
    "        elif self.policy=='cnn':\n",
    "            pass\n",
    "        \n",
    "        self.train()\n",
    "    \n",
    "    def pred_inv(self, obs1, obs2):\n",
    "        obs = T.cat([obs1, obs2], dim=1)\n",
    "        out = self.inv(obs)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2301,
   "metadata": {},
   "outputs": [],
   "source": [
    "POLICY = 'mlp'\n",
    "model_inv = BCO_inv(env, policy=POLICY).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCO_inv(\n",
       "  (inv): Sequential(\n",
       "    (0): Linear(in_features=34, out_features=256, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): Linear(in_features=256, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2303,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func_inv = nn.L1Loss().cuda()\n",
    "optim_inv = T.optim.Adam(model_inv.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_func_inv = nn.MSELoss(reduction='sum').cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check behavior of nn.MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5000)\n"
     ]
    }
   ],
   "source": [
    "a=np.array([[1,2],[3,4],[3,4]])\n",
    "b=np.array([[2,3],[4,4],[3,4]])\n",
    "\n",
    "input_ = T.autograd.Variable(T.from_numpy(a))\n",
    "target_ = T.autograd.Variable(T.from_numpy(b))\n",
    "\n",
    "loss = loss_func_inv(input_.float(), target_.float())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2306,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train_inv_model(model, loss_func, optim, num_epochs=3): \n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_loss = 1000\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'dev', 'test']:\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for obs1, obs2, act, _ in dataloaders[phase]:\n",
    "                # forward\n",
    "                out = model.pred_inv(obs1.float().cuda(), obs2.float().cuda())\n",
    "                ls_bh = loss_func(out, act.cuda())\n",
    "                optim.zero_grad()\n",
    "                \n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    ls_bh.backward()\n",
    "                    optim.step()\n",
    "\n",
    "                ls_bh = ls_bh.cpu().detach().numpy()\n",
    "                running_loss += ls_bh\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f}'.format(\n",
    "                phase, epoch_loss))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'test' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_epoch = epoch\n",
    "                best_model_wts = model.state_dict()\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best test loss: ', best_epoch)\n",
    "    print('Best test loss: {:.4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2307,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 0.3025\n",
      "dev Loss: 0.2283\n",
      "test Loss: 0.2998\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 0.2081\n",
      "dev Loss: 0.1892\n",
      "test Loss: 0.2471\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 0.1818\n",
      "dev Loss: 0.1753\n",
      "test Loss: 0.2253\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 0.1676\n",
      "dev Loss: 0.1655\n",
      "test Loss: 0.2306\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.1590\n",
      "dev Loss: 0.1622\n",
      "test Loss: 0.2133\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.1511\n",
      "dev Loss: 0.1526\n",
      "test Loss: 0.1994\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 0.1431\n",
      "dev Loss: 0.1409\n",
      "test Loss: 0.2060\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 0.1383\n",
      "dev Loss: 0.1386\n",
      "test Loss: 0.2012\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 0.1360\n",
      "dev Loss: 0.1367\n",
      "test Loss: 0.2027\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 0.1321\n",
      "dev Loss: 0.1308\n",
      "test Loss: 0.1942\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 0.1288\n",
      "dev Loss: 0.1396\n",
      "test Loss: 0.1857\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.1271\n",
      "dev Loss: 0.1303\n",
      "test Loss: 0.1926\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.1241\n",
      "dev Loss: 0.1236\n",
      "test Loss: 0.1825\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.1232\n",
      "dev Loss: 0.1278\n",
      "test Loss: 0.1890\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.1208\n",
      "dev Loss: 0.1220\n",
      "test Loss: 0.1828\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.1199\n",
      "dev Loss: 0.1229\n",
      "test Loss: 0.1797\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.1184\n",
      "dev Loss: 0.1208\n",
      "test Loss: 0.1901\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.1174\n",
      "dev Loss: 0.1224\n",
      "test Loss: 0.2004\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.1157\n",
      "dev Loss: 0.1244\n",
      "test Loss: 0.1923\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.1142\n",
      "dev Loss: 0.1170\n",
      "test Loss: 0.1863\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 0.1139\n",
      "dev Loss: 0.1259\n",
      "test Loss: 0.1819\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.1123\n",
      "dev Loss: 0.1185\n",
      "test Loss: 0.1862\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 0.1109\n",
      "dev Loss: 0.1138\n",
      "test Loss: 0.1994\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 0.1101\n",
      "dev Loss: 0.1161\n",
      "test Loss: 0.1815\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 0.1092\n",
      "dev Loss: 0.1156\n",
      "test Loss: 0.2035\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 0.1087\n",
      "dev Loss: 0.1189\n",
      "test Loss: 0.1749\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.1084\n",
      "dev Loss: 0.1124\n",
      "test Loss: 0.1716\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.1078\n",
      "dev Loss: 0.1201\n",
      "test Loss: 0.1794\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 0.1060\n",
      "dev Loss: 0.1127\n",
      "test Loss: 0.1844\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.1055\n",
      "dev Loss: 0.1158\n",
      "test Loss: 0.1935\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 0.1051\n",
      "dev Loss: 0.1142\n",
      "test Loss: 0.1804\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 0.1041\n",
      "dev Loss: 0.1132\n",
      "test Loss: 0.1952\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.1041\n",
      "dev Loss: 0.1125\n",
      "test Loss: 0.1927\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.1033\n",
      "dev Loss: 0.1139\n",
      "test Loss: 0.1783\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.1028\n",
      "dev Loss: 0.1077\n",
      "test Loss: 0.1901\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.1028\n",
      "dev Loss: 0.1117\n",
      "test Loss: 0.1945\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.1021\n",
      "dev Loss: 0.1129\n",
      "test Loss: 0.2137\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.1004\n",
      "dev Loss: 0.1089\n",
      "test Loss: 0.1986\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.1009\n",
      "dev Loss: 0.1107\n",
      "test Loss: 0.1710\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.1005\n",
      "dev Loss: 0.1120\n",
      "test Loss: 0.2029\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 0.0999\n",
      "dev Loss: 0.1103\n",
      "test Loss: 0.1863\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.0991\n",
      "dev Loss: 0.1101\n",
      "test Loss: 0.1888\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.0988\n",
      "dev Loss: 0.1066\n",
      "test Loss: 0.1813\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.0981\n",
      "dev Loss: 0.1121\n",
      "test Loss: 0.1945\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.0981\n",
      "dev Loss: 0.1088\n",
      "test Loss: 0.1922\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.0973\n",
      "dev Loss: 0.1105\n",
      "test Loss: 0.1878\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.0970\n",
      "dev Loss: 0.1068\n",
      "test Loss: 0.1831\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.0968\n",
      "dev Loss: 0.1038\n",
      "test Loss: 0.1777\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.0969\n",
      "dev Loss: 0.1073\n",
      "test Loss: 0.1897\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.0956\n",
      "dev Loss: 0.1062\n",
      "test Loss: 0.2020\n",
      "\n",
      "Training complete in 5m 3s\n",
      "Best test loss: 0.1710\n"
     ]
    }
   ],
   "source": [
    "model_inv = train_inv_model(model_inv, loss_func_inv, optim_inv, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: evaluate the error of inverse dynamic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2308,
   "metadata": {},
   "outputs": [],
   "source": [
    "triple_set = {'train':[], 'dev':[], 'test':[]}\n",
    "\n",
    "for phase in ['train', 'dev', 'test']:\n",
    "    for obs1, obs2, _, triple_index in dataloaders[phase]:\n",
    "        out = model_inv.pred_inv(obs1.float().cuda(), obs2.float().cuda())\n",
    "\n",
    "        obs = obs1.cpu().detach().numpy()\n",
    "        out = out.cpu().detach().numpy()\n",
    "        next_obs = obs2.cpu().detach().numpy()\n",
    "        triple_index = triple_index.numpy()\n",
    "\n",
    "        for i in range(len(obs)):\n",
    "            triple_set[phase].append([obs[i], out[i], next_obs[i], triple_index[i]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2309,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 4.19044744e+00, -8.85814156e+00,  5.68758523e-03,  5.33792137e-01,\n",
      "       -3.64424488e-01, -7.21330612e-01, -1.70011908e+00,  4.70382307e-01,\n",
      "       -2.83932304e+00, -1.34430652e+00,  2.35415947e+00, -9.39758234e-01,\n",
      "        1.45284896e-01,  1.48974599e-01, -1.76787727e-01, -5.94859349e-01,\n",
      "       -3.82208463e-01]), array([ 0.70756334, -1.1016753 , -1.5107692 , -1.6248261 , -0.3258037 ,\n",
      "        1.1263305 ], dtype=float32), array([ 3.72834609, -8.31435903,  0.15072288, -0.30157758, -1.08221433,\n",
      "       -1.14193538, -1.46746739, -0.20343765, -2.67699952, -0.6760081 ,\n",
      "        2.28115635,  0.45220973, -1.11924831, -0.71184952, -0.61224915,\n",
      "       -0.27660858, -1.24325089]), array([ 15, 877])]\n"
     ]
    }
   ],
   "source": [
    "print(triple_set['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### interact with mujoco simulator to get the ground next state (observation) $ground \\_ next \\_ obs = step(s, a) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2310,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_info = {'train':inv_st_info, 'dev':inv_dev_st_info, 'test':demo_st_info}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2311,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 15 877]\n",
      "MjSimState(time=43.849999999999845, qpos=array([-8.85627738, -0.18272453,  0.0858672 ,  0.17505633,  0.07353858,\n",
      "        0.09921173, -0.40525775, -0.62055011,  0.15164656]), qvel=array([-0.79865303, -1.11287913,  3.52431294, -7.21770791,  1.40788695,\n",
      "        0.8753369 , -1.71515932, -5.60308137, -2.13030541]), act=None, udd_state={})\n",
      "[-0.18272453  0.0858672   0.17505633  0.07353858  0.09921173 -0.40525775\n",
      " -0.62055011  0.15164656 -0.79865303 -1.11287913  3.52431294 -7.21770791\n",
      "  1.40788695  0.8753369  -1.71515932 -5.60308137 -2.13030541]\n",
      "[-0.18272453  0.0858672   0.17505633  0.07353858  0.09921173 -0.40525775\n",
      " -0.62055011  0.15164656 -0.79865303 -1.11287913  3.52431294 -7.21770791\n",
      "  1.40788695  0.8753369  -1.71515932 -5.60308137 -2.13030541]\n",
      "[-2.17944302e-01  2.83164225e-01  2.32001882e-01 -3.36330036e-01\n",
      " -6.27752356e-02 -6.67892278e-01 -5.27186704e-01  1.04220754e-02\n",
      " -6.35696906e-01 -5.26920911e-01  3.41554652e+00  3.73766471e+00\n",
      " -1.08377004e+01 -4.44127106e+00 -6.59729739e+00 -2.67687681e+00\n",
      " -6.87425527e+00]\n",
      "[ -0.21835112   0.28390537   0.23084189  -0.33571615  -0.063604\n",
      "  -0.67065474  -0.52981101   0.0146134   -0.60200158  -0.55491395\n",
      "   3.44815983   3.75596521 -10.82916767  -4.46523247  -6.89999436\n",
      "  -2.39601516  -6.6963978 ]\n"
     ]
    }
   ],
   "source": [
    "for phase in ['train', 'dev', 'test']:\n",
    "    for obs, action, next_obs, triple_index in triple_set[phase]:\n",
    "        cur_st = st_info[phase][triple_index[0]][triple_index[1]]\n",
    "        print(triple_index)\n",
    "        print(cur_st)\n",
    "        env.sim.set_state(cur_st) #restore\n",
    "        obs_ = sim_get_obs(env_id)\n",
    "        print(O_recovering(obs))\n",
    "        print(obs_)\n",
    "        ground_next_obs, reward, done, _ = env.step(A_recovering(action))\n",
    "        print(O_recovering(next_obs))\n",
    "        print(ground_next_obs)\n",
    "        break\n",
    "    break    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38bf805a32c344908e40bd92ad7a4d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=100000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average forward error on training set (100000 steps): 0.0221\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb03237b7f7745669aca311a382051e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=100000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average forward error on deving set (100000 steps): 0.0237\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f78c62e25e9488db959329045f117d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=100000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average forward error on testing set (100000 steps): 0.0600\n"
     ]
    }
   ],
   "source": [
    "for phase in ['train', 'dev', 'test']:\n",
    "    cnt = 0\n",
    "    error_sum = 0.0\n",
    "    for tuple_ in tqdm(triple_set[phase]):\n",
    "        obs, action, next_obs, triple_index = tuple_\n",
    "        \n",
    "        cur_st = st_info[phase][triple_index[0]][triple_index[1]]\n",
    "        env.sim.set_state(cur_st) #restore\n",
    "        \n",
    "        ground_next_obs, reward, done, _ = env.step(A_recovering(action))\n",
    "        \n",
    "        ground_next_obs = O_normalizing(ground_next_obs)\n",
    "        tuple_.append(ground_next_obs)\n",
    "        \n",
    "        aver_error = np.mean(np.fabs(ground_next_obs - next_obs))\n",
    "        tuple_.append(aver_error)\n",
    "        error_sum += aver_error\n",
    "        cnt += 1\n",
    "        \n",
    "    error = error_sum / cnt\n",
    "    print('Average forward error on ' + phase + 'ing set (%d steps): %.4f' % (cnt, error))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2313,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 4.19044744e+00, -8.85814156e+00,  5.68758523e-03,  5.33792137e-01,\n",
      "       -3.64424488e-01, -7.21330612e-01, -1.70011908e+00,  4.70382307e-01,\n",
      "       -2.83932304e+00, -1.34430652e+00,  2.35415947e+00, -9.39758234e-01,\n",
      "        1.45284896e-01,  1.48974599e-01, -1.76787727e-01, -5.94859349e-01,\n",
      "       -3.82208463e-01]), array([ 0.70756334, -1.1016753 , -1.5107692 , -1.6248261 , -0.3258037 ,\n",
      "        1.1263305 ], dtype=float32), array([ 3.72834609, -8.31435903,  0.15072288, -0.30157758, -1.08221433,\n",
      "       -1.14193538, -1.46746739, -0.20343765, -2.67699952, -0.6760081 ,\n",
      "        2.28115635,  0.45220973, -1.11924831, -0.71184952, -0.61224915,\n",
      "       -0.27660858, -1.24325089]), array([ 15, 877]), array([ 3.72300846, -8.31231632,  0.14776848, -0.30032639, -1.0858867 ,\n",
      "       -1.14635942, -1.47400688, -0.1834397 , -2.64343499, -0.70793478,\n",
      "        2.30304613,  0.45453496, -1.11836719, -0.71572917, -0.63924815,\n",
      "       -0.24606238, -1.21096918]), 0.013559626979329957]\n",
      "[array([ 5.69734856, -8.60247565, -0.35603443, -0.3228784 ,  0.17215239,\n",
      "       -1.02884519, -0.13116784,  0.10946097, -3.54607684, -0.18162598,\n",
      "        2.66740269, -1.59747439,  0.18563525,  0.67193844, -0.9571896 ,\n",
      "       -0.46080979, -0.25581274]), array([ 0.08572619, -0.17359027,  0.2719009 ,  0.5954081 ,  0.5326405 ,\n",
      "       -1.2503932 ], dtype=float32), array([ 5.4307874 , -8.38781782, -0.7646532 , -0.22740136,  0.15417488,\n",
      "       -0.61080798, -0.80523338, -1.36943262, -1.31558411, -0.64960888,\n",
      "        0.13856071,  0.16954798,  0.0532774 , -0.30190905,  0.99658735,\n",
      "       -0.73706637, -1.86759887]), array([ 42, 262]), array([ 5.43627758, -8.39252396, -0.75387845, -0.22919498,  0.14769208,\n",
      "       -0.61416751, -0.81392046, -1.38653122, -1.33498729, -0.61943633,\n",
      "        0.07046447,  0.18078349,  0.05566065, -0.30632269,  0.96757069,\n",
      "       -0.83250456, -1.99801672]), 0.02640998651393789]\n"
     ]
    }
   ],
   "source": [
    "print(triple_set['train'][0])\n",
    "print(triple_set['train'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### act_error-obs_error scatter diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2314,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_error_list = {'train':[], 'dev':[], 'test':[]}\n",
    "obs_error_list = {'train':[], 'dev':[], 'test':[]}\n",
    "trajs_dict = {'train':trajs_inv, 'dev':trajs_inv_dev, 'test':trajs_demo}\n",
    "\n",
    "for phase in ['train', 'dev', 'test']:\n",
    "    for tuple_ in triple_set[phase]:\n",
    "        obs, act, next_obs, triple_index, ground_next_obs, next_obs_error = tuple_\n",
    "        ground_act = trajs_dict[phase][triple_index[0]][triple_index[1]][1]\n",
    "        act_error = np.mean(np.fabs(A_normalizing(ground_act) - act))\n",
    "        act_error_list[phase].append(act_error)\n",
    "        obs_error_list[phase].append(next_obs_error)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2315,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1e25bc8780>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "phase = 'train'\n",
    "plt.scatter(act_error_list[phase], obs_error_list[phase], \n",
    "            marker = 'o', color = 'blue', s = 1, label = 'Third')\n",
    "plt.xlabel('act_error')\n",
    "plt.ylabel('obs_error')\n",
    "plt.savefig('imgs/act_error-obs_error['+phase+'].jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1e24301c18>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "phase = 'dev'\n",
    "plt.scatter(act_error_list[phase], obs_error_list[phase], \n",
    "            marker = 'o', color = 'blue', s = 1, label = 'Third')\n",
    "plt.xlabel('act_error')\n",
    "plt.ylabel('obs_error') \n",
    "plt.savefig('imgs/act_error-obs_error['+phase+'].jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1e244fecf8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "phase = 'test'\n",
    "plt.scatter(act_error_list[phase], obs_error_list[phase], \n",
    "            marker = 'o', color = 'blue', s = 1, label = 'Third')\n",
    "plt.xlabel('act_error')\n",
    "plt.ylabel('obs_error') \n",
    "plt.savefig('imgs/act_error-obs_error['+phase+'].jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: evaluate the error between the learned policy and the expert (BC vs BCO vs SC-BCO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2464,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DS_Inv_aug(Dataset):\n",
    "    def __init__(self, trajs, sigma, aug_ratio, only_aug=False):\n",
    "        self.dat = []\n",
    "        mu = 0\n",
    "        \n",
    "        for traj in trajs:\n",
    "            for dat in traj:\n",
    "                obs, act, new_obs, triple_index = dat\n",
    "                if not only_aug:\n",
    "                    self.dat.append(np.array([O_normalizing(obs), O_normalizing(new_obs), \n",
    "                                              A_normalizing(act), triple_index]))\n",
    "                for _ in range(aug_ratio):\n",
    "                    self.dat.append(np.array([O_normalizing(obs) + np.random.normal(mu, sigma, dX).astype(np.float32), \n",
    "                                              O_normalizing(new_obs), A_normalizing(act), np.array([-1, -1])]))\n",
    "                \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dat)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        obs, new_obs, act, triple_index = self.dat[idx]\n",
    "        \n",
    "        return obs, new_obs, act, triple_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2320,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DS_Policy(Dataset):\n",
    "    def __init__(self, traj, norm=False):\n",
    "        self.dat = []\n",
    "        if norm:\n",
    "            for dat in traj:\n",
    "                obs, act, _ = dat\n",
    "                self.dat.append([O_normalizing(obs), A_normalizing(act)])\n",
    "        else:\n",
    "            for dat in traj:\n",
    "                obs, act, _ = dat\n",
    "                self.dat.append([obs, act])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dat)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        obs, act = self.dat[idx]\n",
    "        \n",
    "        return obs, act\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2321,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_demo_train_BC = DataLoader(DS_Inv(trajs_demo[0:10]), batch_size=100, shuffle=False)\n",
    "ld_demo_dev_BC = DataLoader(DS_Inv(trajs_demo[10:20]), batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2322,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_demo_BC = {'train':ld_demo_train_BC, 'dev':ld_demo_dev_BC}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2323,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_policy_BC = {'train':[], 'dev':[]}\n",
    "\n",
    "for phase in ['train', 'dev']:\n",
    "    for obs1, obs2, action, triple_index in ld_demo_BC[phase]:\n",
    "        obs = obs1.cpu().detach().numpy()\n",
    "        act = action.cpu().detach().numpy()\n",
    "        triple_index = triple_index.cpu().detach().numpy()\n",
    "\n",
    "        for i in range(len(obs)):\n",
    "            traj_policy_BC[phase].append([obs[i], act[i], triple_index[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2324,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[array([ 5.86431900e+00, -9.05585876e+00, -3.01826134e-01,  2.26589325e-01,\n",
      "       -8.06245074e-01, -2.22211375e-01, -4.89587978e-02, -9.16634164e-02,\n",
      "       -2.08996955e+00, -1.61672505e-01,  2.32280342e-03, -1.87374925e-02,\n",
      "        9.27458995e-03, -2.70580454e-02, -1.50915217e-02,  2.15453684e-02,\n",
      "        1.15263660e-04]), array([ 0.17475496,  0.3421933 ,  1.0769335 ,  0.12584761,  2.720655  ,\n",
      "       -1.8464307 ], dtype=float32), array([0, 0])], [array([ 5.64285616, -9.14364736, -0.05249746,  0.31124472,  0.22048949,\n",
      "       -0.1018104 ,  0.67278857, -1.04635227, -1.55273901, -0.65615573,\n",
      "       -0.52067574,  0.30448087,  0.16797441,  0.83793219,  0.17932269,\n",
      "        0.82118271, -0.94407157]), array([ 0.3123897 ,  0.73254615,  0.04834988,  0.77963144,  2.2767596 ,\n",
      "       -1.8006617 ], dtype=float32), array([0, 1])], [array([ 5.12151586, -9.327651  ,  0.35511856,  0.74467282,  0.25033256,\n",
      "        0.38406181,  0.9558607 , -2.44232486, -1.32452725, -1.25229409,\n",
      "       -0.89084545,  0.38732708,  0.41819003, -0.2253396 ,  0.6044111 ,\n",
      "        0.15404621, -0.68059948]), array([ 0.76662177,  0.606245  ,  0.7018827 ,  1.2220683 ,  2.6459565 ,\n",
      "       -2.2862847 ], dtype=float32), array([0, 2])], [array([ 4.26194662, -9.43105895,  0.75266541,  0.82308941,  0.30324495,\n",
      "        0.99533741,  1.06971234, -2.58187599, -1.42064999, -1.73778576,\n",
      "       -0.23484778,  0.23914642, -0.08348946,  0.17074703,  0.58356329,\n",
      "        0.1607238 ,  0.26439276]), array([ 0.77752507,  0.51765406,  0.63882345,  1.5310333 ,  3.1337752 ,\n",
      "       -2.8957357 ], dtype=float32), array([0, 3])], [array([ 3.54300545, -9.30917324,  0.78603043,  0.90262501,  0.46612868,\n",
      "        1.07223118,  1.48918031, -2.02368805, -1.59335791, -1.27580075,\n",
      "        0.99989881, -0.06737675, -0.05733189,  0.14832909, -0.05525938,\n",
      "        0.21423762,  0.41363952]), array([ 0.1508316 ,  0.40824783,  0.25980875,  1.1803826 ,  3.236358  ,\n",
      "       -3.385739  ], dtype=float32), array([0, 4])]]\n"
     ]
    }
   ],
   "source": [
    "print(traj_policy_BC['train'][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step4, update policy via demo samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2325,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_policy_BC = {'train':DataLoader(DS_Policy(traj_policy_BC['train']), batch_size=64, shuffle=True),\n",
    "                'dev':DataLoader(DS_Policy(traj_policy_BC['dev']), batch_size=64, shuffle=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 157, 'dev': 157}\n"
     ]
    }
   ],
   "source": [
    "ld_policy_sizes_BC = {'train':len(ld_policy_BC['train']), 'dev':len(ld_policy_BC['dev'])}\n",
    "print(ld_policy_sizes_BC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2332,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "tensor([[ 0.1253,  0.2294, -1.5048,  ..., -0.3445, -1.2159, -0.2437],\n",
      "        [-0.0333,  0.1277,  0.6909,  ...,  0.0828,  0.2440, -1.1319],\n",
      "        [ 0.1124,  0.0225, -0.5524,  ..., -0.9322, -0.2727, -0.3629],\n",
      "        ...,\n",
      "        [ 0.1675,  0.3549, -1.1843,  ..., -0.4569, -0.8607, -0.7126],\n",
      "        [-0.8097,  0.0646,  1.3575,  ...,  2.5070, -0.3504, -0.9397],\n",
      "        [ 0.4274,  0.4608, -0.8500,  ..., -0.0637, -0.1948,  1.3345]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for data in ld_policy_BC['train']:\n",
    "    print(len(data))\n",
    "    print(data[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = nn.Linear(20, 64)\n",
    "# input = T.randn(128, 20)\n",
    "# output = w(input)\n",
    "# print(w.weight)\n",
    "# print(w.bias)\n",
    "\n",
    "# from a2c_ppo_acktr.utils import init\n",
    "# from a2c_ppo_acktr.model import Policy\n",
    "\n",
    "# actor_critic = Policy(env.observation_space.shape, env.action_space)\n",
    "\n",
    "# recurrent_hidden_states = torch.zeros(1, actor_critic.recurrent_hidden_state_size)\n",
    "# masks = torch.zeros(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2333,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BC_policy(nn.Module):\n",
    "    def __init__(self, env, policy='mlp'):\n",
    "        super(BC_policy, self).__init__()\n",
    "        \n",
    "        self.policy = policy\n",
    "        self.act_n = env.action_space.shape[0]\n",
    "        \n",
    "        init_ = lambda m: init(m, nn.init.xavier_normal_, lambda x: nn.init.\n",
    "                               constant_(x, 0), 0.5)\n",
    "        \n",
    "        if self.policy=='mlp':\n",
    "            self.obs_n = env.observation_space.shape[0]\n",
    "            \n",
    "            self.pol = nn.Sequential(*[init_(nn.Linear(self.obs_n, 64)), nn.LeakyReLU(),\n",
    "                                       init_(nn.Linear(64, 64)), nn.LeakyReLU(),\n",
    "                                       init_(nn.Linear(64, self.act_n))])\n",
    "            \n",
    "            # self.pol = nn.Sequential(*[(nn.Linear(self.obs_n, self.act_n))])\n",
    "            '''\n",
    "            self.pol = nn.Sequential(\n",
    "                nn.Linear(self.obs_n, 60),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Linear(60,100),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Linear(100,60),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Linear(60,40),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Linear(40,16),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Linear(16, self.act_n),\n",
    "            )\n",
    "            '''\n",
    "        \n",
    "        elif self.policy=='cnn':\n",
    "            pass\n",
    "        \n",
    "        # self.train()\n",
    "    \n",
    "    def pred_act(self, obs):\n",
    "        out = self.pol(obs)\n",
    "        \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2339,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_option = 'mlp'\n",
    "model_policy_BC = BC_policy(env, policy=NN_option).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BC_policy(\n",
       "  (pol): Sequential(\n",
       "    (0): Linear(in_features=17, out_features=64, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=64, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_policy_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_func_policy = nn.MSELoss().cuda()\n",
    "loss_func_policy = nn.L1Loss().cuda()\n",
    "optim_policy_BC = T.optim.Adam(model_policy_BC.parameters(), lr=1e-3)\n",
    "# optim_policy_BC = T.optim.SGD(model_policy_BC.parameters(), lr=5e-2)\n",
    "# optim_policy_BC = T.optim.Adagrad(model_policy_BC.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2421,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_policy_model(model, loss_func, optim, DS, DS_sizes, phase_list, num_epochs=5, \n",
    "                       best_criterion='dev'): \n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_loss = 1000\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in phase_list:\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            # Iterate over data.\n",
    "            for obs, act in DS[phase]:\n",
    "                # forward\n",
    "                out = model.pred_act(obs.float().cuda())\n",
    "                '''\n",
    "                if phase == 'train':\n",
    "                    print('ground act:', act.numpy()[0])\n",
    "                    print('policy act:', out.cpu().detach().numpy()[0])\n",
    "                '''\n",
    "                ls_bh = loss_func(out, act.cuda())\n",
    "                optim.zero_grad()\n",
    "                \n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    ls_bh.backward()\n",
    "                    optim.step()\n",
    "\n",
    "                ls_bh = ls_bh.cpu().detach().numpy()\n",
    "                running_loss += ls_bh\n",
    "\n",
    "            epoch_loss = running_loss / DS_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == best_criterion and epoch_loss < best_loss:\n",
    "                # if epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_epoch = epoch\n",
    "                best_model_wts = model.state_dict()\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best epoch: ', best_epoch)\n",
    "    print('Best dev loss: {:.4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2343,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 0.3709\n",
      "dev Loss: 0.1876\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 0.1657\n",
      "dev Loss: 0.1447\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 0.1284\n",
      "dev Loss: 0.1167\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 0.1080\n",
      "dev Loss: 0.1019\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 0.0949\n",
      "dev Loss: 0.0897\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 0.0852\n",
      "dev Loss: 0.0866\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 0.0777\n",
      "dev Loss: 0.0773\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 0.0722\n",
      "dev Loss: 0.0719\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 0.0678\n",
      "dev Loss: 0.0695\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.0645\n",
      "dev Loss: 0.0643\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 0.0627\n",
      "dev Loss: 0.0651\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.0600\n",
      "dev Loss: 0.0602\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.0585\n",
      "dev Loss: 0.0582\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.0566\n",
      "dev Loss: 0.0588\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.0548\n",
      "dev Loss: 0.0571\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 0.0538\n",
      "dev Loss: 0.0564\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 0.0528\n",
      "dev Loss: 0.0543\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 0.0514\n",
      "dev Loss: 0.0526\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 0.0502\n",
      "dev Loss: 0.0524\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 0.0492\n",
      "dev Loss: 0.0510\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 0.0480\n",
      "dev Loss: 0.0495\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 0.0476\n",
      "dev Loss: 0.0493\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 0.0472\n",
      "dev Loss: 0.0503\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 0.0465\n",
      "dev Loss: 0.0487\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 0.0460\n",
      "dev Loss: 0.0479\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 0.0456\n",
      "dev Loss: 0.0469\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 0.0450\n",
      "dev Loss: 0.0475\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 0.0441\n",
      "dev Loss: 0.0459\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 0.0439\n",
      "dev Loss: 0.0454\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 0.0437\n",
      "dev Loss: 0.0450\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 0.0431\n",
      "dev Loss: 0.0468\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 0.0424\n",
      "dev Loss: 0.0485\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 0.0426\n",
      "dev Loss: 0.0444\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 0.0417\n",
      "dev Loss: 0.0449\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 0.0408\n",
      "dev Loss: 0.0435\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 0.0410\n",
      "dev Loss: 0.0454\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 0.0407\n",
      "dev Loss: 0.0419\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 0.0407\n",
      "dev Loss: 0.0422\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 0.0399\n",
      "dev Loss: 0.0467\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 0.0396\n",
      "dev Loss: 0.0409\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 0.0402\n",
      "dev Loss: 0.0428\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 0.0395\n",
      "dev Loss: 0.0417\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 0.0392\n",
      "dev Loss: 0.0423\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 0.0387\n",
      "dev Loss: 0.0426\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 0.0387\n",
      "dev Loss: 0.0417\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 0.0382\n",
      "dev Loss: 0.0402\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 0.0385\n",
      "dev Loss: 0.0435\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 0.0384\n",
      "dev Loss: 0.0438\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 0.0374\n",
      "dev Loss: 0.0422\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 0.0376\n",
      "dev Loss: 0.0419\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 0.0372\n",
      "dev Loss: 0.0418\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 0.0370\n",
      "dev Loss: 0.0400\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 0.0369\n",
      "dev Loss: 0.0402\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 0.0366\n",
      "dev Loss: 0.0379\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 0.0370\n",
      "dev Loss: 0.0396\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 0.0364\n",
      "dev Loss: 0.0403\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 0.0362\n",
      "dev Loss: 0.0382\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 0.0366\n",
      "dev Loss: 0.0390\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 0.0358\n",
      "dev Loss: 0.0400\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 0.0356\n",
      "dev Loss: 0.0383\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 0.0355\n",
      "dev Loss: 0.0393\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 0.0355\n",
      "dev Loss: 0.0388\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 0.0356\n",
      "dev Loss: 0.0379\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 0.0349\n",
      "dev Loss: 0.0372\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 0.0350\n",
      "dev Loss: 0.0381\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 0.0351\n",
      "dev Loss: 0.0373\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 0.0347\n",
      "dev Loss: 0.0382\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 0.0346\n",
      "dev Loss: 0.0374\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 0.0344\n",
      "dev Loss: 0.0365\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 0.0344\n",
      "dev Loss: 0.0356\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 0.0343\n",
      "dev Loss: 0.0363\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 0.0339\n",
      "dev Loss: 0.0377\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 0.0338\n",
      "dev Loss: 0.0374\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 0.0336\n",
      "dev Loss: 0.0386\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 0.0340\n",
      "dev Loss: 0.0359\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 0.0334\n",
      "dev Loss: 0.0376\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 0.0340\n",
      "dev Loss: 0.0369\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 0.0332\n",
      "dev Loss: 0.0374\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 0.0330\n",
      "dev Loss: 0.0365\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 0.0332\n",
      "dev Loss: 0.0357\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 0.0332\n",
      "dev Loss: 0.0354\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 0.0327\n",
      "dev Loss: 0.0381\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 0.0327\n",
      "dev Loss: 0.0360\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 0.0333\n",
      "dev Loss: 0.0365\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 0.0330\n",
      "dev Loss: 0.0366\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 0.0326\n",
      "dev Loss: 0.0356\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 0.0326\n",
      "dev Loss: 0.0370\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 0.0324\n",
      "dev Loss: 0.0341\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 0.0321\n",
      "dev Loss: 0.0356\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 0.0318\n",
      "dev Loss: 0.0353\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 0.0317\n",
      "dev Loss: 0.0350\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 0.0323\n",
      "dev Loss: 0.0344\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 0.0325\n",
      "dev Loss: 0.0359\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 0.0318\n",
      "dev Loss: 0.0345\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 0.0320\n",
      "dev Loss: 0.0350\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 0.0315\n",
      "dev Loss: 0.0348\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 0.0316\n",
      "dev Loss: 0.0328\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 0.0314\n",
      "dev Loss: 0.0352\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 0.0318\n",
      "dev Loss: 0.0357\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 0.0312\n",
      "dev Loss: 0.0326\n",
      "\n",
      "Training complete in 1m 6s\n",
      "Best epoch:  99\n",
      "Best dev loss: 0.0326\n"
     ]
    }
   ],
   "source": [
    "phase_list = ['train', 'dev']\n",
    "model_policy_BC = train_policy_model(model_policy_BC, loss_func_policy, \n",
    "                                     optim_policy_BC, ld_policy_BC, \n",
    "                                     ld_policy_sizes_BC, phase_list, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2344,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 0.0268\n",
      "dev Loss: 0.0299\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 0.0265\n",
      "dev Loss: 0.0299\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 0.0265\n",
      "dev Loss: 0.0298\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 0.0264\n",
      "dev Loss: 0.0297\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 0.0266\n",
      "dev Loss: 0.0299\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 0.0264\n",
      "dev Loss: 0.0296\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 0.0264\n",
      "dev Loss: 0.0299\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 0.0263\n",
      "dev Loss: 0.0299\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 0.0263\n",
      "dev Loss: 0.0296\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.0264\n",
      "dev Loss: 0.0296\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 0.0263\n",
      "dev Loss: 0.0296\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.0262\n",
      "dev Loss: 0.0297\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.0262\n",
      "dev Loss: 0.0297\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.0262\n",
      "dev Loss: 0.0295\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.0261\n",
      "dev Loss: 0.0296\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 0.0262\n",
      "dev Loss: 0.0295\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 0.0262\n",
      "dev Loss: 0.0298\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 0.0261\n",
      "dev Loss: 0.0297\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 0.0262\n",
      "dev Loss: 0.0297\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 0.0261\n",
      "dev Loss: 0.0297\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 0.0261\n",
      "dev Loss: 0.0300\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 0.0261\n",
      "dev Loss: 0.0293\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 0.0262\n",
      "dev Loss: 0.0296\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 0.0260\n",
      "dev Loss: 0.0295\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 0.0261\n",
      "dev Loss: 0.0296\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 0.0261\n",
      "dev Loss: 0.0295\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 0.0260\n",
      "dev Loss: 0.0295\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 0.0260\n",
      "dev Loss: 0.0294\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 0.0260\n",
      "dev Loss: 0.0292\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 0.0259\n",
      "dev Loss: 0.0293\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 0.0260\n",
      "dev Loss: 0.0295\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 0.0259\n",
      "dev Loss: 0.0296\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 0.0260\n",
      "dev Loss: 0.0294\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 0.0260\n",
      "dev Loss: 0.0294\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 0.0259\n",
      "dev Loss: 0.0293\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 0.0258\n",
      "dev Loss: 0.0293\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 0.0258\n",
      "dev Loss: 0.0296\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 0.0259\n",
      "dev Loss: 0.0293\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 0.0258\n",
      "dev Loss: 0.0293\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 0.0258\n",
      "dev Loss: 0.0292\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 0.0259\n",
      "dev Loss: 0.0297\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 0.0258\n",
      "dev Loss: 0.0292\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 0.0258\n",
      "dev Loss: 0.0293\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 0.0257\n",
      "dev Loss: 0.0293\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 0.0258\n",
      "dev Loss: 0.0291\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 0.0257\n",
      "dev Loss: 0.0296\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 0.0258\n",
      "dev Loss: 0.0291\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 0.0257\n",
      "dev Loss: 0.0294\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 0.0257\n",
      "dev Loss: 0.0291\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 0.0257\n",
      "dev Loss: 0.0291\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 0.0257\n",
      "dev Loss: 0.0293\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 0.0257\n",
      "dev Loss: 0.0291\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 0.0257\n",
      "dev Loss: 0.0293\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 0.0257\n",
      "dev Loss: 0.0291\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 0.0256\n",
      "dev Loss: 0.0291\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 0.0257\n",
      "dev Loss: 0.0290\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 0.0257\n",
      "dev Loss: 0.0292\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 0.0256\n",
      "dev Loss: 0.0291\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 0.0256\n",
      "dev Loss: 0.0289\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 0.0255\n",
      "dev Loss: 0.0291\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 0.0256\n",
      "dev Loss: 0.0289\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 0.0255\n",
      "dev Loss: 0.0291\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 0.0255\n",
      "dev Loss: 0.0294\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 0.0256\n",
      "dev Loss: 0.0290\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 0.0255\n",
      "dev Loss: 0.0289\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 0.0256\n",
      "dev Loss: 0.0291\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 0.0255\n",
      "dev Loss: 0.0289\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 0.0255\n",
      "dev Loss: 0.0292\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 0.0254\n",
      "dev Loss: 0.0289\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 0.0255\n",
      "dev Loss: 0.0289\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 0.0254\n",
      "dev Loss: 0.0290\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 0.0254\n",
      "dev Loss: 0.0291\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 0.0255\n",
      "dev Loss: 0.0292\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 0.0254\n",
      "dev Loss: 0.0286\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 0.0255\n",
      "dev Loss: 0.0292\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 0.0254\n",
      "dev Loss: 0.0289\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 0.0254\n",
      "dev Loss: 0.0291\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 0.0253\n",
      "dev Loss: 0.0288\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 0.0254\n",
      "dev Loss: 0.0290\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 0.0253\n",
      "dev Loss: 0.0292\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 0.0253\n",
      "dev Loss: 0.0288\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 0.0253\n",
      "dev Loss: 0.0288\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 0.0252\n",
      "dev Loss: 0.0288\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 0.0253\n",
      "dev Loss: 0.0289\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 0.0254\n",
      "dev Loss: 0.0288\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 0.0253\n",
      "dev Loss: 0.0288\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 0.0253\n",
      "dev Loss: 0.0288\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 0.0253\n",
      "dev Loss: 0.0286\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 0.0252\n",
      "dev Loss: 0.0288\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 0.0253\n",
      "dev Loss: 0.0287\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 0.0252\n",
      "dev Loss: 0.0288\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 0.0252\n",
      "dev Loss: 0.0288\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 0.0253\n",
      "dev Loss: 0.0288\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 0.0252\n",
      "dev Loss: 0.0288\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 0.0252\n",
      "dev Loss: 0.0289\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 0.0251\n",
      "dev Loss: 0.0287\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 0.0251\n",
      "dev Loss: 0.0286\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 0.0252\n",
      "dev Loss: 0.0288\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 0.0252\n",
      "dev Loss: 0.0287\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 0.0252\n",
      "dev Loss: 0.0286\n",
      "\n",
      "Training complete in 1m 5s\n",
      "Best epoch:  96\n",
      "Best dev loss: 0.0286\n"
     ]
    }
   ],
   "source": [
    "optim_policy_BC = T.optim.Adam(model_policy_BC.parameters(), lr=1e-4)\n",
    "model_policy_BC = train_policy_model(model_policy_BC, loss_func_policy, \n",
    "                                     optim_policy_BC, ld_policy_BC, \n",
    "                                     ld_policy_sizes_BC, phase_list, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BCO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step3, predict inverse action for demo samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2371,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_demo_train = DataLoader(DS_Inv(trajs_demo[0:10]), batch_size=100)\n",
    "ld_demo_dev = DataLoader(DS_Inv(trajs_demo[10:20]), batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2377,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_demo = {'train':ld_demo_train, 'dev':ld_demo_dev}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2378,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_policy = {'train':[] , 'dev':[]}\n",
    "\n",
    "for phase in ['train', 'dev']:\n",
    "    for obs1, obs2, _, triple_index in ld_demo[phase]:\n",
    "        out = model_inv.pred_inv(obs1.float().cuda(), obs2.float().cuda())\n",
    "\n",
    "        obs = obs1.cpu().detach().numpy()\n",
    "        out = out.cpu().detach().numpy()\n",
    "        triple_index = triple_index.cpu().detach().numpy()\n",
    "\n",
    "        for i in range(len(obs)):\n",
    "            traj_policy[phase].append([obs[i], out[i], triple_index[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2379,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[array([ 5.86431900e+00, -9.05585876e+00, -3.01826134e-01,  2.26589325e-01,\n",
      "       -8.06245074e-01, -2.22211375e-01, -4.89587978e-02, -9.16634164e-02,\n",
      "       -2.08996955e+00, -1.61672505e-01,  2.32280342e-03, -1.87374925e-02,\n",
      "        9.27458995e-03, -2.70580454e-02, -1.50915217e-02,  2.15453684e-02,\n",
      "        1.15263660e-04]), array([ 0.20347701,  0.34384775,  1.1193811 ,  0.17079173,  2.0208821 ,\n",
      "       -1.8462422 ], dtype=float32), array([0, 0])], [array([ 5.64285616, -9.14364736, -0.05249746,  0.31124472,  0.22048949,\n",
      "       -0.1018104 ,  0.67278857, -1.04635227, -1.55273901, -0.65615573,\n",
      "       -0.52067574,  0.30448087,  0.16797441,  0.83793219,  0.17932269,\n",
      "        0.82118271, -0.94407157]), array([ 0.3537506 ,  0.80692464, -0.04507238,  0.7677301 ,  1.9528911 ,\n",
      "       -2.1713479 ], dtype=float32), array([0, 1])], [array([ 5.12151586, -9.327651  ,  0.35511856,  0.74467282,  0.25033256,\n",
      "        0.38406181,  0.9558607 , -2.44232486, -1.32452725, -1.25229409,\n",
      "       -0.89084545,  0.38732708,  0.41819003, -0.2253396 ,  0.6044111 ,\n",
      "        0.15404621, -0.68059948]), array([ 0.85847765,  0.63202524,  0.7313404 ,  1.5891402 ,  2.012512  ,\n",
      "       -2.4224703 ], dtype=float32), array([0, 2])], [array([ 4.26194662, -9.43105895,  0.75266541,  0.82308941,  0.30324495,\n",
      "        0.99533741,  1.06971234, -2.58187599, -1.42064999, -1.73778576,\n",
      "       -0.23484778,  0.23914642, -0.08348946,  0.17074703,  0.58356329,\n",
      "        0.1607238 ,  0.26439276]), array([ 0.7857718 ,  0.49362987,  0.717347  ,  1.1524554 ,  1.8319421 ,\n",
      "       -2.9808671 ], dtype=float32), array([0, 3])], [array([ 3.54300545, -9.30917324,  0.78603043,  0.90262501,  0.46612868,\n",
      "        1.07223118,  1.48918031, -2.02368805, -1.59335791, -1.27580075,\n",
      "        0.99989881, -0.06737675, -0.05733189,  0.14832909, -0.05525938,\n",
      "        0.21423762,  0.41363952]), array([ 0.21295416,  0.32014596,  0.3137529 ,  1.7757806 ,  2.2401986 ,\n",
      "       -3.7142265 ], dtype=float32), array([0, 4])], [array([ 2.97818932e+00, -8.97006063e+00,  5.59813432e-01,  7.45758050e-01,\n",
      "        5.65718205e-01,  1.11743871e+00,  1.43068733e+00, -2.08421642e+00,\n",
      "       -1.43333379e+00, -5.52609749e-01,  1.72685738e+00, -2.54788586e-01,\n",
      "       -2.66669137e-01,  5.42169056e-03, -5.21339283e-02,  1.68671890e-01,\n",
      "       -2.81465524e-01]), array([-0.7745243 , -0.32379228,  0.12833418,  1.0220557 ,  2.3705091 ,\n",
      "       -3.7529914 ], dtype=float32), array([0, 5])], [array([ 3.10483694, -8.48226536,  0.12756809,  0.24770338,  0.66523025,\n",
      "        1.04696897,  1.6138647 , -2.54944259, -1.3841659 ,  0.29027073,\n",
      "        2.699517  , -0.54602963, -0.56217021, -0.04707576, -0.1044874 ,\n",
      "        0.04983639, -0.26208459]), array([-1.6765711 , -1.3701285 , -0.2056048 ,  0.45405746,  3.0937004 ,\n",
      "       -3.568444  ], dtype=float32), array([0, 6])], [array([ 3.64929879, -7.8057517 , -0.60391845, -0.40582542,  0.33607448,\n",
      "        0.78645664,  1.84115238, -2.51039896, -1.42118176,  1.067321  ,\n",
      "        3.39852618, -0.82149431, -0.68038121, -0.44123358, -0.38116251,\n",
      "        0.21189613,  0.21813179]), array([-2.1754308 , -1.2005339 ,  0.03310595, -0.46996146,  1.7317042 ,\n",
      "       -2.945224  ], dtype=float32), array([0, 7])], [array([ 4.44939438, -7.1431017 , -1.5747563 , -0.81594951, -0.17852119,\n",
      "        0.43707178,  1.86139695, -1.59254139, -1.22144543,  1.40655126,\n",
      "        3.11352829, -0.9764227 , -0.26678435, -0.28601437, -0.40449597,\n",
      "       -0.10838891,  0.87938413]), array([-1.0245843 , -1.0326127 ,  0.06025844, -0.5717839 ,  2.088354  ,\n",
      "       -1.3713468 ], dtype=float32), array([0, 8])], [array([ 5.47025117e+00, -6.43638059e+00, -1.84613955e+00, -1.18286944e+00,\n",
      "       -5.26927405e-01, -3.09345725e-03,  1.71643651e+00, -2.24453124e-01,\n",
      "        4.32611922e-02,  1.76432324e+00,  3.65057619e+00,  1.17302218e-01,\n",
      "       -2.91374923e-01, -1.72438849e-01, -5.37015005e-01, -1.64223346e-01,\n",
      "        9.64716281e-01]), array([-2.0210552 , -0.41339302,  0.6945389 , -1.6013867 ,  2.4032712 ,\n",
      "       -1.751002  ], dtype=float32), array([0, 9])]]\n"
     ]
    }
   ],
   "source": [
    "print(traj_policy['train'][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step4, update policy via demo samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2380,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_policy = {'train':DataLoader(DS_Policy(traj_policy['train']), batch_size=64, shuffle=True),\n",
    "             'dev':DataLoader(DS_Policy(traj_policy['dev']), batch_size=64, shuffle=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 157, 'dev': 157}\n"
     ]
    }
   ],
   "source": [
    "ld_policy_sizes = {'train':len(ld_policy['train']),'dev':len(ld_policy['dev'])}\n",
    "print(ld_policy_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2382,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCO_policy(nn.Module):\n",
    "    def __init__(self, env, policy='mlp'):\n",
    "        super(BCO_policy, self).__init__()\n",
    "        \n",
    "        self.policy = policy\n",
    "        self.act_n = env.action_space.shape[0]\n",
    "        \n",
    "        init_ = lambda m: init(m, nn.init.xavier_normal_, lambda x: nn.init.\n",
    "                               constant_(x, 0), 0.5)\n",
    "        \n",
    "        if self.policy=='mlp':\n",
    "            self.obs_n = env.observation_space.shape[0]\n",
    "            self.pol = nn.Sequential(*[init_(nn.Linear(self.obs_n, 64)), nn.LeakyReLU(),\n",
    "                                       init_(nn.Linear(64, 64)), nn.LeakyReLU(),\n",
    "                                       init_(nn.Linear(64, self.act_n))])\n",
    "        \n",
    "        elif self.policy=='cnn':\n",
    "            pass\n",
    "    \n",
    "    def pred_act(self, obs):\n",
    "        out = self.pol(obs)\n",
    "        \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2383,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_option = 'mlp'\n",
    "model_policy = BCO_policy(env, policy=NN_option).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCO_policy(\n",
       "  (pol): Sequential(\n",
       "    (0): Linear(in_features=17, out_features=64, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=64, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2385,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func_policy = nn.L1Loss().cuda()\n",
    "optim_policy = T.optim.Adam(model_policy.parameters(), lr=1e-3)\n",
    "# optim_policy = T.optim.SGD(model_policy.parameters(), lr=5e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2386,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 0.3802\n",
      "dev Loss: 0.2008\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 0.1814\n",
      "dev Loss: 0.1650\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 0.1538\n",
      "dev Loss: 0.1445\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 0.1362\n",
      "dev Loss: 0.1303\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 0.1238\n",
      "dev Loss: 0.1195\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 0.1144\n",
      "dev Loss: 0.1108\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 0.1075\n",
      "dev Loss: 0.1052\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 0.1024\n",
      "dev Loss: 0.1016\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 0.0983\n",
      "dev Loss: 0.0985\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.0954\n",
      "dev Loss: 0.0950\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 0.0936\n",
      "dev Loss: 0.0948\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.0910\n",
      "dev Loss: 0.0938\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.0897\n",
      "dev Loss: 0.0912\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.0879\n",
      "dev Loss: 0.0891\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.0856\n",
      "dev Loss: 0.0867\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 0.0845\n",
      "dev Loss: 0.0848\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 0.0834\n",
      "dev Loss: 0.0850\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 0.0826\n",
      "dev Loss: 0.0843\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 0.0820\n",
      "dev Loss: 0.0830\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 0.0804\n",
      "dev Loss: 0.0848\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 0.0798\n",
      "dev Loss: 0.0815\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 0.0789\n",
      "dev Loss: 0.0819\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 0.0786\n",
      "dev Loss: 0.0803\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 0.0776\n",
      "dev Loss: 0.0795\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 0.0769\n",
      "dev Loss: 0.0805\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 0.0766\n",
      "dev Loss: 0.0784\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 0.0756\n",
      "dev Loss: 0.0776\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 0.0752\n",
      "dev Loss: 0.0760\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 0.0745\n",
      "dev Loss: 0.0751\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 0.0736\n",
      "dev Loss: 0.0769\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 0.0736\n",
      "dev Loss: 0.0741\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 0.0731\n",
      "dev Loss: 0.0760\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 0.0721\n",
      "dev Loss: 0.0752\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 0.0727\n",
      "dev Loss: 0.0747\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 0.0718\n",
      "dev Loss: 0.0746\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 0.0716\n",
      "dev Loss: 0.0762\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 0.0710\n",
      "dev Loss: 0.0725\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 0.0709\n",
      "dev Loss: 0.0737\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 0.0705\n",
      "dev Loss: 0.0717\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 0.0698\n",
      "dev Loss: 0.0743\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 0.0701\n",
      "dev Loss: 0.0722\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 0.0694\n",
      "dev Loss: 0.0707\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 0.0700\n",
      "dev Loss: 0.0737\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 0.0690\n",
      "dev Loss: 0.0731\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 0.0687\n",
      "dev Loss: 0.0708\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 0.0687\n",
      "dev Loss: 0.0721\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 0.0676\n",
      "dev Loss: 0.0709\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 0.0680\n",
      "dev Loss: 0.0707\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 0.0675\n",
      "dev Loss: 0.0699\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 0.0673\n",
      "dev Loss: 0.0689\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 0.0672\n",
      "dev Loss: 0.0707\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 0.0673\n",
      "dev Loss: 0.0712\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 0.0671\n",
      "dev Loss: 0.0686\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 0.0668\n",
      "dev Loss: 0.0708\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 0.0667\n",
      "dev Loss: 0.0699\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 0.0655\n",
      "dev Loss: 0.0694\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 0.0656\n",
      "dev Loss: 0.0690\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 0.0656\n",
      "dev Loss: 0.0686\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 0.0661\n",
      "dev Loss: 0.0694\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 0.0660\n",
      "dev Loss: 0.0692\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 0.0653\n",
      "dev Loss: 0.0689\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 0.0649\n",
      "dev Loss: 0.0678\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 0.0643\n",
      "dev Loss: 0.0679\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 0.0648\n",
      "dev Loss: 0.0681\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 0.0643\n",
      "dev Loss: 0.0663\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 0.0645\n",
      "dev Loss: 0.0676\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 0.0646\n",
      "dev Loss: 0.0661\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 0.0636\n",
      "dev Loss: 0.0673\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 0.0637\n",
      "dev Loss: 0.0666\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 0.0641\n",
      "dev Loss: 0.0673\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 0.0633\n",
      "dev Loss: 0.0675\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 0.0640\n",
      "dev Loss: 0.0681\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 0.0636\n",
      "dev Loss: 0.0661\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 0.0630\n",
      "dev Loss: 0.0676\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 0.0635\n",
      "dev Loss: 0.0661\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 0.0627\n",
      "dev Loss: 0.0667\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 0.0631\n",
      "dev Loss: 0.0668\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 0.0625\n",
      "dev Loss: 0.0661\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 0.0632\n",
      "dev Loss: 0.0697\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 0.0626\n",
      "dev Loss: 0.0650\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 0.0621\n",
      "dev Loss: 0.0642\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 0.0626\n",
      "dev Loss: 0.0655\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 0.0623\n",
      "dev Loss: 0.0645\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 0.0624\n",
      "dev Loss: 0.0658\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 0.0623\n",
      "dev Loss: 0.0665\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 0.0621\n",
      "dev Loss: 0.0645\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 0.0615\n",
      "dev Loss: 0.0650\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 0.0616\n",
      "dev Loss: 0.0649\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 0.0615\n",
      "dev Loss: 0.0650\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 0.0619\n",
      "dev Loss: 0.0641\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 0.0610\n",
      "dev Loss: 0.0656\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 0.0614\n",
      "dev Loss: 0.0640\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 0.0610\n",
      "dev Loss: 0.0672\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 0.0614\n",
      "dev Loss: 0.0629\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 0.0608\n",
      "dev Loss: 0.0644\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 0.0609\n",
      "dev Loss: 0.0618\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 0.0605\n",
      "dev Loss: 0.0646\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 0.0605\n",
      "dev Loss: 0.0630\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 0.0603\n",
      "dev Loss: 0.0631\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 0.0608\n",
      "dev Loss: 0.0650\n",
      "\n",
      "Training complete in 1m 4s\n",
      "Best epoch:  95\n",
      "Best dev loss: 0.0618\n"
     ]
    }
   ],
   "source": [
    "phase_list = ['train', 'dev']\n",
    "model_policy = train_policy_model(model_policy, loss_func_policy, \n",
    "                                  optim_policy, ld_policy, ld_policy_sizes, phase_list, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2388,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 0.0553\n",
      "dev Loss: 0.0587\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 0.0552\n",
      "dev Loss: 0.0587\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 0.0551\n",
      "dev Loss: 0.0587\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 0.0552\n",
      "dev Loss: 0.0586\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 0.0550\n",
      "dev Loss: 0.0587\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 0.0551\n",
      "dev Loss: 0.0585\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 0.0549\n",
      "dev Loss: 0.0586\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 0.0549\n",
      "dev Loss: 0.0586\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 0.0549\n",
      "dev Loss: 0.0586\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.0548\n",
      "dev Loss: 0.0589\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 0.0549\n",
      "dev Loss: 0.0584\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.0549\n",
      "dev Loss: 0.0588\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.0548\n",
      "dev Loss: 0.0585\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.0548\n",
      "dev Loss: 0.0585\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.0548\n",
      "dev Loss: 0.0584\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 0.0548\n",
      "dev Loss: 0.0587\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 0.0548\n",
      "dev Loss: 0.0586\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 0.0547\n",
      "dev Loss: 0.0585\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 0.0548\n",
      "dev Loss: 0.0585\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 0.0549\n",
      "dev Loss: 0.0584\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 0.0546\n",
      "dev Loss: 0.0585\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 0.0547\n",
      "dev Loss: 0.0584\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 0.0548\n",
      "dev Loss: 0.0584\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 0.0547\n",
      "dev Loss: 0.0584\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 0.0546\n",
      "dev Loss: 0.0584\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 0.0546\n",
      "dev Loss: 0.0586\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 0.0546\n",
      "dev Loss: 0.0586\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 0.0545\n",
      "dev Loss: 0.0586\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 0.0545\n",
      "dev Loss: 0.0583\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 0.0545\n",
      "dev Loss: 0.0582\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 0.0546\n",
      "dev Loss: 0.0580\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 0.0545\n",
      "dev Loss: 0.0583\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 0.0544\n",
      "dev Loss: 0.0589\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 0.0544\n",
      "dev Loss: 0.0583\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 0.0544\n",
      "dev Loss: 0.0583\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 0.0545\n",
      "dev Loss: 0.0581\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 0.0543\n",
      "dev Loss: 0.0584\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 0.0544\n",
      "dev Loss: 0.0582\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 0.0547\n",
      "dev Loss: 0.0584\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 0.0545\n",
      "dev Loss: 0.0582\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 0.0544\n",
      "dev Loss: 0.0582\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 0.0544\n",
      "dev Loss: 0.0581\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 0.0543\n",
      "dev Loss: 0.0578\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 0.0543\n",
      "dev Loss: 0.0581\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 0.0543\n",
      "dev Loss: 0.0581\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 0.0544\n",
      "dev Loss: 0.0583\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 0.0542\n",
      "dev Loss: 0.0579\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 0.0544\n",
      "dev Loss: 0.0584\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 0.0542\n",
      "dev Loss: 0.0579\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 0.0542\n",
      "dev Loss: 0.0582\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 0.0542\n",
      "dev Loss: 0.0581\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 0.0541\n",
      "dev Loss: 0.0581\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 0.0542\n",
      "dev Loss: 0.0578\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 0.0542\n",
      "dev Loss: 0.0581\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 0.0542\n",
      "dev Loss: 0.0580\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 0.0542\n",
      "dev Loss: 0.0580\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 0.0541\n",
      "dev Loss: 0.0580\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 0.0541\n",
      "dev Loss: 0.0580\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 0.0540\n",
      "dev Loss: 0.0580\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 0.0540\n",
      "dev Loss: 0.0579\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 0.0541\n",
      "dev Loss: 0.0578\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 0.0542\n",
      "dev Loss: 0.0581\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 0.0540\n",
      "dev Loss: 0.0582\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 0.0539\n",
      "dev Loss: 0.0578\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 0.0541\n",
      "dev Loss: 0.0578\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 0.0540\n",
      "dev Loss: 0.0578\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 0.0544\n",
      "dev Loss: 0.0580\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 0.0541\n",
      "dev Loss: 0.0579\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 0.0540\n",
      "dev Loss: 0.0579\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 0.0539\n",
      "dev Loss: 0.0580\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 0.0539\n",
      "dev Loss: 0.0577\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 0.0539\n",
      "dev Loss: 0.0580\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 0.0540\n",
      "dev Loss: 0.0577\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 0.0539\n",
      "dev Loss: 0.0578\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 0.0539\n",
      "dev Loss: 0.0578\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 0.0539\n",
      "dev Loss: 0.0577\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 0.0540\n",
      "dev Loss: 0.0576\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 0.0539\n",
      "dev Loss: 0.0576\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 0.0538\n",
      "dev Loss: 0.0577\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 0.0538\n",
      "dev Loss: 0.0577\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 0.0539\n",
      "dev Loss: 0.0577\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 0.0538\n",
      "dev Loss: 0.0577\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 0.0538\n",
      "dev Loss: 0.0577\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 0.0538\n",
      "dev Loss: 0.0580\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 0.0537\n",
      "dev Loss: 0.0578\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 0.0539\n",
      "dev Loss: 0.0579\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 0.0537\n",
      "dev Loss: 0.0576\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 0.0537\n",
      "dev Loss: 0.0576\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 0.0537\n",
      "dev Loss: 0.0577\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 0.0537\n",
      "dev Loss: 0.0576\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 0.0537\n",
      "dev Loss: 0.0578\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 0.0537\n",
      "dev Loss: 0.0577\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 0.0536\n",
      "dev Loss: 0.0575\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 0.0536\n",
      "dev Loss: 0.0576\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 0.0535\n",
      "dev Loss: 0.0575\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 0.0536\n",
      "dev Loss: 0.0574\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 0.0537\n",
      "dev Loss: 0.0576\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 0.0535\n",
      "dev Loss: 0.0576\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 0.0536\n",
      "dev Loss: 0.0574\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 0.0535\n",
      "dev Loss: 0.0577\n",
      "\n",
      "Training complete in 1m 4s\n",
      "Best epoch:  95\n",
      "Best dev loss: 0.0574\n"
     ]
    }
   ],
   "source": [
    "optim_policy = T.optim.Adam(model_policy.parameters(), lr=1e-4)\n",
    "model_policy = train_policy_model(model_policy, loss_func_policy, \n",
    "                                  optim_policy, ld_policy, ld_policy_sizes, phase_list, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SC-BCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2404,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_sigma = 0.1\n",
    "aug_ratio = 4\n",
    "noise_sigma_dev = 0.1\n",
    "aug_ratio_dev = 5\n",
    "# print(np.random.normal(0, noise_sigma, dX).astype(np.float32).dtype)\n",
    "\n",
    "demo_DS_Inv_aug_train = DS_Inv_aug(trajs_demo[0:10], noise_sigma, aug_ratio)\n",
    "demo_DS_Inv_aug_dev_local = DS_Inv_aug(trajs_demo[0:10], noise_sigma_dev, aug_ratio_dev, True)\n",
    "demo_DS_Inv_aug_dev_global = DS_Inv_aug(trajs_demo[10:20], noise_sigma, aug_ratio)\n",
    "\n",
    "ld_demo_aug_train = DataLoader(demo_DS_Inv_aug_train, batch_size=100)\n",
    "ld_demo_aug_dev_local = DataLoader(demo_DS_Inv_aug_dev_local, batch_size=100)\n",
    "ld_demo_aug_dev_global = DataLoader(demo_DS_Inv_aug_dev_global, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2405,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_demo_aug = {'train':ld_demo_aug_train, 'dev_local':ld_demo_aug_dev_local,\n",
    "               'dev_global':ld_demo_aug_dev_global}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2411,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_policy_aug = {'train':[] , 'dev_local':[], 'dev_global':[]}\n",
    "\n",
    "for phase in ['train', 'dev_local', 'dev_global']:\n",
    "    for obs1, obs2, _, triple_index in ld_demo_aug[phase]:\n",
    "        out = model_inv.pred_inv(obs1.float().cuda(), obs2.float().cuda())\n",
    "\n",
    "        obs = obs1.cpu().detach().numpy()\n",
    "        out = out.cpu().detach().numpy()\n",
    "        triple_index = triple_index.cpu().detach().numpy()\n",
    "\n",
    "        for i in range(len(obs)):\n",
    "            traj_policy_aug[phase].append([obs[i], out[i], triple_index[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2412,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_policy_aug = {'train':DataLoader(DS_Policy(traj_policy_aug['train']), batch_size=128, shuffle=True),\n",
    "                 'dev_local':DataLoader(DS_Policy(traj_policy_aug['dev_local']), batch_size=128, shuffle=True),\n",
    "                 'dev_global':DataLoader(DS_Policy(traj_policy_aug['dev_global']), batch_size=128, shuffle=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 391, 'dev_local': 391, 'dev_global': 391}\n"
     ]
    }
   ],
   "source": [
    "ld_policy_aug_sizes = {'train':len(ld_policy_aug['train']),\n",
    "                       'dev_local':len(ld_policy_aug['dev_local']),\n",
    "                       'dev_global':len(ld_policy_aug['dev_global']),}\n",
    "print(ld_policy_aug_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2450,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCO_policy_aug(nn.Module):\n",
    "    def __init__(self, env, policy='mlp'):\n",
    "        super(BCO_policy_aug, self).__init__()\n",
    "        \n",
    "        self.policy = policy\n",
    "        self.act_n = env.action_space.shape[0]\n",
    "        \n",
    "        init_ = lambda m: init(m, nn.init.xavier_normal_, lambda x: nn.init.\n",
    "                               constant_(x, 0), 0.5)\n",
    "        \n",
    "        if self.policy=='mlp':\n",
    "            self.obs_n = env.observation_space.shape[0]\n",
    "            self.pol = nn.Sequential(*[init_(nn.Linear(self.obs_n, 64)), nn.LeakyReLU(),\n",
    "                                       init_(nn.Linear(64, 64)), nn.LeakyReLU(),\n",
    "                                       init_(nn.Linear(64, self.act_n))])\n",
    "        \n",
    "        elif self.policy=='cnn':\n",
    "            pass\n",
    "    \n",
    "    def pred_act(self, obs):\n",
    "        out = self.pol(obs)\n",
    "        \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2451,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_policy_aug = BCO_policy_aug(env, policy=NN_option).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCO_policy_aug(\n",
       "  (pol): Sequential(\n",
       "    (0): Linear(in_features=17, out_features=64, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=64, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_policy_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2453,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_policy_aug = T.optim.Adam(model_policy_aug.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2454,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 0.2811\n",
      "dev_local Loss: 0.1899\n",
      "dev_global Loss: 0.1803\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 0.1654\n",
      "dev_local Loss: 0.1629\n",
      "dev_global Loss: 0.1534\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 0.1475\n",
      "dev_local Loss: 0.1519\n",
      "dev_global Loss: 0.1427\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 0.1389\n",
      "dev_local Loss: 0.1453\n",
      "dev_global Loss: 0.1367\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 0.1330\n",
      "dev_local Loss: 0.1404\n",
      "dev_global Loss: 0.1321\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 0.1296\n",
      "dev_local Loss: 0.1362\n",
      "dev_global Loss: 0.1279\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 0.1266\n",
      "dev_local Loss: 0.1335\n",
      "dev_global Loss: 0.1255\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 0.1243\n",
      "dev_local Loss: 0.1325\n",
      "dev_global Loss: 0.1248\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 0.1225\n",
      "dev_local Loss: 0.1295\n",
      "dev_global Loss: 0.1219\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.1210\n",
      "dev_local Loss: 0.1303\n",
      "dev_global Loss: 0.1232\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 0.1197\n",
      "dev_local Loss: 0.1280\n",
      "dev_global Loss: 0.1208\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.1185\n",
      "dev_local Loss: 0.1259\n",
      "dev_global Loss: 0.1187\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.1174\n",
      "dev_local Loss: 0.1255\n",
      "dev_global Loss: 0.1187\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.1161\n",
      "dev_local Loss: 0.1242\n",
      "dev_global Loss: 0.1174\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.1155\n",
      "dev_local Loss: 0.1224\n",
      "dev_global Loss: 0.1156\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 0.1146\n",
      "dev_local Loss: 0.1227\n",
      "dev_global Loss: 0.1164\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 0.1139\n",
      "dev_local Loss: 0.1240\n",
      "dev_global Loss: 0.1174\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 0.1131\n",
      "dev_local Loss: 0.1209\n",
      "dev_global Loss: 0.1143\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 0.1128\n",
      "dev_local Loss: 0.1213\n",
      "dev_global Loss: 0.1149\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 0.1117\n",
      "dev_local Loss: 0.1186\n",
      "dev_global Loss: 0.1120\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 0.1113\n",
      "dev_local Loss: 0.1196\n",
      "dev_global Loss: 0.1130\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 0.1109\n",
      "dev_local Loss: 0.1205\n",
      "dev_global Loss: 0.1144\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 0.1104\n",
      "dev_local Loss: 0.1184\n",
      "dev_global Loss: 0.1120\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 0.1100\n",
      "dev_local Loss: 0.1187\n",
      "dev_global Loss: 0.1127\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 0.1093\n",
      "dev_local Loss: 0.1181\n",
      "dev_global Loss: 0.1119\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 0.1090\n",
      "dev_local Loss: 0.1170\n",
      "dev_global Loss: 0.1108\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 0.1087\n",
      "dev_local Loss: 0.1166\n",
      "dev_global Loss: 0.1105\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 0.1084\n",
      "dev_local Loss: 0.1176\n",
      "dev_global Loss: 0.1116\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 0.1082\n",
      "dev_local Loss: 0.1160\n",
      "dev_global Loss: 0.1099\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 0.1078\n",
      "dev_local Loss: 0.1152\n",
      "dev_global Loss: 0.1093\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 0.1078\n",
      "dev_local Loss: 0.1171\n",
      "dev_global Loss: 0.1111\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 0.1074\n",
      "dev_local Loss: 0.1171\n",
      "dev_global Loss: 0.1111\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 0.1072\n",
      "dev_local Loss: 0.1153\n",
      "dev_global Loss: 0.1092\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 0.1068\n",
      "dev_local Loss: 0.1161\n",
      "dev_global Loss: 0.1105\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 0.1066\n",
      "dev_local Loss: 0.1146\n",
      "dev_global Loss: 0.1088\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 0.1066\n",
      "dev_local Loss: 0.1150\n",
      "dev_global Loss: 0.1091\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 0.1058\n",
      "dev_local Loss: 0.1151\n",
      "dev_global Loss: 0.1096\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 0.1063\n",
      "dev_local Loss: 0.1154\n",
      "dev_global Loss: 0.1099\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 0.1058\n",
      "dev_local Loss: 0.1144\n",
      "dev_global Loss: 0.1087\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 0.1056\n",
      "dev_local Loss: 0.1128\n",
      "dev_global Loss: 0.1068\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 0.1054\n",
      "dev_local Loss: 0.1137\n",
      "dev_global Loss: 0.1081\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 0.1053\n",
      "dev_local Loss: 0.1143\n",
      "dev_global Loss: 0.1090\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 0.1051\n",
      "dev_local Loss: 0.1131\n",
      "dev_global Loss: 0.1074\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 0.1050\n",
      "dev_local Loss: 0.1135\n",
      "dev_global Loss: 0.1081\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 0.1047\n",
      "dev_local Loss: 0.1123\n",
      "dev_global Loss: 0.1068\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 0.1047\n",
      "dev_local Loss: 0.1140\n",
      "dev_global Loss: 0.1084\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 0.1046\n",
      "dev_local Loss: 0.1127\n",
      "dev_global Loss: 0.1074\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 0.1040\n",
      "dev_local Loss: 0.1117\n",
      "dev_global Loss: 0.1060\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 0.1042\n",
      "dev_local Loss: 0.1141\n",
      "dev_global Loss: 0.1088\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 0.1040\n",
      "dev_local Loss: 0.1125\n",
      "dev_global Loss: 0.1072\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 0.1040\n",
      "dev_local Loss: 0.1130\n",
      "dev_global Loss: 0.1075\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 0.1037\n",
      "dev_local Loss: 0.1126\n",
      "dev_global Loss: 0.1074\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 0.1036\n",
      "dev_local Loss: 0.1126\n",
      "dev_global Loss: 0.1070\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 0.1034\n",
      "dev_local Loss: 0.1118\n",
      "dev_global Loss: 0.1062\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 0.1034\n",
      "dev_local Loss: 0.1129\n",
      "dev_global Loss: 0.1077\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 0.1033\n",
      "dev_local Loss: 0.1110\n",
      "dev_global Loss: 0.1056\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 0.1029\n",
      "dev_local Loss: 0.1112\n",
      "dev_global Loss: 0.1058\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 0.1031\n",
      "dev_local Loss: 0.1118\n",
      "dev_global Loss: 0.1066\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 0.1029\n",
      "dev_local Loss: 0.1114\n",
      "dev_global Loss: 0.1060\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 0.1028\n",
      "dev_local Loss: 0.1107\n",
      "dev_global Loss: 0.1054\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 0.1027\n",
      "dev_local Loss: 0.1125\n",
      "dev_global Loss: 0.1075\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 0.1026\n",
      "dev_local Loss: 0.1136\n",
      "dev_global Loss: 0.1085\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 0.1023\n",
      "dev_local Loss: 0.1111\n",
      "dev_global Loss: 0.1058\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 0.1025\n",
      "dev_local Loss: 0.1112\n",
      "dev_global Loss: 0.1059\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 0.1021\n",
      "dev_local Loss: 0.1118\n",
      "dev_global Loss: 0.1070\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 0.1021\n",
      "dev_local Loss: 0.1120\n",
      "dev_global Loss: 0.1068\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 0.1020\n",
      "dev_local Loss: 0.1094\n",
      "dev_global Loss: 0.1045\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 0.1018\n",
      "dev_local Loss: 0.1111\n",
      "dev_global Loss: 0.1064\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 0.1021\n",
      "dev_local Loss: 0.1105\n",
      "dev_global Loss: 0.1056\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 0.1017\n",
      "dev_local Loss: 0.1102\n",
      "dev_global Loss: 0.1053\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 0.1019\n",
      "dev_local Loss: 0.1098\n",
      "dev_global Loss: 0.1048\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 0.1016\n",
      "dev_local Loss: 0.1107\n",
      "dev_global Loss: 0.1057\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 0.1015\n",
      "dev_local Loss: 0.1108\n",
      "dev_global Loss: 0.1056\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 0.1014\n",
      "dev_local Loss: 0.1095\n",
      "dev_global Loss: 0.1047\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 0.1015\n",
      "dev_local Loss: 0.1091\n",
      "dev_global Loss: 0.1041\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 0.1013\n",
      "dev_local Loss: 0.1117\n",
      "dev_global Loss: 0.1068\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 0.1011\n",
      "dev_local Loss: 0.1091\n",
      "dev_global Loss: 0.1042\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 0.1012\n",
      "dev_local Loss: 0.1094\n",
      "dev_global Loss: 0.1045\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 0.1012\n",
      "dev_local Loss: 0.1103\n",
      "dev_global Loss: 0.1054\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 0.1010\n",
      "dev_local Loss: 0.1090\n",
      "dev_global Loss: 0.1040\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 0.1009\n",
      "dev_local Loss: 0.1092\n",
      "dev_global Loss: 0.1044\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 0.1009\n",
      "dev_local Loss: 0.1088\n",
      "dev_global Loss: 0.1039\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 0.1008\n",
      "dev_local Loss: 0.1115\n",
      "dev_global Loss: 0.1068\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 0.1007\n",
      "dev_local Loss: 0.1091\n",
      "dev_global Loss: 0.1043\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 0.1008\n",
      "dev_local Loss: 0.1094\n",
      "dev_global Loss: 0.1047\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 0.1006\n",
      "dev_local Loss: 0.1106\n",
      "dev_global Loss: 0.1059\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 0.1006\n",
      "dev_local Loss: 0.1083\n",
      "dev_global Loss: 0.1032\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 0.1007\n",
      "dev_local Loss: 0.1090\n",
      "dev_global Loss: 0.1041\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 0.1006\n",
      "dev_local Loss: 0.1083\n",
      "dev_global Loss: 0.1034\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 0.1005\n",
      "dev_local Loss: 0.1088\n",
      "dev_global Loss: 0.1040\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 0.1003\n",
      "dev_local Loss: 0.1078\n",
      "dev_global Loss: 0.1028\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1004\n",
      "dev_local Loss: 0.1090\n",
      "dev_global Loss: 0.1040\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 0.1004\n",
      "dev_local Loss: 0.1088\n",
      "dev_global Loss: 0.1039\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 0.1003\n",
      "dev_local Loss: 0.1090\n",
      "dev_global Loss: 0.1042\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 0.1001\n",
      "dev_local Loss: 0.1084\n",
      "dev_global Loss: 0.1035\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 0.1000\n",
      "dev_local Loss: 0.1083\n",
      "dev_global Loss: 0.1034\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 0.0999\n",
      "dev_local Loss: 0.1085\n",
      "dev_global Loss: 0.1040\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 0.1000\n",
      "dev_local Loss: 0.1079\n",
      "dev_global Loss: 0.1034\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 0.1000\n",
      "dev_local Loss: 0.1084\n",
      "dev_global Loss: 0.1035\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 0.0998\n",
      "dev_local Loss: 0.1080\n",
      "dev_global Loss: 0.1035\n",
      "\n",
      "Training complete in 4m 37s\n",
      "Best epoch:  90\n",
      "Best dev loss: 0.1028\n"
     ]
    }
   ],
   "source": [
    "phase_list = ['train', 'dev_local', 'dev_global']\n",
    "model_policy_aug = train_policy_model(model_policy_aug, loss_func_policy, \n",
    "                                      optim_policy_aug, ld_policy_aug, \n",
    "                                      ld_policy_aug_sizes, phase_list, 100, \n",
    "                                      best_criterion='dev_global')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2455,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 0.0961\n",
      "dev_local Loss: 0.1053\n",
      "dev_global Loss: 0.1002\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 0.0958\n",
      "dev_local Loss: 0.1055\n",
      "dev_global Loss: 0.1004\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 0.0958\n",
      "dev_local Loss: 0.1054\n",
      "dev_global Loss: 0.1003\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 0.0957\n",
      "dev_local Loss: 0.1053\n",
      "dev_global Loss: 0.1002\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 0.0957\n",
      "dev_local Loss: 0.1054\n",
      "dev_global Loss: 0.1002\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 0.0957\n",
      "dev_local Loss: 0.1054\n",
      "dev_global Loss: 0.1002\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 0.0957\n",
      "dev_local Loss: 0.1053\n",
      "dev_global Loss: 0.1001\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 0.0957\n",
      "dev_local Loss: 0.1052\n",
      "dev_global Loss: 0.1001\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 0.0956\n",
      "dev_local Loss: 0.1055\n",
      "dev_global Loss: 0.1003\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.0956\n",
      "dev_local Loss: 0.1053\n",
      "dev_global Loss: 0.1001\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 0.0956\n",
      "dev_local Loss: 0.1053\n",
      "dev_global Loss: 0.1002\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.0956\n",
      "dev_local Loss: 0.1053\n",
      "dev_global Loss: 0.1002\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.0956\n",
      "dev_local Loss: 0.1053\n",
      "dev_global Loss: 0.1003\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.0956\n",
      "dev_local Loss: 0.1052\n",
      "dev_global Loss: 0.1002\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.0956\n",
      "dev_local Loss: 0.1052\n",
      "dev_global Loss: 0.1001\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 0.0955\n",
      "dev_local Loss: 0.1051\n",
      "dev_global Loss: 0.1000\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 0.0955\n",
      "dev_local Loss: 0.1052\n",
      "dev_global Loss: 0.1001\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 0.0955\n",
      "dev_local Loss: 0.1051\n",
      "dev_global Loss: 0.1001\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 0.0955\n",
      "dev_local Loss: 0.1052\n",
      "dev_global Loss: 0.1001\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 0.0955\n",
      "dev_local Loss: 0.1052\n",
      "dev_global Loss: 0.1002\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 0.0955\n",
      "dev_local Loss: 0.1051\n",
      "dev_global Loss: 0.1001\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 0.0955\n",
      "dev_local Loss: 0.1053\n",
      "dev_global Loss: 0.1002\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 0.0954\n",
      "dev_local Loss: 0.1052\n",
      "dev_global Loss: 0.1000\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 0.0954\n",
      "dev_local Loss: 0.1050\n",
      "dev_global Loss: 0.0999\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 0.0954\n",
      "dev_local Loss: 0.1051\n",
      "dev_global Loss: 0.0999\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 0.0954\n",
      "dev_local Loss: 0.1051\n",
      "dev_global Loss: 0.1000\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 0.0954\n",
      "dev_local Loss: 0.1052\n",
      "dev_global Loss: 0.1001\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 0.0954\n",
      "dev_local Loss: 0.1051\n",
      "dev_global Loss: 0.1000\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 0.0954\n",
      "dev_local Loss: 0.1052\n",
      "dev_global Loss: 0.1002\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 0.0954\n",
      "dev_local Loss: 0.1051\n",
      "dev_global Loss: 0.1001\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 0.0954\n",
      "dev_local Loss: 0.1051\n",
      "dev_global Loss: 0.1000\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 0.0953\n",
      "dev_local Loss: 0.1053\n",
      "dev_global Loss: 0.1002\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 0.0953\n",
      "dev_local Loss: 0.1051\n",
      "dev_global Loss: 0.1000\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 0.0953\n",
      "dev_local Loss: 0.1049\n",
      "dev_global Loss: 0.0999\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 0.0953\n",
      "dev_local Loss: 0.1051\n",
      "dev_global Loss: 0.1000\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 0.0953\n",
      "dev_local Loss: 0.1050\n",
      "dev_global Loss: 0.0998\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 0.0953\n",
      "dev_local Loss: 0.1051\n",
      "dev_global Loss: 0.1000\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 0.0953\n",
      "dev_local Loss: 0.1050\n",
      "dev_global Loss: 0.1001\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 0.0953\n",
      "dev_local Loss: 0.1049\n",
      "dev_global Loss: 0.0999\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 0.0952\n",
      "dev_local Loss: 0.1050\n",
      "dev_global Loss: 0.1000\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 0.0952\n",
      "dev_local Loss: 0.1050\n",
      "dev_global Loss: 0.0999\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 0.0952\n",
      "dev_local Loss: 0.1049\n",
      "dev_global Loss: 0.0998\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 0.0952\n",
      "dev_local Loss: 0.1049\n",
      "dev_global Loss: 0.1000\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 0.0952\n",
      "dev_local Loss: 0.1050\n",
      "dev_global Loss: 0.0999\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 0.0952\n",
      "dev_local Loss: 0.1049\n",
      "dev_global Loss: 0.0999\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 0.0952\n",
      "dev_local Loss: 0.1049\n",
      "dev_global Loss: 0.1000\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 0.0952\n",
      "dev_local Loss: 0.1049\n",
      "dev_global Loss: 0.0998\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 0.0952\n",
      "dev_local Loss: 0.1049\n",
      "dev_global Loss: 0.0998\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 0.0952\n",
      "dev_local Loss: 0.1050\n",
      "dev_global Loss: 0.1000\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 0.0951\n",
      "dev_local Loss: 0.1049\n",
      "dev_global Loss: 0.0999\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 0.0951\n",
      "dev_local Loss: 0.1049\n",
      "dev_global Loss: 0.0998\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 0.0951\n",
      "dev_local Loss: 0.1050\n",
      "dev_global Loss: 0.0999\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 0.0951\n",
      "dev_local Loss: 0.1048\n",
      "dev_global Loss: 0.0999\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 0.0951\n",
      "dev_local Loss: 0.1048\n",
      "dev_global Loss: 0.0998\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 0.0951\n",
      "dev_local Loss: 0.1048\n",
      "dev_global Loss: 0.0998\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 0.0951\n",
      "dev_local Loss: 0.1049\n",
      "dev_global Loss: 0.0998\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 0.0951\n",
      "dev_local Loss: 0.1048\n",
      "dev_global Loss: 0.0998\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 0.0951\n",
      "dev_local Loss: 0.1050\n",
      "dev_global Loss: 0.0999\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 0.0951\n",
      "dev_local Loss: 0.1047\n",
      "dev_global Loss: 0.0996\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 0.0950\n",
      "dev_local Loss: 0.1048\n",
      "dev_global Loss: 0.0998\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 0.0950\n",
      "dev_local Loss: 0.1050\n",
      "dev_global Loss: 0.0999\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 0.0951\n",
      "dev_local Loss: 0.1047\n",
      "dev_global Loss: 0.0997\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 0.0950\n",
      "dev_local Loss: 0.1048\n",
      "dev_global Loss: 0.0998\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 0.0950\n",
      "dev_local Loss: 0.1048\n",
      "dev_global Loss: 0.0998\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 0.0950\n",
      "dev_local Loss: 0.1049\n",
      "dev_global Loss: 0.0999\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 0.0949\n",
      "dev_local Loss: 0.1048\n",
      "dev_global Loss: 0.0998\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 0.0950\n",
      "dev_local Loss: 0.1047\n",
      "dev_global Loss: 0.0996\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 0.0949\n",
      "dev_local Loss: 0.1047\n",
      "dev_global Loss: 0.0997\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 0.0950\n",
      "dev_local Loss: 0.1048\n",
      "dev_global Loss: 0.0998\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 0.0950\n",
      "dev_local Loss: 0.1047\n",
      "dev_global Loss: 0.0998\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 0.0950\n",
      "dev_local Loss: 0.1047\n",
      "dev_global Loss: 0.0997\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 0.0949\n",
      "dev_local Loss: 0.1046\n",
      "dev_global Loss: 0.0997\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 0.0949\n",
      "dev_local Loss: 0.1047\n",
      "dev_global Loss: 0.0998\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 0.0949\n",
      "dev_local Loss: 0.1047\n",
      "dev_global Loss: 0.0997\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 0.0949\n",
      "dev_local Loss: 0.1047\n",
      "dev_global Loss: 0.0997\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 0.0949\n",
      "dev_local Loss: 0.1050\n",
      "dev_global Loss: 0.1001\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 0.0949\n",
      "dev_local Loss: 0.1046\n",
      "dev_global Loss: 0.0997\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 0.0949\n",
      "dev_local Loss: 0.1046\n",
      "dev_global Loss: 0.0996\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 0.0948\n",
      "dev_local Loss: 0.1047\n",
      "dev_global Loss: 0.0996\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 0.0948\n",
      "dev_local Loss: 0.1047\n",
      "dev_global Loss: 0.0997\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 0.0949\n",
      "dev_local Loss: 0.1046\n",
      "dev_global Loss: 0.0996\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 0.0949\n",
      "dev_local Loss: 0.1046\n",
      "dev_global Loss: 0.0997\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 0.0948\n",
      "dev_local Loss: 0.1047\n",
      "dev_global Loss: 0.0996\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 0.0948\n",
      "dev_local Loss: 0.1047\n",
      "dev_global Loss: 0.0997\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 0.0948\n",
      "dev_local Loss: 0.1047\n",
      "dev_global Loss: 0.0997\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 0.0948\n",
      "dev_local Loss: 0.1047\n",
      "dev_global Loss: 0.0996\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 0.0948\n",
      "dev_local Loss: 0.1047\n",
      "dev_global Loss: 0.0999\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 0.0948\n",
      "dev_local Loss: 0.1047\n",
      "dev_global Loss: 0.0998\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 0.0948\n",
      "dev_local Loss: 0.1046\n",
      "dev_global Loss: 0.0997\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 0.0948\n",
      "dev_local Loss: 0.1046\n",
      "dev_global Loss: 0.0996\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 0.0948\n",
      "dev_local Loss: 0.1046\n",
      "dev_global Loss: 0.0995\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0947\n",
      "dev_local Loss: 0.1046\n",
      "dev_global Loss: 0.0997\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 0.0948\n",
      "dev_local Loss: 0.1046\n",
      "dev_global Loss: 0.0995\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 0.0947\n",
      "dev_local Loss: 0.1045\n",
      "dev_global Loss: 0.0995\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 0.0947\n",
      "dev_local Loss: 0.1044\n",
      "dev_global Loss: 0.0994\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 0.0947\n",
      "dev_local Loss: 0.1045\n",
      "dev_global Loss: 0.0997\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 0.0947\n",
      "dev_local Loss: 0.1047\n",
      "dev_global Loss: 0.0997\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 0.0947\n",
      "dev_local Loss: 0.1045\n",
      "dev_global Loss: 0.0995\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 0.0947\n",
      "dev_local Loss: 0.1045\n",
      "dev_global Loss: 0.0997\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 0.0947\n",
      "dev_local Loss: 0.1046\n",
      "dev_global Loss: 0.0997\n",
      "\n",
      "Training complete in 4m 46s\n",
      "Best epoch:  94\n",
      "Best dev loss: 0.0994\n"
     ]
    }
   ],
   "source": [
    "optim_policy_aug = T.optim.Adam(model_policy_aug.parameters(), lr=1e-4)\n",
    "model_policy_aug = train_policy_model(model_policy_aug, loss_func_policy, \n",
    "                                      optim_policy_aug, ld_policy_aug, \n",
    "                                      ld_policy_aug_sizes, phase_list, 100, \n",
    "                                      best_criterion='dev_global')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SC-BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2465,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_sigma = 0.1\n",
    "aug_ratio = 4\n",
    "noise_sigma_dev = 0.1\n",
    "aug_ratio_dev = 5\n",
    "# print(np.random.normal(0, noise_sigma, dX).astype(np.float32).dtype)\n",
    "\n",
    "demo_DS_Inv_aug_train = DS_Inv_aug(trajs_demo[0:10], noise_sigma, aug_ratio)\n",
    "demo_DS_Inv_aug_dev_local = DS_Inv_aug(trajs_demo[0:10], noise_sigma_dev, aug_ratio_dev, True)\n",
    "demo_DS_Inv_aug_dev_global = DS_Inv_aug(trajs_demo[10:20], noise_sigma, aug_ratio)\n",
    "\n",
    "ld_demo_aug_train = DataLoader(demo_DS_Inv_aug_train, batch_size=100)\n",
    "ld_demo_aug_dev_local = DataLoader(demo_DS_Inv_aug_dev_local, batch_size=100)\n",
    "ld_demo_aug_dev_global = DataLoader(demo_DS_Inv_aug_dev_global, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2466,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_demo_aug = {'train':ld_demo_aug_train, 'dev_local':ld_demo_aug_dev_local,\n",
    "               'dev_global':ld_demo_aug_dev_global}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2467,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_policy_aug_BC = {'train':[] , 'dev_local':[], 'dev_global':[]}\n",
    "\n",
    "for phase in ['train', 'dev_local', 'dev_global']:\n",
    "    for obs1, obs2, act, triple_index in ld_demo_aug[phase]:\n",
    "        out = model_inv.pred_inv(obs1.float().cuda(), obs2.float().cuda())\n",
    "\n",
    "        obs = obs1.cpu().detach().numpy()\n",
    "        out = out.cpu().detach().numpy()  \n",
    "        triple_index = triple_index.cpu().detach().numpy()\n",
    "        act = act.cpu().detach().numpy()  \n",
    "\n",
    "        for i in range(len(obs)):\n",
    "            if triple_index[i][0] < 0:\n",
    "                action = out[i]\n",
    "            else:\n",
    "                action = act[i]\n",
    "            traj_policy_aug_BC[phase].append([obs[i], action, triple_index[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2468,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_policy_aug_BC = {'train':DataLoader(DS_Policy(traj_policy_aug_BC['train']), batch_size=128, shuffle=True),\n",
    "                    'dev_local':DataLoader(DS_Policy(traj_policy_aug_BC['dev_local']), batch_size=128, shuffle=True),\n",
    "                    'dev_global':DataLoader(DS_Policy(traj_policy_aug_BC['dev_global']), batch_size=128, shuffle=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 391, 'dev_local': 391, 'dev_global': 391}\n"
     ]
    }
   ],
   "source": [
    "ld_policy_aug_sizes_BC = {'train':len(ld_policy_aug_BC['train']),\n",
    "                          'dev_local':len(ld_policy_aug_BC['dev_local']),\n",
    "                          'dev_global':len(ld_policy_aug_BC['dev_global']),}\n",
    "print(ld_policy_aug_sizes_BC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2470,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCO_policy_aug_BC(nn.Module):\n",
    "    def __init__(self, env, policy='mlp'):\n",
    "        super(BCO_policy_aug_BC, self).__init__()\n",
    "        \n",
    "        self.policy = policy\n",
    "        self.act_n = env.action_space.shape[0]\n",
    "        \n",
    "        init_ = lambda m: init(m, nn.init.xavier_normal_, lambda x: nn.init.\n",
    "                               constant_(x, 0), 0.5)\n",
    "        \n",
    "        if self.policy=='mlp':\n",
    "            self.obs_n = env.observation_space.shape[0]\n",
    "            self.pol = nn.Sequential(*[init_(nn.Linear(self.obs_n, 64)), nn.LeakyReLU(),\n",
    "                                       init_(nn.Linear(64, 64)), nn.LeakyReLU(),\n",
    "                                       init_(nn.Linear(64, self.act_n))])\n",
    "        \n",
    "        elif self.policy=='cnn':\n",
    "            pass\n",
    "    \n",
    "    def pred_act(self, obs):\n",
    "        out = self.pol(obs)\n",
    "        \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2471,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_policy_aug_BC = BCO_policy_aug_BC(env, policy=NN_option).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCO_policy_aug_BC(\n",
       "  (pol): Sequential(\n",
       "    (0): Linear(in_features=17, out_features=64, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=64, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_policy_aug_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2474,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_policy_aug_BC = T.optim.Adam(model_policy_aug_BC.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2475,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 0.2906\n",
      "dev_local Loss: 0.1951\n",
      "dev_global Loss: 0.1976\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 0.1836\n",
      "dev_local Loss: 0.1688\n",
      "dev_global Loss: 0.1750\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 0.1679\n",
      "dev_local Loss: 0.1582\n",
      "dev_global Loss: 0.1647\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 0.1598\n",
      "dev_local Loss: 0.1494\n",
      "dev_global Loss: 0.1576\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 0.1553\n",
      "dev_local Loss: 0.1486\n",
      "dev_global Loss: 0.1565\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 0.1524\n",
      "dev_local Loss: 0.1457\n",
      "dev_global Loss: 0.1537\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 0.1496\n",
      "dev_local Loss: 0.1408\n",
      "dev_global Loss: 0.1499\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 0.1475\n",
      "dev_local Loss: 0.1376\n",
      "dev_global Loss: 0.1481\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 0.1460\n",
      "dev_local Loss: 0.1391\n",
      "dev_global Loss: 0.1479\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.1446\n",
      "dev_local Loss: 0.1330\n",
      "dev_global Loss: 0.1462\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 0.1432\n",
      "dev_local Loss: 0.1338\n",
      "dev_global Loss: 0.1439\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.1423\n",
      "dev_local Loss: 0.1323\n",
      "dev_global Loss: 0.1433\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.1414\n",
      "dev_local Loss: 0.1315\n",
      "dev_global Loss: 0.1418\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.1405\n",
      "dev_local Loss: 0.1332\n",
      "dev_global Loss: 0.1430\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.1395\n",
      "dev_local Loss: 0.1307\n",
      "dev_global Loss: 0.1421\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 0.1389\n",
      "dev_local Loss: 0.1277\n",
      "dev_global Loss: 0.1413\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 0.1382\n",
      "dev_local Loss: 0.1270\n",
      "dev_global Loss: 0.1391\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 0.1372\n",
      "dev_local Loss: 0.1244\n",
      "dev_global Loss: 0.1378\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 0.1364\n",
      "dev_local Loss: 0.1270\n",
      "dev_global Loss: 0.1391\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 0.1360\n",
      "dev_local Loss: 0.1266\n",
      "dev_global Loss: 0.1388\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 0.1358\n",
      "dev_local Loss: 0.1243\n",
      "dev_global Loss: 0.1382\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 0.1350\n",
      "dev_local Loss: 0.1245\n",
      "dev_global Loss: 0.1371\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 0.1346\n",
      "dev_local Loss: 0.1278\n",
      "dev_global Loss: 0.1376\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 0.1340\n",
      "dev_local Loss: 0.1235\n",
      "dev_global Loss: 0.1359\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 0.1338\n",
      "dev_local Loss: 0.1229\n",
      "dev_global Loss: 0.1353\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 0.1332\n",
      "dev_local Loss: 0.1218\n",
      "dev_global Loss: 0.1346\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 0.1330\n",
      "dev_local Loss: 0.1239\n",
      "dev_global Loss: 0.1352\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 0.1327\n",
      "dev_local Loss: 0.1233\n",
      "dev_global Loss: 0.1365\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 0.1324\n",
      "dev_local Loss: 0.1226\n",
      "dev_global Loss: 0.1347\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 0.1321\n",
      "dev_local Loss: 0.1228\n",
      "dev_global Loss: 0.1356\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 0.1320\n",
      "dev_local Loss: 0.1198\n",
      "dev_global Loss: 0.1327\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 0.1312\n",
      "dev_local Loss: 0.1188\n",
      "dev_global Loss: 0.1328\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 0.1311\n",
      "dev_local Loss: 0.1229\n",
      "dev_global Loss: 0.1343\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 0.1310\n",
      "dev_local Loss: 0.1201\n",
      "dev_global Loss: 0.1329\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 0.1307\n",
      "dev_local Loss: 0.1193\n",
      "dev_global Loss: 0.1327\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 0.1303\n",
      "dev_local Loss: 0.1190\n",
      "dev_global Loss: 0.1334\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 0.1303\n",
      "dev_local Loss: 0.1214\n",
      "dev_global Loss: 0.1327\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 0.1298\n",
      "dev_local Loss: 0.1205\n",
      "dev_global Loss: 0.1334\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 0.1298\n",
      "dev_local Loss: 0.1228\n",
      "dev_global Loss: 0.1338\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 0.1297\n",
      "dev_local Loss: 0.1208\n",
      "dev_global Loss: 0.1326\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 0.1292\n",
      "dev_local Loss: 0.1185\n",
      "dev_global Loss: 0.1330\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 0.1294\n",
      "dev_local Loss: 0.1204\n",
      "dev_global Loss: 0.1345\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 0.1294\n",
      "dev_local Loss: 0.1209\n",
      "dev_global Loss: 0.1339\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 0.1289\n",
      "dev_local Loss: 0.1175\n",
      "dev_global Loss: 0.1321\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 0.1289\n",
      "dev_local Loss: 0.1178\n",
      "dev_global Loss: 0.1305\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 0.1286\n",
      "dev_local Loss: 0.1176\n",
      "dev_global Loss: 0.1309\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 0.1284\n",
      "dev_local Loss: 0.1180\n",
      "dev_global Loss: 0.1305\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 0.1281\n",
      "dev_local Loss: 0.1190\n",
      "dev_global Loss: 0.1320\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 0.1283\n",
      "dev_local Loss: 0.1156\n",
      "dev_global Loss: 0.1308\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 0.1276\n",
      "dev_local Loss: 0.1154\n",
      "dev_global Loss: 0.1304\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 0.1279\n",
      "dev_local Loss: 0.1177\n",
      "dev_global Loss: 0.1305\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 0.1275\n",
      "dev_local Loss: 0.1174\n",
      "dev_global Loss: 0.1304\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 0.1276\n",
      "dev_local Loss: 0.1159\n",
      "dev_global Loss: 0.1290\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 0.1274\n",
      "dev_local Loss: 0.1162\n",
      "dev_global Loss: 0.1298\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 0.1271\n",
      "dev_local Loss: 0.1171\n",
      "dev_global Loss: 0.1302\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 0.1272\n",
      "dev_local Loss: 0.1183\n",
      "dev_global Loss: 0.1319\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 0.1270\n",
      "dev_local Loss: 0.1210\n",
      "dev_global Loss: 0.1330\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 0.1270\n",
      "dev_local Loss: 0.1178\n",
      "dev_global Loss: 0.1304\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 0.1267\n",
      "dev_local Loss: 0.1164\n",
      "dev_global Loss: 0.1305\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 0.1267\n",
      "dev_local Loss: 0.1211\n",
      "dev_global Loss: 0.1347\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 0.1266\n",
      "dev_local Loss: 0.1147\n",
      "dev_global Loss: 0.1300\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 0.1264\n",
      "dev_local Loss: 0.1178\n",
      "dev_global Loss: 0.1307\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 0.1265\n",
      "dev_local Loss: 0.1142\n",
      "dev_global Loss: 0.1289\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 0.1263\n",
      "dev_local Loss: 0.1148\n",
      "dev_global Loss: 0.1293\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 0.1260\n",
      "dev_local Loss: 0.1150\n",
      "dev_global Loss: 0.1285\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 0.1261\n",
      "dev_local Loss: 0.1143\n",
      "dev_global Loss: 0.1294\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 0.1260\n",
      "dev_local Loss: 0.1137\n",
      "dev_global Loss: 0.1301\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 0.1260\n",
      "dev_local Loss: 0.1178\n",
      "dev_global Loss: 0.1308\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 0.1257\n",
      "dev_local Loss: 0.1180\n",
      "dev_global Loss: 0.1304\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 0.1257\n",
      "dev_local Loss: 0.1157\n",
      "dev_global Loss: 0.1295\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 0.1257\n",
      "dev_local Loss: 0.1142\n",
      "dev_global Loss: 0.1285\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 0.1255\n",
      "dev_local Loss: 0.1145\n",
      "dev_global Loss: 0.1290\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 0.1255\n",
      "dev_local Loss: 0.1154\n",
      "dev_global Loss: 0.1298\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 0.1253\n",
      "dev_local Loss: 0.1155\n",
      "dev_global Loss: 0.1291\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 0.1255\n",
      "dev_local Loss: 0.1139\n",
      "dev_global Loss: 0.1278\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 0.1253\n",
      "dev_local Loss: 0.1137\n",
      "dev_global Loss: 0.1280\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 0.1252\n",
      "dev_local Loss: 0.1158\n",
      "dev_global Loss: 0.1284\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 0.1251\n",
      "dev_local Loss: 0.1141\n",
      "dev_global Loss: 0.1297\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 0.1251\n",
      "dev_local Loss: 0.1148\n",
      "dev_global Loss: 0.1290\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 0.1249\n",
      "dev_local Loss: 0.1155\n",
      "dev_global Loss: 0.1306\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 0.1248\n",
      "dev_local Loss: 0.1150\n",
      "dev_global Loss: 0.1280\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 0.1248\n",
      "dev_local Loss: 0.1143\n",
      "dev_global Loss: 0.1288\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 0.1247\n",
      "dev_local Loss: 0.1129\n",
      "dev_global Loss: 0.1277\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 0.1249\n",
      "dev_local Loss: 0.1158\n",
      "dev_global Loss: 0.1288\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 0.1244\n",
      "dev_local Loss: 0.1166\n",
      "dev_global Loss: 0.1295\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 0.1246\n",
      "dev_local Loss: 0.1161\n",
      "dev_global Loss: 0.1286\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 0.1244\n",
      "dev_local Loss: 0.1154\n",
      "dev_global Loss: 0.1286\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 0.1244\n",
      "dev_local Loss: 0.1150\n",
      "dev_global Loss: 0.1288\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 0.1243\n",
      "dev_local Loss: 0.1154\n",
      "dev_global Loss: 0.1281\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 0.1245\n",
      "dev_local Loss: 0.1158\n",
      "dev_global Loss: 0.1296\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 0.1239\n",
      "dev_local Loss: 0.1141\n",
      "dev_global Loss: 0.1268\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1239\n",
      "dev_local Loss: 0.1139\n",
      "dev_global Loss: 0.1279\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 0.1240\n",
      "dev_local Loss: 0.1171\n",
      "dev_global Loss: 0.1294\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 0.1244\n",
      "dev_local Loss: 0.1137\n",
      "dev_global Loss: 0.1289\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 0.1239\n",
      "dev_local Loss: 0.1143\n",
      "dev_global Loss: 0.1285\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 0.1240\n",
      "dev_local Loss: 0.1141\n",
      "dev_global Loss: 0.1269\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 0.1239\n",
      "dev_local Loss: 0.1129\n",
      "dev_global Loss: 0.1272\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 0.1236\n",
      "dev_local Loss: 0.1151\n",
      "dev_global Loss: 0.1282\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 0.1239\n",
      "dev_local Loss: 0.1119\n",
      "dev_global Loss: 0.1274\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 0.1237\n",
      "dev_local Loss: 0.1124\n",
      "dev_global Loss: 0.1268\n",
      "\n",
      "Training complete in 4m 54s\n",
      "Best epoch:  90\n",
      "Best dev loss: 0.1268\n"
     ]
    }
   ],
   "source": [
    "phase_list = ['train', 'dev_local', 'dev_global']\n",
    "model_policy_aug_BC = train_policy_model(model_policy_aug_BC, loss_func_policy, \n",
    "                                         optim_policy_aug_BC, ld_policy_aug_BC, \n",
    "                                         ld_policy_aug_sizes_BC, phase_list, 100, \n",
    "                                         best_criterion='dev_global')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2476,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 0.1199\n",
      "dev_local Loss: 0.1094\n",
      "dev_global Loss: 0.1240\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 0.1196\n",
      "dev_local Loss: 0.1095\n",
      "dev_global Loss: 0.1239\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 0.1196\n",
      "dev_local Loss: 0.1096\n",
      "dev_global Loss: 0.1240\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 0.1196\n",
      "dev_local Loss: 0.1096\n",
      "dev_global Loss: 0.1240\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 0.1195\n",
      "dev_local Loss: 0.1096\n",
      "dev_global Loss: 0.1240\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 0.1195\n",
      "dev_local Loss: 0.1087\n",
      "dev_global Loss: 0.1239\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 0.1195\n",
      "dev_local Loss: 0.1093\n",
      "dev_global Loss: 0.1239\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 0.1195\n",
      "dev_local Loss: 0.1097\n",
      "dev_global Loss: 0.1241\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 0.1195\n",
      "dev_local Loss: 0.1094\n",
      "dev_global Loss: 0.1239\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.1194\n",
      "dev_local Loss: 0.1093\n",
      "dev_global Loss: 0.1240\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 0.1194\n",
      "dev_local Loss: 0.1093\n",
      "dev_global Loss: 0.1237\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.1194\n",
      "dev_local Loss: 0.1091\n",
      "dev_global Loss: 0.1238\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.1194\n",
      "dev_local Loss: 0.1094\n",
      "dev_global Loss: 0.1238\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.1194\n",
      "dev_local Loss: 0.1090\n",
      "dev_global Loss: 0.1238\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.1194\n",
      "dev_local Loss: 0.1092\n",
      "dev_global Loss: 0.1237\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 0.1194\n",
      "dev_local Loss: 0.1097\n",
      "dev_global Loss: 0.1238\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 0.1193\n",
      "dev_local Loss: 0.1093\n",
      "dev_global Loss: 0.1238\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 0.1193\n",
      "dev_local Loss: 0.1096\n",
      "dev_global Loss: 0.1240\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 0.1193\n",
      "dev_local Loss: 0.1097\n",
      "dev_global Loss: 0.1238\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 0.1193\n",
      "dev_local Loss: 0.1101\n",
      "dev_global Loss: 0.1239\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 0.1193\n",
      "dev_local Loss: 0.1092\n",
      "dev_global Loss: 0.1238\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 0.1192\n",
      "dev_local Loss: 0.1099\n",
      "dev_global Loss: 0.1239\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 0.1193\n",
      "dev_local Loss: 0.1099\n",
      "dev_global Loss: 0.1238\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 0.1193\n",
      "dev_local Loss: 0.1094\n",
      "dev_global Loss: 0.1237\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 0.1192\n",
      "dev_local Loss: 0.1090\n",
      "dev_global Loss: 0.1238\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 0.1192\n",
      "dev_local Loss: 0.1096\n",
      "dev_global Loss: 0.1240\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 0.1192\n",
      "dev_local Loss: 0.1098\n",
      "dev_global Loss: 0.1240\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 0.1192\n",
      "dev_local Loss: 0.1089\n",
      "dev_global Loss: 0.1237\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 0.1192\n",
      "dev_local Loss: 0.1093\n",
      "dev_global Loss: 0.1237\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 0.1192\n",
      "dev_local Loss: 0.1093\n",
      "dev_global Loss: 0.1237\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 0.1191\n",
      "dev_local Loss: 0.1093\n",
      "dev_global Loss: 0.1236\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 0.1191\n",
      "dev_local Loss: 0.1091\n",
      "dev_global Loss: 0.1236\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 0.1191\n",
      "dev_local Loss: 0.1094\n",
      "dev_global Loss: 0.1238\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 0.1191\n",
      "dev_local Loss: 0.1087\n",
      "dev_global Loss: 0.1236\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 0.1191\n",
      "dev_local Loss: 0.1096\n",
      "dev_global Loss: 0.1239\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 0.1191\n",
      "dev_local Loss: 0.1094\n",
      "dev_global Loss: 0.1235\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 0.1191\n",
      "dev_local Loss: 0.1095\n",
      "dev_global Loss: 0.1238\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 0.1191\n",
      "dev_local Loss: 0.1091\n",
      "dev_global Loss: 0.1236\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 0.1191\n",
      "dev_local Loss: 0.1086\n",
      "dev_global Loss: 0.1236\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 0.1190\n",
      "dev_local Loss: 0.1100\n",
      "dev_global Loss: 0.1237\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 0.1190\n",
      "dev_local Loss: 0.1098\n",
      "dev_global Loss: 0.1239\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 0.1190\n",
      "dev_local Loss: 0.1092\n",
      "dev_global Loss: 0.1236\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 0.1190\n",
      "dev_local Loss: 0.1088\n",
      "dev_global Loss: 0.1235\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 0.1190\n",
      "dev_local Loss: 0.1099\n",
      "dev_global Loss: 0.1237\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 0.1190\n",
      "dev_local Loss: 0.1095\n",
      "dev_global Loss: 0.1235\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 0.1190\n",
      "dev_local Loss: 0.1086\n",
      "dev_global Loss: 0.1236\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 0.1190\n",
      "dev_local Loss: 0.1094\n",
      "dev_global Loss: 0.1237\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 0.1189\n",
      "dev_local Loss: 0.1088\n",
      "dev_global Loss: 0.1237\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 0.1189\n",
      "dev_local Loss: 0.1088\n",
      "dev_global Loss: 0.1235\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 0.1190\n",
      "dev_local Loss: 0.1089\n",
      "dev_global Loss: 0.1234\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 0.1189\n",
      "dev_local Loss: 0.1090\n",
      "dev_global Loss: 0.1234\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 0.1189\n",
      "dev_local Loss: 0.1094\n",
      "dev_global Loss: 0.1235\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 0.1189\n",
      "dev_local Loss: 0.1093\n",
      "dev_global Loss: 0.1236\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 0.1189\n",
      "dev_local Loss: 0.1095\n",
      "dev_global Loss: 0.1239\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 0.1189\n",
      "dev_local Loss: 0.1088\n",
      "dev_global Loss: 0.1239\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 0.1189\n",
      "dev_local Loss: 0.1086\n",
      "dev_global Loss: 0.1234\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 0.1189\n",
      "dev_local Loss: 0.1092\n",
      "dev_global Loss: 0.1237\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 0.1189\n",
      "dev_local Loss: 0.1095\n",
      "dev_global Loss: 0.1237\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 0.1189\n",
      "dev_local Loss: 0.1092\n",
      "dev_global Loss: 0.1236\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 0.1189\n",
      "dev_local Loss: 0.1091\n",
      "dev_global Loss: 0.1236\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 0.1188\n",
      "dev_local Loss: 0.1087\n",
      "dev_global Loss: 0.1235\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 0.1188\n",
      "dev_local Loss: 0.1097\n",
      "dev_global Loss: 0.1237\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 0.1189\n",
      "dev_local Loss: 0.1087\n",
      "dev_global Loss: 0.1235\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 0.1188\n",
      "dev_local Loss: 0.1088\n",
      "dev_global Loss: 0.1234\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 0.1188\n",
      "dev_local Loss: 0.1089\n",
      "dev_global Loss: 0.1234\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 0.1188\n",
      "dev_local Loss: 0.1092\n",
      "dev_global Loss: 0.1235\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 0.1188\n",
      "dev_local Loss: 0.1090\n",
      "dev_global Loss: 0.1235\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 0.1188\n",
      "dev_local Loss: 0.1091\n",
      "dev_global Loss: 0.1234\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 0.1187\n",
      "dev_local Loss: 0.1091\n",
      "dev_global Loss: 0.1234\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 0.1187\n",
      "dev_local Loss: 0.1087\n",
      "dev_global Loss: 0.1236\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 0.1188\n",
      "dev_local Loss: 0.1088\n",
      "dev_global Loss: 0.1234\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 0.1187\n",
      "dev_local Loss: 0.1092\n",
      "dev_global Loss: 0.1234\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 0.1187\n",
      "dev_local Loss: 0.1086\n",
      "dev_global Loss: 0.1234\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 0.1187\n",
      "dev_local Loss: 0.1084\n",
      "dev_global Loss: 0.1234\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 0.1187\n",
      "dev_local Loss: 0.1095\n",
      "dev_global Loss: 0.1236\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 0.1188\n",
      "dev_local Loss: 0.1086\n",
      "dev_global Loss: 0.1235\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 0.1187\n",
      "dev_local Loss: 0.1083\n",
      "dev_global Loss: 0.1236\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 0.1187\n",
      "dev_local Loss: 0.1085\n",
      "dev_global Loss: 0.1235\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 0.1187\n",
      "dev_local Loss: 0.1085\n",
      "dev_global Loss: 0.1233\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 0.1187\n",
      "dev_local Loss: 0.1085\n",
      "dev_global Loss: 0.1233\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 0.1187\n",
      "dev_local Loss: 0.1092\n",
      "dev_global Loss: 0.1234\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 0.1187\n",
      "dev_local Loss: 0.1090\n",
      "dev_global Loss: 0.1234\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 0.1187\n",
      "dev_local Loss: 0.1081\n",
      "dev_global Loss: 0.1233\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 0.1186\n",
      "dev_local Loss: 0.1094\n",
      "dev_global Loss: 0.1234\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 0.1186\n",
      "dev_local Loss: 0.1091\n",
      "dev_global Loss: 0.1234\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 0.1186\n",
      "dev_local Loss: 0.1090\n",
      "dev_global Loss: 0.1233\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 0.1186\n",
      "dev_local Loss: 0.1082\n",
      "dev_global Loss: 0.1233\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 0.1186\n",
      "dev_local Loss: 0.1086\n",
      "dev_global Loss: 0.1232\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 0.1186\n",
      "dev_local Loss: 0.1089\n",
      "dev_global Loss: 0.1233\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 0.1186\n",
      "dev_local Loss: 0.1085\n",
      "dev_global Loss: 0.1234\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 0.1185\n",
      "dev_local Loss: 0.1091\n",
      "dev_global Loss: 0.1235\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1186\n",
      "dev_local Loss: 0.1088\n",
      "dev_global Loss: 0.1233\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 0.1186\n",
      "dev_local Loss: 0.1085\n",
      "dev_global Loss: 0.1232\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 0.1186\n",
      "dev_local Loss: 0.1090\n",
      "dev_global Loss: 0.1234\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 0.1185\n",
      "dev_local Loss: 0.1088\n",
      "dev_global Loss: 0.1238\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 0.1185\n",
      "dev_local Loss: 0.1090\n",
      "dev_global Loss: 0.1232\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 0.1186\n",
      "dev_local Loss: 0.1085\n",
      "dev_global Loss: 0.1234\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 0.1185\n",
      "dev_local Loss: 0.1088\n",
      "dev_global Loss: 0.1232\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 0.1186\n",
      "dev_local Loss: 0.1088\n",
      "dev_global Loss: 0.1233\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 0.1185\n",
      "dev_local Loss: 0.1086\n",
      "dev_global Loss: 0.1233\n",
      "\n",
      "Training complete in 4m 57s\n",
      "Best epoch:  97\n",
      "Best dev loss: 0.1232\n"
     ]
    }
   ],
   "source": [
    "optim_policy_aug_BC = T.optim.Adam(model_policy_aug_BC.parameters(), lr=1e-4)\n",
    "model_policy_aug_BC = train_policy_model(model_policy_aug_BC, loss_func_policy, \n",
    "                                         optim_policy_aug_BC, ld_policy_aug_BC, \n",
    "                                         ld_policy_aug_sizes_BC, phase_list, 100, \n",
    "                                         best_criterion='dev_global')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average error variation along demo trajectories (BC vs BCO vs SC-BCO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mimic_trajs(model, model_name, traj_num=10, horizon=1000, start_step=0, random_init=False):\n",
    "    trajs_mimic = []\n",
    "    st_info = []\n",
    "    rew_info = []\n",
    "    cnt = 0\n",
    "    epn = traj_num\n",
    "    rews = 0.0\n",
    "\n",
    "    for traj_index in range(epn):\n",
    "        traj = []\n",
    "        traj_st = []\n",
    "        traj_rew = []\n",
    "        rew = 0.0\n",
    "\n",
    "        if random_init:\n",
    "            obs = env.reset()\n",
    "            st = env.sim.get_state()\n",
    "            traj_st.append(st)\n",
    "        else:\n",
    "            st = demo_st_info[traj_index][start_step]\n",
    "            # print(st)\n",
    "            env.sim.set_state(st)\n",
    "            obs = trajs_demo[traj_index][start_step][0]\n",
    "            traj_st.append(st)\n",
    "            \n",
    "        for step in range(horizon):\n",
    "            obs_norm = O_normalizing(obs)\n",
    "            act_norm = model.pred_act(T.Tensor([obs_norm]).cuda())\n",
    "\n",
    "            act_norm = act_norm.cpu().detach().numpy()[0]\n",
    "            # print(act_norm)\n",
    "            act = A_recovering(act_norm)\n",
    "            # print(act)\n",
    "            \n",
    "            new_obs, r, done, _ = env.step(act)\n",
    "\n",
    "            traj.append([obs, act, new_obs])\n",
    "            traj_rew.append(r)\n",
    "            # print(r)\n",
    "\n",
    "            obs = new_obs\n",
    "            rew += r\n",
    "            cnt += 1\n",
    "            st = env.sim.get_state()\n",
    "            traj_st.append(st)\n",
    "            # print(done)\n",
    "\n",
    "        # print(rew)\n",
    "        rews += rew\n",
    "        trajs_mimic.append(traj)\n",
    "        st_info.append(traj_st)\n",
    "        rew_info.append(traj_rew)\n",
    "\n",
    "    rews /= epn\n",
    "    print(len(trajs_mimic), len(trajs_mimic[0]))\n",
    "    print('average reward of {} {} trajs: {:.2f}'.format(epn, model_name, rews))\n",
    "    \n",
    "    return trajs_mimic, st_info, rew_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2350,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1775.5519 1759.2698 1814.5062 1800.4143 1821.2463 1799.949  1822.4805\n",
      " 1800.2778 1778.0415 1794.0184]\n"
     ]
    }
   ],
   "source": [
    "rews_list = np.sum(demo_rew_info[0:10], axis=1)\n",
    "print(rews_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward of 10 expert trajs: 1796.58\n"
     ]
    }
   ],
   "source": [
    "expert_rews = np.sum(demo_rew_info[0:10])\n",
    "print('average reward of {} {} trajs: {:.2f}'.format(10, 'expert', expert_rews / 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1000\n",
      "average reward of 10 BC trajs: 1784.78\n"
     ]
    }
   ],
   "source": [
    "trajs_BC, BC_st_info, BC_rew_info = get_mimic_trajs(model_policy_BC, 'BC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2361,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.05514779  0.01413077  0.05431647 -0.07718844 -0.00049597 -0.09359713\n",
      "  0.0420625   0.0338486  -0.04637919 -0.07595596  0.0203414   0.03111221\n",
      "  0.09077952 -0.21187306  0.09768369  0.06454459 -0.02387658]\n",
      "[-0.05514779  0.01413077  0.05431647 -0.07718844 -0.00049597 -0.09359713\n",
      "  0.0420625   0.0338486  -0.04637919 -0.07595596  0.0203414   0.03111221\n",
      "  0.09077952 -0.21187306  0.09768369  0.06454459 -0.02387658]\n",
      "[ 4.6757808e-01  1.4283210e-03  6.5947473e-01  1.8217321e-01\n",
      "  1.4451610e+00 -5.1731670e-01]\n",
      "[ 0.42033994  0.02886429  0.6486292   0.16582847  1.4224373  -0.49327943]\n",
      "[ 0.17475496  0.3421933   1.0769335   0.12584761  2.720655   -1.8464307 ]\n",
      "[ 0.09094691  0.373872    1.0461167   0.10510217  2.6799183  -1.7665071 ]\n",
      "[-0.07202693 -0.01772098  0.15221099 -0.03565283  0.23121198 -0.01841621\n",
      "  0.33170059 -0.1662426   0.49294502 -0.50951452 -0.75886748  2.57497647\n",
      "  1.62760945  5.13046556  2.27734184  7.41693052 -5.22591411]\n",
      "[-0.07302701 -0.01909278  0.13912533 -0.02592187  0.23464571 -0.02502976\n",
      "  0.3329457  -0.15887763  0.42766601 -0.57785546 -0.88940909  2.4258476\n",
      "  1.4643304   5.67592225  2.14057316  7.4549319  -5.02186238]\n",
      "MjSimState(time=0.0, qpos=array([ 0.0665112 , -0.05514779,  0.01413077,  0.05431647, -0.07718844,\n",
      "       -0.00049597, -0.09359713,  0.0420625 ,  0.0338486 ]), qvel=array([-0.04637919, -0.07595596,  0.0203414 ,  0.03111221,  0.09077952,\n",
      "       -0.21187306,  0.09768369,  0.06454459, -0.02387658]), act=None, udd_state={})\n",
      "[2.910793454727229, 0.3523472492690871, 0.952804883147769, 3.184330085014699, 3.215617035638661, 2.9433630524468186, 2.0631023160025164, 0.9858284182859847, 2.253924152622426, -0.11030872365187183, 1.0260160108213654, 3.233057156105599, 3.0172516751761123, 2.519657250729358, 1.8909119414146516, 2.1428750959438845, 2.44582347105262, -0.18437406894790914, 0.9736597815406526, 3.256898950429206, 3.0933991241258925, 2.668765125573452, 1.9344046585643242, 1.2616386028342732, 2.062690008620211, -0.0985587152415292, 1.0611311601203623, 3.165511412350315, 2.9149209089910415, 2.450643196652732, 1.8402698386248404, 2.0740050291982244, 2.771311615142281, -0.278268419655899, 0.7178998380392272, 3.0632053574604696, 3.039320283558402, 2.7325027298989424, 1.9819696599551322, 0.8975262097746621, 2.0471234764679593, 0.10383108093172949, 0.5844782257763768, 3.0025697193291876, 2.9830891297049336, 2.585540638321379, 2.1869090591650773, 2.076890193651002, 2.7703828720519597, 2.357639431161965]\n"
     ]
    }
   ],
   "source": [
    "print(trajs_demo[0][0][0])\n",
    "print(trajs_BC[0][0][0])\n",
    "print(trajs_demo[0][0][1])\n",
    "print(trajs_BC[0][0][1])\n",
    "print(A_normalizing(trajs_demo[0][0][1]))\n",
    "print(A_normalizing(trajs_BC[0][0][1]))\n",
    "print(trajs_demo[0][0][2])\n",
    "print(trajs_BC[0][0][2])\n",
    "print(BC_st_info[0][0])\n",
    "print(BC_rew_info[0][900:950])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1000\n",
      "average reward of 10 BC trajs: 1788.89\n"
     ]
    }
   ],
   "source": [
    "trajs_BC_r, BC_st_info_r, BC_rew_info_r = get_mimic_trajs(model_policy_BC, 'BC', random_init=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1000\n",
      "average reward of 10 BCO trajs: 1502.79\n"
     ]
    }
   ],
   "source": [
    "trajs_BCO, BCO_st_info, BCO_rew_info = get_mimic_trajs(model_policy, 'BCO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1000\n",
      "average reward of 10 BCO trajs: 1508.25\n"
     ]
    }
   ],
   "source": [
    "trajs_BCO_r, BCO_st_info_r, BCO_rew_info_r = get_mimic_trajs(model_policy, 'BCO', random_init=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SC-BCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1000\n",
      "average reward of 10 SC-BCO trajs: 1494.63\n"
     ]
    }
   ],
   "source": [
    "trajs_SCBCO, SCBCO_st_info, SCBCO_rew_info = get_mimic_trajs(model_policy_aug, 'SC-BCO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1000\n",
      "average reward of 10 SC-BCO trajs: 1502.10\n"
     ]
    }
   ],
   "source": [
    "trajs_SCBCO_r, SCBCO_st_info_r, SCBCO_rew_info_r = get_mimic_trajs(model_policy_aug, 'SC-BCO',\n",
    "                                                                   random_init=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SC-BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1000\n",
      "average reward of 10 SC-BC trajs: 1511.55\n"
     ]
    }
   ],
   "source": [
    "trajs_SCBC, SCBC_st_info, SCBC_rew_info = get_mimic_trajs(model_policy_aug_BC, 'SC-BC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1000\n",
      "average reward of 10 SC-BC trajs: 1520.58\n"
     ]
    }
   ],
   "source": [
    "trajs_SCBC_r, SCBC_st_info_r, SCBC_rew_info_r = get_mimic_trajs(model_policy_aug_BC, 'SC-BC',\n",
    "                                                                random_init=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### error-time_step graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_seq(trajs_mimic, traj_num=10, horizon=1000):\n",
    "    error_trajs = []\n",
    "    for t in range(traj_num):\n",
    "        error_traj = []\n",
    "        for s in range(horizon):\n",
    "            error_traj.append(np.mean(np.fabs(O_normalizing(trajs_demo[t][s][0])\n",
    "                                             - O_normalizing(trajs_mimic[t][s][0]))))\n",
    "        error_trajs.append(error_traj)\n",
    "    return np.mean(error_trajs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1090,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.23606797749979"
      ]
     },
     "execution_count": 1090,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm([1,1,1,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.03709429 0.06066196 0.0661917  0.07986158 0.09171651\n",
      " 0.10687396 0.10951049 0.12491487 0.1386254  0.17556303 0.1479805\n",
      " 0.16397097 0.16302392 0.18410762 0.1885767  0.19223864 0.19893697\n",
      " 0.23363699 0.28842538 0.36925132 0.45389676 0.5374315  0.54692789\n",
      " 0.54251717 0.52005354 0.49146955 0.53328538 0.60725863 0.67209322\n",
      " 0.77528877 0.76554828 0.76220382 0.6945462  0.6313261  0.61021354\n",
      " 0.61808325 0.71116527 0.80454982 0.90790527 0.74632317 0.65894932\n",
      " 0.56071076 0.55615193 0.65297805 0.68701146 0.82554504 0.79929748\n",
      " 0.7106361  0.57492339 0.49869279 0.53479559 0.57506404 0.68211924\n",
      " 0.80459807 0.84319659 0.74038203 0.68990208 0.54941857 0.53103932\n",
      " 0.64938996 0.74687221 0.83448913 0.81108926 0.710002   0.59868549\n",
      " 0.54075962 0.5541457  0.65948345 0.72730599 0.85126388 0.88328108\n",
      " 0.71646403 0.61429286 0.57581142 0.57467808 0.66456999 0.71451617\n",
      " 0.82816436 0.85048298 0.71756291 0.64159659 0.5762558  0.58656519\n",
      " 0.62911628 0.73627928 0.85611397 0.89270981 0.74528871 0.69491735\n",
      " 0.6160101  0.58996396 0.69088869 0.80688409 0.94442424 0.93248825\n",
      " 0.75999365 0.64371893 0.60636766 0.63270141 0.72553821 0.83249596\n",
      " 0.9539752  0.98294092 0.73276625 0.62586404 0.56984818 0.64648678\n",
      " 0.73685889 0.8547724  0.94872122 0.94124525 0.733027   0.60635827\n",
      " 0.54918759 0.63679541 0.68378463 0.85091291 0.98077962 0.96243262\n",
      " 0.83408573 0.69363414 0.63256721 0.62498442 0.75312995 0.87770381\n",
      " 0.9992558  1.09888984 0.96026012 0.85454638 0.81825594 0.69665468\n",
      " 0.82662626 0.91919899 1.0549461  1.0914497  0.92005479 0.83436834\n",
      " 0.73355018 0.69426512 0.76639216 0.85178661 0.99020594 0.99203406\n",
      " 0.88494564 0.83315951 0.75174244 0.69667114 0.79652075 0.77783303\n",
      " 0.95930805 0.96129702 0.94196013 0.81675557 0.76746786 0.63882431\n",
      " 0.68602179 0.77471384 0.91140285 0.90100881 0.81607121 0.67700624\n",
      " 0.65068284 0.62798105 0.65315181 0.73701461 0.80289941 0.83710166\n",
      " 0.65960675 0.61119358 0.51092969 0.55216952 0.61871743 0.70545401\n",
      " 0.82874186 0.78282544 0.63422601 0.55819762 0.52598705 0.52290804\n",
      " 0.5752161  0.66724895 0.78829238 0.75812418 0.69197621 0.62804797\n",
      " 0.53397572 0.48693254 0.61930693 0.66816338 0.82401217 0.75004109\n",
      " 0.629876   0.53694951 0.49352835 0.47674196 0.56603111 0.6127996\n",
      " 0.72497748 0.65205647 0.67071739 0.60100476 0.5405021  0.57901707\n",
      " 0.61648718 0.69680578 0.79057264 0.76253428 0.67459057 0.72024117\n",
      " 0.56963132 0.58992601 0.65606389 0.75219566 0.90081452 0.84917011\n",
      " 0.65454691 0.7646188  0.73168658 0.67326348 0.74341843 0.81640797\n",
      " 0.9230619  0.86845754 0.7540047  0.73852359 0.74855628 0.68685852\n",
      " 0.72947189 0.80643691 0.93586249 0.99074283 0.88881541 0.84843151\n",
      " 0.83486298 0.71337948 0.79957211 0.85788851 1.00403658 1.04583101\n",
      " 0.95256421 0.90998484 0.91410164 0.82559032 0.85920716 0.92528446\n",
      " 1.03634521 1.07352722 0.95952523 0.92791105 0.84917614 0.76209338\n",
      " 0.83756196 0.89198854 1.03920419 1.03268882 0.94340189 0.88647318\n",
      " 0.77356085 0.69294669 0.741329   0.79443229 1.01050843 0.97932825\n",
      " 0.87630895 0.7946178  0.74758373 0.66860754 0.76540774 0.84900255\n",
      " 1.00518926 1.07946583 1.02021838 0.87489201 0.78491861 0.72814578\n",
      " 0.80459422 0.91117306 1.09743828 1.1321619  0.96783247 0.85214868\n",
      " 0.79137834 0.72476371 0.86359466 0.95033176 1.12474496 1.0723907\n",
      " 0.97888312 0.86607396 0.8009505  0.71644745 0.87019265 0.94523045\n",
      " 1.13182161 1.06013475 0.97102561 0.88898099 0.83345666 0.73947212\n",
      " 0.8678925  0.90794477 1.11143415 1.07885668 0.97535723 0.90380432\n",
      " 0.82376045 0.73508633 0.82738177 0.87730763 1.06926405 1.01219715\n",
      " 0.89958966 0.83407516 0.75254152 0.72103634 0.89419416 0.88935406\n",
      " 1.11652054 1.07147939 0.96768662 0.94611005 0.82489467 0.72520209\n",
      " 0.88971802 0.92826409 1.24530366 1.14641608 1.04505401 0.94891078\n",
      " 0.857156   0.77978371 0.92013649 1.00964979 1.23826908 1.15834424\n",
      " 1.08613155 1.00753618 0.88717886 0.86085903 0.98086922 1.07071985\n",
      " 1.23902794 1.16668503 1.1040816  0.99783277 0.8690212  0.8324728\n",
      " 0.96377201 1.05458802 1.20398311 1.14478765 1.00314071 0.94888036\n",
      " 0.8117826  0.80745472 0.8853313  1.00691582 1.11677437 1.09763056\n",
      " 0.91002303 0.81607526 0.76232094 0.78043073 0.89952853 1.01407745\n",
      " 1.09282003 0.96789551 0.85649573 0.81226491 0.75514681 0.78194538\n",
      " 0.87777118 0.97870078 1.06127507 0.97280262 0.89585743 0.8364145\n",
      " 0.72590193 0.7495894  0.85593652 0.9542405  1.01299664 0.9654967\n",
      " 0.82574792 0.75789422 0.72286018 0.75719149 0.79140858 0.90912153\n",
      " 0.96618864 0.91197956 0.70477837 0.67478328 0.64393813 0.74802391\n",
      " 0.81372703 0.89058894 0.92505056 0.86032944 0.71934998 0.71385671\n",
      " 0.61541595 0.66651564 0.76837568 0.91365595 0.86780947 0.87166509\n",
      " 0.7432041  0.70019796 0.62800116 0.64598647 0.77881648 0.852139\n",
      " 0.87634542 0.89527633 0.76445001 0.73510004 0.64946922 0.67570058\n",
      " 0.75512337 0.88614654 0.98921498 0.93401052 0.81098515 0.77830952\n",
      " 0.69632115 0.7078779  0.83432398 0.9092962  0.98149199 0.97386978\n",
      " 0.79830813 0.79084056 0.68216962 0.73061896 0.84589793 1.00179123\n",
      " 1.08298457 1.09557722 0.92086491 0.84170326 0.75023874 0.80126839\n",
      " 0.82392634 0.94384883 1.09625242 1.06786233 0.88872721 0.8286498\n",
      " 0.71100764 0.69332316 0.81176071 0.9502904  1.07105341 0.97168959\n",
      " 0.82681297 0.7898382  0.69635652 0.79412698 0.90151172 1.03986068\n",
      " 1.09929675 1.04944825 0.87127272 0.85167739 0.73398028 0.81355829\n",
      " 0.91620053 1.01876377 1.07878439 0.9868124  0.87238152 0.89224584\n",
      " 0.73686443 0.74254899 0.84870611 0.97750128 1.0524856  0.96014209\n",
      " 0.83175942 0.85684025 0.75498685 0.77184504 0.75275325 0.91048034\n",
      " 0.93451077 0.85163281 0.77524167 0.80606419 0.70546265 0.65863841\n",
      " 0.80371282 0.83929021 0.98100787 0.96332851 0.84709875 0.79901451\n",
      " 0.72973594 0.69712662 0.87860228 0.93041436 1.13541458 1.06673217\n",
      " 0.92347113 0.84751749 0.7797589  0.74339829 0.88952752 1.00249083\n",
      " 1.1667125  1.15510751 0.94616015 0.90413021 0.74701987 0.72186577\n",
      " 0.84368745 0.95602141 1.10425053 1.05983395 0.83030338 0.79329801\n",
      " 0.6983574  0.69815888 0.79677887 0.9677872  1.05564517 1.07586803\n",
      " 0.85430496 0.80794691 0.70227472 0.72391173 0.79187822 0.91204032\n",
      " 1.04834291 1.01805887 0.87034751 0.77969824 0.70698325 0.73203639\n",
      " 0.83278399 0.98940817 1.11772965 1.09784058 0.86800332 0.84634021\n",
      " 0.69419689 0.72040582 0.86072102 1.02998716 1.11743618 1.04747072\n",
      " 0.8609194  0.82733932 0.77125271 0.75430289 0.91422905 1.1015483\n",
      " 1.30293789 1.11533217 0.8841963  0.8965619  0.77108023 0.7467993\n",
      " 0.86763141 0.94921751 1.11414919 1.046357   0.90977304 0.89474428\n",
      " 0.74463101 0.76732252 0.9323915  1.04807504 1.19362728 1.05567424\n",
      " 1.03261407 0.92661118 0.85091892 0.84110307 0.97949537 1.10454715\n",
      " 1.23744778 1.10036355 0.9984125  0.94274275 0.7928496  0.86716522\n",
      " 0.93191864 1.13527131 1.26595549 1.10100818 1.01513055 0.93758571\n",
      " 0.78300901 0.81596469 0.90940825 1.05037313 1.21927666 1.07246829\n",
      " 1.00314746 0.93085925 0.79365613 0.83824512 0.9439958  1.07434346\n",
      " 1.15352522 1.07228536 0.97073678 0.91720793 0.73802314 0.77197013\n",
      " 0.88517322 1.06698885 1.16643584 1.07252535 0.96039425 0.94144887\n",
      " 0.74565047 0.71136584 0.88631015 0.92457032 1.08514633 0.96374693\n",
      " 0.86646631 0.81329227 0.68167851 0.73289272 0.88534628 0.98554695\n",
      " 1.09216624 1.07112782 0.98415152 0.88678732 0.7651558  0.72545101\n",
      " 0.85631143 0.89478982 1.06116772 1.00995378 0.93372714 0.85528912\n",
      " 0.76296748 0.82186888 0.91477302 1.06357366 1.23056242 1.07027325\n",
      " 1.00394747 0.94431517 0.74970018 0.7985427  0.85060348 1.03011086\n",
      " 1.10108248 0.93692272 0.82464569 0.81173669 0.69421312 0.75008685\n",
      " 0.82063236 1.00082187 1.1068598  0.9134359  0.80986401 0.76369548\n",
      " 0.65703304 0.75031115 0.83187509 0.97161886 1.04939594 0.79576708\n",
      " 0.73405489 0.68622431 0.60732164 0.67500953 0.80025391 0.92061262\n",
      " 1.02840722 0.7701523  0.65561678 0.70483956 0.60145336 0.6740009\n",
      " 0.77604677 0.99044161 1.01414657 0.8275307  0.69957005 0.69468525\n",
      " 0.60495189 0.71853314 0.78178941 0.96080777 1.0088645  0.83463948\n",
      " 0.73601895 0.75961762 0.63927071 0.75013507 0.86278667 0.98906459\n",
      " 1.11802598 0.88012152 0.77749489 0.71562391 0.64045987 0.73649809\n",
      " 0.80897418 0.99734285 1.03896415 0.78069545 0.71133728 0.72269281\n",
      " 0.64616749 0.75360261 0.87564522 0.9949867  1.15251729 0.86728895\n",
      " 0.75106775 0.72865935 0.65238777 0.69679435 0.79625221 1.00888722\n",
      " 1.09148668 0.91327114 0.81401258 0.75133161 0.6618597  0.72560895\n",
      " 0.82041155 0.9893754  1.07732668 0.93457137 0.82842369 0.76207696\n",
      " 0.67151984 0.7430133  0.7622934  0.95741903 1.03039056 0.87354363\n",
      " 0.80784336 0.77793785 0.66864106 0.69861146 0.76456756 0.95326315\n",
      " 1.02485473 0.89085851 0.84320876 0.78474705 0.64931037 0.71420283\n",
      " 0.81651831 0.94906621 1.02285741 0.94978181 0.84270719 0.78558257\n",
      " 0.68414577 0.70329709 0.78244355 0.89914019 1.02574177 0.89184509\n",
      " 0.86123361 0.80817746 0.73947682 0.73118021 0.81814202 0.94051439\n",
      " 1.07346797 0.93791109 0.87462197 0.81589846 0.72668747 0.7343067\n",
      " 0.80781107 0.93711948 1.04514178 0.87638198 0.76610377 0.73849402\n",
      " 0.64080059 0.70444089 0.79173027 0.94548114 0.93352648 0.78191269\n",
      " 0.73069729 0.68829108 0.59935877 0.65580065 0.71488192 0.87438896\n",
      " 0.90643005 0.7517668  0.76671264 0.68586072 0.60301216 0.63800824\n",
      " 0.72271814 0.85233289 0.75735157 0.78956903 0.71958601 0.71537349\n",
      " 0.58006295 0.67258669 0.68826562 0.8492103  0.83273836 0.81839213\n",
      " 0.81166844 0.78548386 0.64176149 0.75371855 0.76188254 1.00098919\n",
      " 0.97805919 0.84425109 0.78727088 0.77297385 0.64195598 0.69727522\n",
      " 0.78514628 0.96474783 0.92738801 0.84191883 0.79884264 0.77870683\n",
      " 0.56082345 0.64903526 0.70763544 0.95576901 0.92798391 0.86388758\n",
      " 0.71771228 0.75537766 0.59001275 0.61353955 0.69603734 0.8634677\n",
      " 0.80997098 0.76981066 0.70347294 0.78425071 0.7087722  0.68364175\n",
      " 0.75629877 0.94068007 0.87711184 0.79094784 0.73413648 0.74135028\n",
      " 0.62471575 0.72020963 0.79299955 1.01905707 0.99056647 0.95877848\n",
      " 0.87372757 0.88361839 0.7270378  0.71829894 0.81116524 0.99601182\n",
      " 0.99843286 0.96720333 0.82514184 0.83046833 0.65198621 0.68994192\n",
      " 0.7738148  0.9822577  0.951009   0.86320037 0.77837971 0.7954203\n",
      " 0.63174597 0.64717668 0.70432939 0.89045015 0.89730405 0.81189619\n",
      " 0.77555007 0.83865486 0.63417374 0.63878838 0.66404053 0.91571489\n",
      " 0.8833439  0.91313593 0.79150235 0.79488177 0.65575003 0.64770346\n",
      " 0.70499224 0.94550881 0.90341388 0.84729783 0.77430408 0.70024825\n",
      " 0.5942617  0.65671075 0.7267408  0.91154773 0.89945124 0.8911224\n",
      " 0.74035695 0.67187258 0.58072154 0.64492758 0.73088821 0.84061545\n",
      " 0.83653187 0.87130229 0.8221344  0.66013966 0.55492819 0.64826138\n",
      " 0.68721765 0.93169299 0.87861119 0.93869023 0.82607908 0.70476825\n",
      " 0.63253322 0.70458075 0.77148081 0.96460294 1.03290882 1.052579\n",
      " 0.89234113 0.75000163 0.64993508 0.72496948 0.74887713 0.96854336\n",
      " 0.91772361 0.82913999 0.81227728 0.7384619  0.62394096 0.65931088\n",
      " 0.70624512 0.91140918 0.88455571 0.95555396 0.84150733 0.68072532\n",
      " 0.5915178  0.69481061 0.71916702 0.90456263 0.89641401 1.02476325\n",
      " 0.89720682 0.78852995 0.69253993 0.74616969 0.78170801 0.96291073\n",
      " 0.96460748 1.03611699 0.98454183 0.7398354  0.73252695 0.76030416\n",
      " 0.78404402 0.90913208 0.98565321 1.00925096 1.01244034 0.76565123\n",
      " 0.71485435 0.75040609 0.77393079 0.92261643 0.92875187 1.02467653\n",
      " 0.96384451 0.78703031 0.76824016 0.81864105 0.82243362 1.05113955\n",
      " 1.01657827 1.16609459 1.05092276 0.91416005 0.81478631 0.89388268\n",
      " 0.87021053 1.09038012 1.06092376 1.12425685]\n"
     ]
    }
   ],
   "source": [
    "BC_error_seq = get_error_seq(trajs_BC)\n",
    "print(BC_error_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.04208672 0.07048247 0.09530356 0.12468443 0.19442156\n",
      " 0.20752649 0.24224397 0.25574469 0.27961868 0.33825758 0.35389042\n",
      " 0.34307171 0.31243333 0.32518412 0.28912608 0.28651197 0.28801419\n",
      " 0.3271352  0.39468921 0.51672314 0.62487591 0.72410458 0.95021456\n",
      " 0.89159407 0.85035115 0.81462773 0.77829017 0.81843841 0.87243643\n",
      " 0.90768794 1.0157635  1.07689269 0.96680297 0.90267691 0.88853044\n",
      " 0.82016577 0.85563268 0.94843198 1.08912879 1.12949157 0.94214998\n",
      " 0.92833416 0.88244647 0.85522272 0.90822037 0.98522057 1.04582707\n",
      " 1.10405766 0.95709263 0.90367854 0.84071972 0.81657622 0.91067915\n",
      " 0.97012788 1.11206033 1.03868137 1.0129302  0.91220759 0.87759866\n",
      " 0.88659637 0.92213872 0.97703651 1.06367256 0.95413063 0.95026432\n",
      " 0.91383788 0.81213304 0.85283307 0.86139299 0.93896208 0.98507154\n",
      " 0.90670221 0.92563604 0.91021584 0.81393488 0.78203013 0.85068335\n",
      " 0.95057691 1.00572034 0.8902554  0.8785812  0.80139862 0.76408122\n",
      " 0.74897541 0.84437006 0.94542646 1.00177864 0.92677133 0.90679598\n",
      " 0.86103127 0.75021988 0.87138897 0.94537633 1.09147794 1.13302612\n",
      " 1.09163322 1.03493204 0.89244169 0.82221863 0.89086533 0.95281814\n",
      " 1.10372101 1.1067363  1.03901398 0.98583432 0.78079813 0.71708552\n",
      " 0.80042594 0.85958284 0.97453987 1.06895534 0.99701218 0.92961631\n",
      " 0.81754385 0.79782679 0.83629145 0.92404875 1.03460025 1.04941967\n",
      " 1.01021146 1.0375422  0.82339008 0.75105127 0.82664585 0.88853082\n",
      " 0.92259656 1.09118596 0.96837806 0.95867654 0.79572041 0.72456854\n",
      " 0.82062332 0.90471696 0.94872961 1.07749235 0.96450897 0.97995221\n",
      " 0.82529166 0.81978741 0.83434697 0.91993977 1.01360213 1.11088203\n",
      " 0.95407708 0.95731151 0.81503173 0.87519667 0.83970029 0.97140753\n",
      " 1.03524818 1.0974409  1.03067401 1.0103294  0.90739385 0.95109012\n",
      " 0.96862286 1.00787785 1.03893669 1.14477661 1.07971901 1.04338533\n",
      " 0.86758322 0.87649191 0.83454267 0.95190973 0.99705109 1.06972608\n",
      " 1.04953102 1.06239164 0.96993874 0.91484116 1.02007866 0.99257989\n",
      " 1.06465088 1.12537315 1.09091642 1.06490209 0.94334657 0.91686391\n",
      " 1.02314089 1.00848347 1.00446645 1.07967457 1.03382132 1.02305369\n",
      " 0.91200162 0.87564035 0.98099413 0.93975838 1.02206494 1.07615375\n",
      " 1.05831245 0.99414854 0.89604699 0.86849675 0.90856329 1.02697012\n",
      " 1.07988273 1.08425269 1.0483099  0.95134192 0.83624695 0.77178658\n",
      " 0.83642643 0.85079261 0.99049113 1.01595372 1.00691469 0.95739883\n",
      " 0.86114604 0.82174045 0.88795353 0.92624169 0.99230945 1.03795918\n",
      " 1.05269819 1.04092643 0.99499189 0.92055553 0.97237163 0.94221525\n",
      " 1.03408983 1.04629803 1.10925682 1.03319897 1.04294817 0.96724637\n",
      " 0.94341651 0.95098017 0.95692007 0.9703828  0.97382702 0.94892733\n",
      " 0.9842776  0.88873575 0.87808138 0.87833408 0.92734861 0.91646742\n",
      " 0.95573314 1.04885327 1.08429468 1.01227488 1.0115532  1.00571573\n",
      " 0.97908347 1.05865476 0.98734392 1.05982702 1.01133921 0.87471913\n",
      " 0.97981383 0.94537779 0.9451546  1.06481517 0.98396039 1.10660147\n",
      " 0.97763048 0.86575824 0.90691859 0.92761413 0.98028514 1.05209309\n",
      " 1.04478624 1.04775642 0.92258957 0.93567971 0.87991344 0.94112087\n",
      " 0.90791004 1.02297955 1.0058416  1.09811979 1.06730002 0.9529687\n",
      " 0.96365541 0.96840113 0.95507492 1.00842018 0.9660762  1.16316177\n",
      " 1.09125799 0.98677459 0.95958926 1.02733463 0.95887675 1.05100634\n",
      " 1.0465747  1.16147955 1.10389115 1.00321909 0.98242996 0.99894197\n",
      " 0.99869698 1.02747186 1.05203312 1.07936293 1.02964497 0.92408543\n",
      " 1.02257102 1.00096936 1.03568916 1.04626667 1.05543781 1.03114688\n",
      " 0.95581591 0.9285741  1.0485864  1.04243945 1.06373281 1.1230119\n",
      " 1.1045967  1.01532332 0.94404578 0.9226268  0.9466491  0.9493914\n",
      " 1.12778175 1.14913619 1.09173997 1.05558583 0.91665208 0.85904218\n",
      " 0.94505502 1.02394693 1.20204943 1.18899567 1.1417117  1.06340425\n",
      " 0.93988147 0.9787287  1.08720528 1.12088473 1.1846138  1.22381135\n",
      " 1.1988193  1.12463145 1.04128948 1.06847381 1.11633543 1.14582563\n",
      " 1.18188123 1.17753224 1.27853752 1.14554509 1.00528114 1.06503282\n",
      " 1.03860756 1.11247967 1.22629827 1.26393274 1.18154481 1.11005012\n",
      " 0.99221248 0.9614234  0.98321101 1.07466005 1.19104556 1.21601093\n",
      " 1.1945835  1.08357316 0.99091343 1.01359932 1.01001403 1.08254695\n",
      " 1.12336594 1.0959706  1.11155887 0.98277842 0.92990518 0.9880773\n",
      " 0.95914416 1.0914148  1.11875141 1.137615   1.13721203 1.04131206\n",
      " 0.98997346 1.04650526 0.97863219 1.08280031 1.13381484 1.21793665\n",
      " 1.12985645 1.08412811 1.00185855 1.07435083 1.07757039 1.10303196\n",
      " 1.20215201 1.26768894 1.13733252 1.08412194 1.05704187 1.14421792\n",
      " 1.17498183 1.18870594 1.3082502  1.21583478 1.22437609 1.15844174\n",
      " 1.0433568  1.1164432  1.06104183 1.12465445 1.23616461 1.16701558\n",
      " 1.12765613 1.133803   1.03780949 1.02317436 1.07168928 1.17741614\n",
      " 1.24777217 1.2476899  1.18709042 1.12809228 1.02952779 1.0568729\n",
      " 1.06036559 1.07418874 1.29346996 1.13457847 1.15984104 1.09529113\n",
      " 1.02698234 1.00668239 0.95688314 0.97632289 1.1061541  1.16405453\n",
      " 1.10913386 1.07440391 1.00764523 0.93488604 0.90586979 1.00698894\n",
      " 1.00090486 1.05530567 1.04285118 1.00685101 0.93328476 0.89615468\n",
      " 0.88733649 0.95241667 1.05343548 1.01937606 1.07956599 1.03478005\n",
      " 1.06545093 0.95455138 0.95595881 1.03064169 1.09954595 1.0616589\n",
      " 1.06366403 1.04932515 1.0372162  0.9678415  0.96189923 1.04732414\n",
      " 0.97389359 1.08458509 1.11447817 1.09545439 1.14182081 1.05131617\n",
      " 1.11355039 1.1322109  1.11538259 1.13008492 1.08184891 1.11686076\n",
      " 1.09039038 0.94805999 1.02340912 1.12240024 1.19316647 1.16357239\n",
      " 1.10488074 1.14569835 1.04360251 0.95987323 0.99774315 1.06568556\n",
      " 1.12977844 1.15592146 1.12788812 1.13159244 1.01361166 0.89606545\n",
      " 1.0049306  1.05490503 1.10102361 1.16270817 1.11501136 0.97659507\n",
      " 0.87391311 0.80515544 0.95404021 0.98557456 1.14993126 1.20515583\n",
      " 1.06527391 0.95076817 0.88316449 0.80158377 0.92446329 0.94477918\n",
      " 1.07272296 1.06540023 1.07019424 0.96711174 0.85888593 0.84008034\n",
      " 0.92485656 0.96235732 1.0545181  1.06955848 0.95910032 0.82131094\n",
      " 0.77839535 0.82333215 0.92020789 0.93446255 0.96831063 1.00013056\n",
      " 0.84081445 0.81561208 0.81389862 0.82800741 0.91540979 1.05563743\n",
      " 1.08398018 1.00739328 0.86630326 0.75854343 0.7690079  0.76647827\n",
      " 0.88636468 0.94639613 1.02792784 0.97254229 0.84738629 0.73959562\n",
      " 0.74636306 0.81643921 0.90707493 0.94397244 1.00044405 0.87369913\n",
      " 0.74790185 0.73445491 0.74907701 0.79363504 0.81624174 0.86549254\n",
      " 0.93433584 0.82518481 0.6262214  0.71783812 0.72950997 0.80914079\n",
      " 0.82113799 0.8318005  0.91844812 0.71228373 0.61640342 0.70180414\n",
      " 0.79133562 0.83764399 0.9623756  0.96478919 1.15549203 0.97225988\n",
      " 0.86420532 0.80105574 0.9016605  0.90804666 1.00428337 0.98775364\n",
      " 1.20645341 1.00149849 0.91025839 0.8635883  0.90119013 0.89971302\n",
      " 0.94223923 0.91716844 1.12058953 1.02080035 0.90123573 0.88384665\n",
      " 0.93631632 0.90837525 0.94916927 0.89125696 1.01224771 0.91495412\n",
      " 0.90194979 0.91972792 0.97102038 0.87152903 0.99588028 0.92693246\n",
      " 1.07006085 0.93360251 0.9038721  0.86594966 0.93941888 0.9037557\n",
      " 0.96386909 0.98602032 1.04526588 0.91500354 0.90789805 0.91433018\n",
      " 0.93240474 0.93307056 0.96716192 0.90660979 0.96217666 0.92353082\n",
      " 0.84471747 0.90583009 0.92018619 0.9257037  0.9834321  0.92169516\n",
      " 1.04005759 0.90375082 0.93185105 0.98304859 0.98839483 0.973681\n",
      " 1.05133623 0.97268185 1.01594297 1.03558836 0.98205272 1.04679422\n",
      " 1.00481848 1.05049196 1.05157428 1.06010638 1.0941171  1.06144577\n",
      " 1.00764271 1.06025441 1.01267153 1.06955036 1.03594234 1.05600461\n",
      " 1.05641193 1.05780773 1.01933055 1.08185589 1.04639418 1.01020318\n",
      " 0.9544255  1.0295611  1.09635403 0.97231508 1.03446186 1.03762317\n",
      " 0.97808379 0.96743633 0.99385475 1.03938975 1.12438168 1.02240447\n",
      " 1.0420331  1.08242196 0.94761419 0.94333316 1.03556559 1.04573089\n",
      " 1.17871302 1.04347208 1.09671845 1.05498209 1.00615283 0.93865959\n",
      " 0.97072353 1.12312412 1.15755584 1.02961505 1.05883118 1.01743011\n",
      " 0.92547981 0.9014494  0.96925355 1.04452963 1.05251054 0.92977624\n",
      " 0.98041416 0.91124459 0.90638527 0.83200554 0.95909844 0.99006743\n",
      " 1.01884056 0.99828049 1.06205145 0.9893348  0.92029746 0.87685664\n",
      " 1.0047304  1.04342628 1.08346259 1.07883824 1.06530208 0.91644816\n",
      " 0.89114422 0.81490032 0.98941905 0.99514357 1.1959735  1.05418663\n",
      " 1.06036023 0.90097605 0.88917388 0.86251349 1.0242644  1.04422006\n",
      " 1.12656786 0.98979475 1.02598777 0.91193917 0.91445536 0.90644067\n",
      " 1.00648856 1.04592954 1.13043278 1.02401291 0.98301357 0.89283941\n",
      " 0.90858421 0.83833324 0.99136658 1.02171983 1.11941617 0.98425088\n",
      " 0.94993599 0.87988194 0.84159977 0.82769932 1.01607188 1.04372039\n",
      " 1.06419967 1.01014849 0.93906696 0.90705206 0.85753887 0.85554635\n",
      " 0.99614926 1.00661727 1.05413364 1.07177033 1.06291753 1.08698207\n",
      " 0.94998669 0.91215502 0.98915683 0.91489106 0.96442977 0.94932631\n",
      " 1.03256915 0.99629404 0.90591229 0.94359059 1.00344123 0.94428933\n",
      " 1.14312501 1.01650346 1.05823569 1.0591193  0.93467516 0.94488325\n",
      " 1.03980931 1.02292893 1.11886974 1.01589723 1.04312266 1.03497569\n",
      " 0.98130799 0.98453399 1.02229007 1.06256664 1.15053092 1.083504\n",
      " 1.18339504 1.06132543 1.00330137 0.95138281 0.98531427 1.04325677\n",
      " 1.18962506 1.08088345 1.17209879 1.09196956 0.97247631 1.01776583\n",
      " 1.02026377 1.15054976 1.21393892 1.26852683 1.2568671  1.09865944\n",
      " 0.97713348 0.97145728 0.98208548 1.11333672 1.22466016 1.20102374\n",
      " 1.28760707 1.06928089 0.94468623 0.91322592 0.97589276 1.07614892\n",
      " 1.17677958 1.18521913 1.31765656 1.11432732 1.01588177 0.95693442\n",
      " 0.98793253 1.07605599 1.2074543  1.14983536 1.13457582 1.00554133\n",
      " 0.91650078 0.93481509 0.98001421 1.02028978 1.20411151 1.19715235\n",
      " 1.11523462 1.07406229 0.94889252 1.01184332 1.04798596 1.13081266\n",
      " 1.18232518 1.24622266 1.18373135 1.06640462 0.96128393 0.94886991\n",
      " 1.00189811 1.07484108 1.18002197 1.27174328 1.22465571 1.11058822\n",
      " 0.94134341 0.94312184 0.97810944 1.02936991 1.1279222  1.17644136\n",
      " 1.09959958 1.06447799 0.91287062 0.96342453 0.99236549 1.01087245\n",
      " 1.16063098 1.25377374 1.12362734 1.004393   0.91408944 0.9730583\n",
      " 1.04973925 1.17256586 1.27451082 1.24612256 1.11496112 1.02586944\n",
      " 0.88544745 0.94886298 0.98874764 1.15995377 1.32644908 1.22968203\n",
      " 1.09161799 1.01555029 0.88113604 0.96307678 1.00979391 1.12756808\n",
      " 1.27871367 1.22003977 1.08360793 0.99840379 0.94641235 0.9536258\n",
      " 0.98005125 1.08168988 1.15556651 1.24535707 1.09489051 0.93071296\n",
      " 0.92243833 0.94250242 1.0003917  1.13579925 1.16807623 1.31989074\n",
      " 1.14304119 1.00766254 1.02258246 1.01969363 1.13191451 1.22422514\n",
      " 1.27091753 1.31974464 1.10066675 0.99772187 0.94520604 0.99995419\n",
      " 1.07058805 1.18997598 1.21423544 1.25114082 1.09473585 1.10867569\n",
      " 1.02028503 1.05615877 1.05858637 1.09266972 1.19731711 1.1722\n",
      " 1.0422256  1.06441535 1.0270357  1.07597221 1.06941427 1.15301097\n",
      " 1.18473627 1.13300418 1.05279133 1.07259026 1.03764331 1.08348277\n",
      " 1.15262782 1.21698547 1.30379736 1.12380605 1.09049296 1.01088213\n",
      " 0.99580138 1.06383771 1.01514648 1.11843432 1.18289196 1.04727102\n",
      " 0.9994717  0.96151862 0.91597914 0.946945   0.9267582  0.95621101\n",
      " 0.98251924 0.96068003 0.94209824 0.90919215 0.98383257 0.9890924\n",
      " 0.89468844 0.93613194 0.93481736 0.87809986 0.97011286 0.94711833\n",
      " 0.96078901 0.98524999 0.92849289 0.95051306 0.89824034 0.90474001\n",
      " 0.93799678 1.0482753  0.97643294 1.07006658 1.03208368 1.13104413\n",
      " 1.05825705 1.00009474 0.99354036 1.08883897 0.91643226 1.01235383\n",
      " 0.99530746 1.12678829 1.11147489 1.03830109]\n"
     ]
    }
   ],
   "source": [
    "BCO_error_seq = get_error_seq(trajs_BCO)\n",
    "print(BCO_error_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SC-BCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.04138397 0.07204708 0.07124966 0.06772004 0.10214348\n",
      " 0.11907995 0.1403129  0.18199039 0.22654778 0.28306955 0.30295882\n",
      " 0.27721352 0.26136631 0.25233694 0.21769896 0.21651477 0.22696915\n",
      " 0.2650398  0.33147212 0.42866787 0.53996903 0.58637556 0.7432231\n",
      " 0.76130755 0.72873128 0.65391098 0.67711099 0.71989381 0.73504258\n",
      " 0.79890452 0.90420893 0.97906015 0.89428898 0.67945745 0.67541137\n",
      " 0.63855362 0.7372314  0.82778369 0.98449906 1.10869327 0.90223917\n",
      " 0.68441983 0.71116479 0.73530848 0.79668298 0.94360663 1.12571747\n",
      " 1.23093007 1.10489061 0.79400063 0.78001444 0.72435647 0.80763089\n",
      " 0.95959435 1.10090168 1.31086415 1.07689815 0.72456397 0.71950336\n",
      " 0.70832208 0.80773586 0.9524308  1.1323352  1.31565694 1.03748831\n",
      " 0.75070256 0.751641   0.75186525 0.81705634 0.95648234 1.24811807\n",
      " 1.36336049 1.0515725  0.77234718 0.78737297 0.78129927 0.85393753\n",
      " 0.99910524 1.17548161 1.29121271 0.97501196 0.76637434 0.83950434\n",
      " 0.78511547 0.91979741 0.9911508  1.25711996 1.33763012 1.142208\n",
      " 0.86408732 0.91228233 0.87189839 0.90118065 1.03982487 1.27713749\n",
      " 1.35744111 1.23737061 0.94567895 0.91020502 0.91489086 0.94157965\n",
      " 1.10041786 1.25806936 1.39242202 1.17338643 0.90293756 0.80829923\n",
      " 0.84439426 0.90850881 1.03865203 1.16473299 1.37986425 1.17864635\n",
      " 0.9092004  0.78262781 0.77922016 0.87313372 0.98662639 1.13528838\n",
      " 1.3462594  1.16225029 0.87847281 0.7840111  0.80025115 0.89235812\n",
      " 0.98028847 1.1289723  1.29720893 1.18605182 0.87219081 0.72993019\n",
      " 0.78501708 0.86608089 0.893689   1.04797194 1.19781731 1.07389858\n",
      " 0.84424372 0.76957452 0.81191023 0.90024341 0.92243689 1.09739682\n",
      " 1.21354176 1.10349523 0.84966728 0.81379314 0.87788011 0.91524574\n",
      " 0.97246496 1.13311279 1.25361576 1.16016816 0.88605447 0.80361665\n",
      " 0.84266594 0.87249827 0.95243461 1.13133354 1.24350096 1.22146569\n",
      " 0.92641483 0.81318472 0.82830602 0.83534361 0.85031801 0.95110328\n",
      " 0.97476235 1.00691085 0.80531766 0.73819843 0.84752704 0.81746229\n",
      " 0.88735522 1.00911781 1.05226245 1.06919915 0.87264325 0.76273223\n",
      " 0.89016667 0.84441399 0.92577727 1.13437947 1.10473577 1.03830534\n",
      " 0.8579723  0.77171634 0.89615542 0.83557763 0.91396486 1.08361106\n",
      " 1.05889948 1.06405327 0.87962888 0.80051997 0.86303049 0.88248265\n",
      " 0.98741494 1.06394777 1.08601916 1.11879259 0.98117312 0.8725377\n",
      " 0.90172734 0.88008714 1.0255232  1.11754817 1.11253451 1.09555731\n",
      " 0.98775072 0.90761251 0.95969533 0.89013286 1.05566633 1.15089291\n",
      " 1.17970987 1.16063477 1.03378781 0.96547229 1.00196556 1.0170922\n",
      " 1.08075789 1.13513147 1.01302076 1.03790008 0.95591713 0.90812076\n",
      " 0.90980603 0.95934362 1.01312368 1.07249444 0.97691259 0.93262275\n",
      " 0.82611436 0.82721985 0.91107874 0.95403193 1.03695377 1.01229677\n",
      " 0.88123312 0.83108466 0.82219708 0.8814278  0.90650269 1.01208886\n",
      " 1.09131546 1.13691556 0.93755324 0.89347172 0.84789635 0.87874601\n",
      " 0.95016122 1.04290634 1.11418679 1.15144286 1.01473673 0.98381721\n",
      " 0.92874686 0.90476296 0.99188534 1.03960539 1.16585996 1.14850775\n",
      " 1.12049253 1.00153378 0.8992708  0.96472493 0.95836722 0.99276599\n",
      " 1.06638611 1.15562701 1.00876039 0.91254141 0.83653456 0.91085422\n",
      " 0.93898543 1.0045816  1.15427973 1.16608291 1.00455808 0.87447315\n",
      " 0.89039401 0.90642097 0.90000555 0.98336963 1.00823842 1.05682205\n",
      " 0.98711686 0.92895903 0.85017144 0.90982575 0.94072866 0.98331755\n",
      " 1.1226227  1.1473555  1.05274233 0.92256279 0.87435063 0.887644\n",
      " 0.88888491 0.86773168 1.01426934 1.05199537 0.94533224 0.78409923\n",
      " 0.79057525 0.87317185 0.80807223 0.82121737 0.97948668 0.96573479\n",
      " 0.89654961 0.82910258 0.81559382 0.83556903 0.77421089 0.8486414\n",
      " 1.01757436 1.01609499 0.85929417 0.87060884 0.82340219 0.87449464\n",
      " 0.80446964 0.86156599 1.03676847 1.02658821 0.93325566 0.87778893\n",
      " 0.78017769 0.83649673 0.77029049 0.79699693 0.91609689 0.90841499\n",
      " 0.84697271 0.83513347 0.83047503 0.83466133 0.8030853  0.81879084\n",
      " 0.85145945 0.8889999  0.92265491 0.87431411 0.75248632 0.88265355\n",
      " 0.87427394 0.94526134 0.92062205 1.02128496 0.89514793 0.86630607\n",
      " 0.81768798 0.89025617 0.89129517 0.96509694 1.00027631 0.99807893\n",
      " 0.91876505 0.93752445 0.89734575 0.93156275 0.87064097 1.00229799\n",
      " 1.06187247 0.85630254 0.80542357 0.8156543  0.76744115 0.79872603\n",
      " 0.78282465 0.89961328 0.90451853 0.82338016 0.80157472 0.83236932\n",
      " 0.72078629 0.72431028 0.76652208 0.85982668 0.88266662 0.78337637\n",
      " 0.7450562  0.76387116 0.70534341 0.78435112 0.85516804 0.92518236\n",
      " 0.94426099 0.82537781 0.7513948  0.76581555 0.70298184 0.79185351\n",
      " 0.79970901 0.91679085 0.91931768 0.81338594 0.84902302 0.8048659\n",
      " 0.74351142 0.69331652 0.72016671 0.82559808 0.86860062 0.79937041\n",
      " 0.79748733 0.83320218 0.80475375 0.74579533 0.79934969 0.88500437\n",
      " 0.98638422 0.87524423 0.86807547 0.93655367 0.86880239 0.80026955\n",
      " 0.80793126 0.90702045 0.96372773 0.84435151 0.86638077 0.91160707\n",
      " 0.82518991 0.78987415 0.88823778 0.92963822 1.00454562 0.92824502\n",
      " 0.91524678 0.98863326 0.90443258 0.88189129 0.92949997 1.00463968\n",
      " 1.0044825  1.00242472 0.90200066 0.9704835  0.88423175 0.89887604\n",
      " 0.91890904 1.06469687 1.08153329 0.98365239 0.94886963 1.04590067\n",
      " 0.95336868 0.97683166 1.04078126 1.19781214 1.22688876 1.037521\n",
      " 0.99151627 1.0731446  0.9166479  0.95850454 1.00580754 1.15421565\n",
      " 1.10622882 1.05064651 0.98245136 1.11353243 1.00753123 0.97097319\n",
      " 1.08563639 1.23920109 1.20830324 1.05874682 1.077095   1.09402002\n",
      " 0.99913442 0.93333529 0.9808594  1.14011981 1.13579078 1.00963943\n",
      " 0.97264951 1.05472748 0.96190302 0.91503518 0.96043504 1.11582462\n",
      " 1.0961534  1.03460525 1.0154827  1.06328862 1.07744073 0.92379978\n",
      " 1.0292059  1.08531214 1.15785413 1.13315496 1.07505596 1.16309055\n",
      " 1.09577945 1.0391242  1.04421499 1.11712825 1.14195816 1.03556177\n",
      " 1.009502   1.0779965  1.09819892 0.97953619 0.9909179  1.09803088\n",
      " 1.14397089 1.05677539 0.94638133 1.0454796  0.9629079  0.97693472\n",
      " 0.97431037 1.10466741 1.13368649 1.10261121 0.97397086 1.05209637\n",
      " 0.988694   1.00847215 0.97953501 1.08782733 1.11932649 1.14585537\n",
      " 1.06546199 1.08213661 1.01678099 0.97897935 1.0101838  1.13358437\n",
      " 1.22632764 1.12135331 1.03321697 1.09012109 1.00216176 0.91602127\n",
      " 0.98030679 1.0742508  1.14016525 1.1401621  1.06583479 1.11910882\n",
      " 1.01018671 0.97192663 1.07719567 1.16049959 1.22878608 1.22579689\n",
      " 1.20280684 1.27253179 1.14267407 1.0004777  1.06227605 1.20071367\n",
      " 1.27268982 1.27143138 1.16670426 1.27519797 1.13010543 1.05751801\n",
      " 1.0568134  1.1049481  1.20129155 1.21112819 1.13097217 1.19894068\n",
      " 1.06952707 0.98974843 1.00338057 1.03628836 1.1771231  1.17494017\n",
      " 1.11022997 1.12869214 1.03511613 0.93893264 0.99285884 1.08818369\n",
      " 1.14769576 1.12981027 1.15060261 1.12418791 0.99932673 0.96932673\n",
      " 0.99266577 1.03863531 1.21481361 1.1483421  1.14486735 1.13294237\n",
      " 0.96062128 0.97315748 1.0034993  1.03456135 1.13949103 1.12439936\n",
      " 1.10281325 1.12964347 0.99088662 0.9373481  0.99117866 1.10834976\n",
      " 1.19286872 1.13652889 1.10483648 1.1130652  0.97450515 0.94435742\n",
      " 0.96059534 1.05843314 1.1909248  1.18996579 1.1914257  1.17562857\n",
      " 1.10359455 0.99405447 1.04582949 1.07660254 1.16088527 1.15442427\n",
      " 1.11049003 1.0622912  0.98193993 0.91882122 0.99352135 1.12244525\n",
      " 1.16608497 1.21150842 1.12134993 1.17594453 1.04807943 0.90450921\n",
      " 1.00251341 1.02586216 1.17318088 1.21130479 1.20461795 1.1711193\n",
      " 1.05641255 0.95531179 1.07830489 1.10509978 1.22952833 1.17965827\n",
      " 1.21762559 1.23010613 1.03196782 0.95203216 1.02229159 1.08422396\n",
      " 1.15378153 1.18233798 1.10534239 1.06772993 0.99316594 0.93709112\n",
      " 1.00472693 1.00994932 1.13176739 1.13986969 1.14587171 1.12886812\n",
      " 1.02146475 0.9484442  0.98517757 0.99763854 1.08632855 1.10510559\n",
      " 1.09690074 1.15083152 0.99361602 0.92262196 1.03527089 1.06648655\n",
      " 1.14140618 1.14547855 1.13314558 1.17149575 1.03994728 0.98011361\n",
      " 1.04988448 1.02602323 1.11676737 1.09472748 1.10502383 1.08676237\n",
      " 0.99756593 0.90430723 1.01388283 1.0610938  1.18246488 1.07057196\n",
      " 1.15017377 1.16500458 1.01987861 0.97675536 1.05617753 1.08238491\n",
      " 1.15353834 1.18809179 1.17273475 1.20236256 1.01357773 0.94206372\n",
      " 1.06478587 1.04773945 1.16351887 1.20826304 1.15098798 1.20297995\n",
      " 0.95530765 0.92171625 1.03707836 1.08317975 1.19069675 1.21857782\n",
      " 1.06738354 1.09039572 0.98569122 0.92047203 0.95961986 1.10579593\n",
      " 1.0963567  1.11403151 0.95685033 0.98229274 0.91919817 0.88772748\n",
      " 0.99803404 1.03540026 1.07935682 1.15138103 0.97825184 0.99213325\n",
      " 0.9170578  0.86302352 0.94301742 1.06362288 1.09989392 1.18452324\n",
      " 0.99075018 1.00707675 0.90182214 0.89567447 0.9836802  1.1008861\n",
      " 1.08154519 1.11248126 0.95984514 0.98909247 0.91364538 0.92415318\n",
      " 0.94943311 1.06041185 1.13315199 1.22306184 1.09602423 1.09269647\n",
      " 1.00604572 0.93877656 1.01898062 1.05637951 1.10901731 1.16589104\n",
      " 1.02559427 1.07542116 0.99584664 0.94340819 1.04166007 1.09684731\n",
      " 1.16651051 1.21549084 1.13095726 1.16800531 1.01386079 0.97486778\n",
      " 1.01144277 1.08589234 1.15629583 1.22290711 1.16898533 1.13488804\n",
      " 0.96966673 1.04628963 1.08064461 1.09365826 1.2280916  1.21988539\n",
      " 1.08837553 1.0416479  0.99986109 0.96043713 1.02833456 1.09339445\n",
      " 1.19425123 1.28622857 1.14081108 1.15034943 0.96719007 1.00162086\n",
      " 1.0436198  1.10152292 1.11237581 1.25233906 1.1334339  1.1313665\n",
      " 0.9292585  0.94162876 0.99522116 1.08531115 1.1284779  1.14925894\n",
      " 1.12440679 1.08448843 0.90902651 0.92218715 0.94876722 1.11093466\n",
      " 1.12694942 1.13552102 1.14219708 1.11734018 0.96201428 0.95057689\n",
      " 0.93370255 1.08590013 1.14364127 1.12403789 1.03499919 0.98407135\n",
      " 0.88059851 0.87521011 0.88040223 1.02477015 1.09408552 1.16467784\n",
      " 1.01812486 0.96376553 0.84563795 0.88513067 0.89251929 1.00579433\n",
      " 1.05202248 1.10627083 0.99969989 0.95944392 0.89426626 0.85515696\n",
      " 0.92756512 1.05380301 1.05962224 1.04432099 1.02860819 0.96045623\n",
      " 0.83989502 0.88076793 0.94930138 1.05303454 1.15016498 1.13808666\n",
      " 1.08203205 0.93658753 0.86384496 0.85991053 0.88514927 1.01876597\n",
      " 1.10826051 1.14655933 1.04919868 0.91195448 0.82887963 0.80690483\n",
      " 0.86509223 1.09543348 1.09094987 1.16060041 1.04847253 0.9536441\n",
      " 0.83844807 0.87388906 0.86054011 1.05997926 1.17769718 1.1159042\n",
      " 1.05629588 0.97219162 0.87526541 0.87216224 0.8563614  1.03686963\n",
      " 1.08975491 1.13472701 1.01267938 0.92380465 0.86522025 0.8696849\n",
      " 0.82391049 0.94544188 0.97519932 1.12016674 0.99523707 0.85638116\n",
      " 0.81113848 0.81533835 0.79155327 0.90108587 0.92303362 1.15500685\n",
      " 0.96661375 0.84654633 0.78130652 0.82865725 0.81713739 1.01882983\n",
      " 1.06661337 1.10533642 1.01990366 0.85627458 0.80638596 0.80312034\n",
      " 0.79376257 0.97270386 0.96488053 0.96967378 0.97147866 0.93162105\n",
      " 0.92552837 0.9446577  0.92754997 1.00662704 1.07684937 1.07330999\n",
      " 1.02906848 0.94489845 0.92303297 0.96422198 0.9519462  1.10942926\n",
      " 1.11619392 1.08741204 0.98478098 0.89176169 0.93641266 0.89958936\n",
      " 0.89956657 1.07284496 1.05898983 1.10598083 1.04018519 0.9733842\n",
      " 0.8925999  0.8609972  0.8695218  1.05847513 1.02463865 1.01644474\n",
      " 1.00545676 1.00668209 0.85843873 0.76275254 0.81534657 0.99918227\n",
      " 1.04903348 1.05781245 1.02087057 0.95005865 0.8146685  0.76424743\n",
      " 0.76024497 0.8914919  0.90763104 0.99164797 1.02248511 0.9740561\n",
      " 0.85788437 0.82543284 0.82801527 0.91812753 0.96166788 1.0199473\n",
      " 1.15706179 1.07044105 0.93158859 0.87867154 0.91091466 1.07316311\n",
      " 1.06533721 1.10346001 1.13077078 1.10653444 0.946116   0.92214816\n",
      " 0.93686061 1.11436673 1.08728744 1.09398233]\n"
     ]
    }
   ],
   "source": [
    "SCBCO_error_seq = get_error_seq(trajs_SCBCO)\n",
    "print(SCBCO_error_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SC-BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.02893601 0.0489682  0.05018145 0.05785727 0.09532154\n",
      " 0.09278589 0.11927643 0.12341421 0.20381614 0.26226513 0.27827405\n",
      " 0.27031057 0.24910224 0.22930755 0.21706489 0.21456658 0.24559313\n",
      " 0.29472506 0.36482244 0.46762068 0.59255455 0.6701096  0.84605207\n",
      " 0.91272751 0.85989671 0.7965243  0.78433699 0.83271361 0.85442169\n",
      " 0.87468374 0.97329419 0.96162428 1.10526033 0.91109427 0.87900376\n",
      " 0.83049533 0.83274302 0.84082355 1.02724932 1.08112696 1.02181524\n",
      " 0.85384911 0.8849476  0.87175658 0.76858757 0.88272613 0.9249383\n",
      " 0.96285516 0.92591883 0.82403544 0.88779983 0.80476224 0.82595078\n",
      " 0.81328033 0.95548002 0.96101982 0.94222073 0.83680953 0.90388191\n",
      " 0.81597566 0.85582913 0.88691156 1.01277713 1.06282849 0.93800173\n",
      " 0.81168713 0.91902181 0.85410457 0.86026012 0.83929227 1.05918902\n",
      " 1.11167022 1.03720654 0.8734812  0.9620691  0.89635716 0.87004854\n",
      " 0.92667513 1.05725646 1.13825645 1.03601537 0.9647252  0.97481115\n",
      " 0.84426246 0.84990335 0.92946098 1.03025799 1.09040675 0.97494884\n",
      " 0.91260231 0.96915351 0.86744609 0.8526544  0.92442299 0.97993943\n",
      " 1.07634063 1.08010539 0.91123735 0.96198845 0.8877916  0.88702085\n",
      " 0.98974964 1.10678043 1.18191735 1.11095205 0.95741213 0.98057128\n",
      " 0.87892858 0.8683013  0.92236231 1.07125774 1.088255   1.08368719\n",
      " 0.92214832 0.97984736 0.9087691  0.88836448 0.91644605 1.0252216\n",
      " 1.0658298  1.01795469 0.88180182 0.99401602 0.9195182  0.91262729\n",
      " 0.9396036  1.11322125 1.09420296 1.0248799  0.90347316 0.96294425\n",
      " 0.85849089 0.90020177 0.96323299 1.03056217 1.03270683 0.92170309\n",
      " 0.83516195 0.90887474 0.87395298 0.86719252 0.93751136 1.10508354\n",
      " 1.12990415 0.94685794 0.86266064 0.91742327 0.9112332  0.86458309\n",
      " 0.9639029  1.04061366 1.06288579 0.95179827 0.89524164 0.90209979\n",
      " 0.96788777 0.87649765 0.91974319 1.08272502 1.1157524  0.99588199\n",
      " 0.93583886 0.93574532 0.9222502  0.86442352 0.95495339 1.05637287\n",
      " 1.0402683  0.93164046 0.93046971 0.81812252 0.84256709 0.70620972\n",
      " 0.80865412 0.90274861 0.85595335 0.80071308 0.80130584 0.78825723\n",
      " 0.89973022 0.7748041  0.88273492 0.96654566 0.87975938 0.84193632\n",
      " 0.82477729 0.75456235 0.78568674 0.72506165 0.81744481 0.85709585\n",
      " 0.87077716 0.78852696 0.79710764 0.75494132 0.73603746 0.70100647\n",
      " 0.73972082 0.82559456 0.79313713 0.7313352  0.81154498 0.76979411\n",
      " 0.76180906 0.69802472 0.84205099 0.89195735 0.86047378 0.85758026\n",
      " 0.80873332 0.84631622 0.81852014 0.78962359 0.84596495 0.91128115\n",
      " 0.90953158 0.85225916 0.85840744 0.9176827  0.89107385 0.82715493\n",
      " 0.93546105 0.99616357 0.92344336 0.9144884  0.84803789 0.88789399\n",
      " 0.83679043 0.80552259 0.91494999 0.95259218 0.91426866 0.90364918\n",
      " 0.90803073 0.92958212 0.88372646 0.90470344 1.01585666 1.16022625\n",
      " 1.09112156 0.92485789 0.91243144 0.9670606  0.81836675 0.77399792\n",
      " 0.83770749 0.8939828  0.81531534 0.77457358 0.81674    0.85959991\n",
      " 0.83879442 0.79487827 0.86091157 0.88300784 0.81617064 0.81316771\n",
      " 0.78642284 0.83704497 0.86017431 0.85072412 0.98806887 1.00109328\n",
      " 0.83069768 0.7966934  0.78623269 0.90132004 0.81895859 0.80400014\n",
      " 0.89595504 0.95169686 0.8491912  0.83308586 0.82581862 0.92849435\n",
      " 0.85224909 0.79303059 0.85232054 0.84212052 0.77776484 0.86250073\n",
      " 0.8520859  0.9605842  0.85836444 0.85615267 0.90661914 0.86016991\n",
      " 0.81769635 0.83218004 0.84013164 0.98950611 0.87654961 0.80643363\n",
      " 0.89562446 0.8866552  0.82806768 0.85023923 0.86805473 1.02106837\n",
      " 0.93704295 0.86114915 1.00391231 0.91739465 0.86402643 0.84285092\n",
      " 0.86438104 0.99735863 0.88629224 0.84207904 0.9881238  0.92792743\n",
      " 0.81151697 0.80048621 0.77758637 0.95839244 0.87353529 0.8830178\n",
      " 1.02935342 1.04386694 0.91908879 0.88807709 0.83351131 0.91324029\n",
      " 0.88751951 0.83361221 1.0623744  1.07763735 0.92442607 0.91269242\n",
      " 0.92013062 0.99574395 1.04451346 1.03757445 1.17714447 1.13922985\n",
      " 0.96328862 0.96352063 1.01729627 1.08272655 1.1538834  1.15504379\n",
      " 1.2534245  1.27374927 1.1069134  0.97847062 0.95629587 1.09887784\n",
      " 1.06299982 1.09955298 1.18133726 1.24310521 0.947152   0.95228379\n",
      " 0.85726221 0.99329322 0.97487288 0.97981354 1.1437117  1.02734541\n",
      " 0.94398194 0.89440488 0.8729187  1.0335579  0.96478477 1.0489877\n",
      " 1.21913413 1.11496934 0.9674765  0.90133224 0.95191566 1.00477443\n",
      " 0.95754867 0.99211769 1.13590992 1.05063329 0.99411083 0.87010112\n",
      " 0.92698167 0.99878095 0.91450055 0.99462846 1.08703451 1.01106079\n",
      " 0.97165992 0.92159501 0.96566983 1.0733117  0.97102355 0.99436483\n",
      " 1.11598665 1.0551483  1.00753567 1.00037093 0.94855857 1.01861495\n",
      " 0.95666463 0.96570486 1.11106107 1.0222861  0.99259401 0.94918757\n",
      " 0.98014322 0.98091703 0.92392724 0.9725682  1.14866349 1.03491326\n",
      " 0.97568313 0.94636719 0.94651282 0.99460604 0.9674133  1.08055662\n",
      " 1.19680671 1.08597816 0.99164253 1.00539981 0.92186415 1.003268\n",
      " 0.98658537 1.06812209 1.2176056  1.08969885 1.04317755 1.0032214\n",
      " 1.02597994 1.00647865 0.95641115 1.05999033 1.16360043 1.10236563\n",
      " 1.04595159 1.01844889 1.07956457 1.01018631 1.01494521 1.15679132\n",
      " 1.20542902 1.11656923 1.07859829 0.97910224 1.06449997 0.95619477\n",
      " 0.97782837 1.09303915 1.25974073 1.17387788 1.09945359 0.95471153\n",
      " 1.05179445 0.92933175 0.95246359 1.02298578 1.20883227 1.09427098\n",
      " 0.98963501 0.94893581 1.03245124 0.97755031 0.9674565  1.13491046\n",
      " 1.21270682 1.2107642  1.03023901 0.99337852 1.01993889 0.92544036\n",
      " 0.99934626 1.08501717 1.22959117 1.1278979  0.95499855 0.85476455\n",
      " 0.95864985 0.92435457 0.92077133 1.06748347 1.2698304  1.18167882\n",
      " 1.06907828 0.95372762 1.03643674 0.85737903 0.90880189 1.02842459\n",
      " 1.14016814 1.07245899 0.92229924 0.88184069 1.01393183 0.9018371\n",
      " 0.9104889  0.9347901  1.13326473 1.00544214 0.96219787 0.94177424\n",
      " 0.99032535 0.89860695 0.97169245 1.03361784 1.25134292 1.09011218\n",
      " 1.03404454 0.93133749 0.9909728  0.90853719 0.97833243 1.09669559\n",
      " 1.25571695 1.16648707 0.98778272 0.92201138 0.97935009 0.93683908\n",
      " 0.97264548 1.19617293 1.27130153 1.16272224 0.90474576 0.89836262\n",
      " 0.98488252 0.97158108 0.90878039 1.10842684 1.18813208 1.12838673\n",
      " 0.90945889 0.86058602 0.9775587  0.89988531 0.87808482 0.9698528\n",
      " 1.10737405 0.99266531 0.8677608  0.8143231  0.93978554 0.89793566\n",
      " 0.87959147 0.98992878 1.05732796 1.06401797 0.94593909 0.89109088\n",
      " 0.9519795  0.92084896 0.93477834 0.99943085 1.04158354 1.04594309\n",
      " 0.94157993 0.83824468 0.88577042 0.92038724 0.87124424 1.06611244\n",
      " 1.10977538 1.08384939 0.93311202 0.96448743 0.93471547 0.91999309\n",
      " 0.89497604 1.06309872 1.1496745  1.10882207 0.96337908 0.94656797\n",
      " 0.97155584 0.92056423 0.90187081 1.11996414 1.19435396 1.13284823\n",
      " 0.97410492 0.92384864 0.92869315 0.95321022 0.91593807 1.02908919\n",
      " 1.11700635 1.09302058 0.90029731 0.89266533 0.89959266 0.88445931\n",
      " 0.89481102 1.00814125 1.18865265 1.12800116 0.88296932 1.00088595\n",
      " 0.99765216 0.8624872  0.91731499 1.03879522 1.22520541 1.08970922\n",
      " 0.96594972 0.99071299 1.00430896 0.87472704 0.94807071 1.02268402\n",
      " 1.12307497 1.06639348 0.95591201 0.96628681 0.97458735 0.91579949\n",
      " 0.96660621 1.09900686 1.19238178 1.12333221 1.06948208 1.08134735\n",
      " 1.1299551  0.98255773 1.0594382  1.16063527 1.20880998 1.13247205\n",
      " 1.07266077 1.04363844 1.04867298 0.96677328 1.05562272 1.16124281\n",
      " 1.25553304 1.15137052 1.14031995 1.09442743 1.10136782 0.99304671\n",
      " 1.07526896 1.17263708 1.29318371 1.2364137  1.15303145 1.03143549\n",
      " 1.02420998 0.98886224 1.04968704 1.09882836 1.24434175 1.1493114\n",
      " 1.1223676  1.06817955 1.02465858 0.90635185 0.9621643  1.01288691\n",
      " 1.1185755  1.10146152 1.00101559 0.8870689  0.84168775 0.79241547\n",
      " 0.8571455  0.92180053 1.03870637 0.94277859 0.89705905 0.85947938\n",
      " 0.86078041 0.78947034 0.86891737 0.9050362  1.04069322 0.92656864\n",
      " 0.87084529 0.74498228 0.80065103 0.77559069 0.86044834 0.90887767\n",
      " 1.01922665 0.85977092 0.8207423  0.81886257 0.84126124 0.84346965\n",
      " 0.99141768 1.11737534 1.12470798 0.98476042 0.88088124 0.878634\n",
      " 0.94145326 0.93588636 1.00433389 1.09043031 1.18649088 1.0727463\n",
      " 0.94612898 0.84464494 0.84604485 0.89098727 0.92477648 1.01664362\n",
      " 1.12619417 1.00574153 0.89909294 0.81758974 0.7701361  0.77532451\n",
      " 0.8486991  1.00697492 1.16799242 1.01373342 0.92074542 0.90930278\n",
      " 0.75996933 0.72732602 0.82583405 0.94420348 1.1258919  0.98761881\n",
      " 0.92234613 0.86243292 0.71140656 0.69742701 0.74947874 0.85389382\n",
      " 1.01701916 0.89519324 0.88532082 0.85890197 0.72868618 0.70906736\n",
      " 0.86050148 0.92370252 1.13890422 1.0254982  0.98254525 0.90912854\n",
      " 0.77885994 0.71083801 0.81838378 0.87982218 1.0412951  0.95870286\n",
      " 0.94269528 0.89753303 0.76953165 0.68196315 0.78459056 0.84479275\n",
      " 0.9411755  0.90759457 0.92350498 0.81167116 0.74028185 0.67602669\n",
      " 0.78205897 0.84958593 0.91100903 0.89950648 0.96662958 0.91878293\n",
      " 0.70152186 0.68521405 0.76468824 0.88144214 0.97832434 0.9825708\n",
      " 0.95153078 0.93086558 0.76499457 0.73164885 0.85970872 0.93614221\n",
      " 1.03689092 1.00126685 0.99109436 0.92448254 0.75840466 0.71041536\n",
      " 0.88393472 0.98259516 1.12267431 1.03310816 0.98605973 0.96163625\n",
      " 0.7577498  0.710316   0.79946658 0.90569635 1.00195752 0.91923017\n",
      " 0.93009802 0.89199764 0.79960032 0.74693586 0.79858201 0.91303097\n",
      " 0.99851713 0.94951687 1.0130028  0.94436291 0.81298833 0.74943365\n",
      " 0.87078246 0.88219344 0.93790813 0.96680569 0.98200437 0.87189358\n",
      " 0.74168101 0.76354723 0.83281923 0.90625685 1.00692965 0.983073\n",
      " 1.01845778 0.83759945 0.76521947 0.71793716 0.78841749 0.8767992\n",
      " 0.95714495 0.97856591 0.99043934 0.84756194 0.74665047 0.76262212\n",
      " 0.80400557 0.86936752 0.96796286 0.95504825 1.01225911 0.83485873\n",
      " 0.76040847 0.73883931 0.84476127 0.90675071 1.0155022  1.07075949\n",
      " 1.06620458 0.91830015 0.87702392 0.85335804 0.93868089 1.01403574\n",
      " 1.08590221 1.14899465 1.2120794  0.98103728 0.88065827 0.82691569\n",
      " 0.93552807 0.96252407 0.98460156 1.0211015  1.07995488 0.91104432\n",
      " 0.77434507 0.79096125 0.8639315  0.93771004 1.00112114 1.03950136\n",
      " 1.02982132 0.86044045 0.78668059 0.78706446 0.89856151 1.03401045\n",
      " 0.98090346 1.12704679 1.0066668  0.79109493 0.7374757  0.73635918\n",
      " 0.89316547 0.98924107 1.00322879 1.0888195  0.96512487 0.80112382\n",
      " 0.76834852 0.72496671 0.88214822 0.94541378 1.01474892 1.04379805\n",
      " 0.92286099 0.82356814 0.72957697 0.71515688 0.77201234 0.94468981\n",
      " 0.90172113 0.9118625  0.84042973 0.75747972 0.66515436 0.70542357\n",
      " 0.82956625 0.88147553 0.86515946 0.90918051 0.84102498 0.69607741\n",
      " 0.65137581 0.66552457 0.75603895 0.83705713 0.81393474 0.89340134\n",
      " 0.80378436 0.68485183 0.67501872 0.71651427 0.85264835 0.93716238\n",
      " 0.96417033 0.9542292  0.9067621  0.71588182 0.69012413 0.70731813\n",
      " 0.82590485 0.86967885 0.91816365 0.99422121 0.95289605 0.75998922\n",
      " 0.77680577 0.78060318 0.90255574 0.90979447 0.96524073 1.0887438\n",
      " 1.00480474 0.83025112 0.9149936  0.94393543 1.07841588 1.07652468\n",
      " 1.12912378 1.11659064 1.10653933 0.9511218  0.89285797 0.96461083\n",
      " 1.05417568 1.11517707 1.15495449 1.03980273 1.02736839 0.88727859\n",
      " 0.8932132  1.00220179 1.04249244 1.13729082 1.15525271 1.06595015\n",
      " 1.06771237 0.88524147 0.88209993 0.93580635 1.05957337 1.08747594\n",
      " 1.13231347 1.15297921 1.12942547 0.92340513 0.89523308 0.88711651\n",
      " 0.93634113 0.9827447  1.10713356 1.06417767 1.11649291 0.95781962\n",
      " 0.92434503 0.95545021 1.12536978 1.1223592  1.14996891 1.15415689\n",
      " 1.2073466  0.96749791 0.92503971 0.9855594  1.07486464 1.12594132\n",
      " 1.13024695 1.10926123 1.07218435 0.89903977 0.89169982 0.9583005\n",
      " 1.04896511 1.15572824 1.19051862 1.14394092]\n"
     ]
    }
   ],
   "source": [
    "SCBC_error_seq = get_error_seq(trajs_SCBC)\n",
    "print(SCBC_error_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1e2143a208>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "horizon=1000\n",
    "x = np.arange(horizon)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, BC_error_seq[0:horizon], label='BC', linewidth=0.6)\n",
    "ax.plot(x, BCO_error_seq[0:horizon], label='BCO', linewidth=0.6) \n",
    "ax.plot(x, SCBCO_error_seq[0:horizon], label='SC-BCO', linewidth=0.6) \n",
    "ax.plot(x, SCBC_error_seq[0:horizon], label='SC-BC', linewidth=0.6) \n",
    "ax.set_xlabel('time step')  # Add an x-label to the axes.\n",
    "ax.set_ylabel('error')  # Add a y-label to the axes.\n",
    "ax.set_title(\"Error-time_step graph\")  # Add a title to the axes.\n",
    "ax.legend()  # Add a legend.\n",
    "plt.savefig('imgs/error_graph.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BCO($\\alpha$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "M = 5000\n",
    "EPS = 0.9\n",
    "DECAY = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb68c871882e4e8faa5d1bc7da7e89b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1: reward=-41.51\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef9fed6262764ad9b5e27493332646e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=157), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1: loss_inv=0.209\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21919715f76e417c82810a48d82d8407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=157), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1: loss_policy=0.003\n",
      "Ep 2: reward=-11.53\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306a21e3955a46e9bb2da87929efe1a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=313), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 2: loss_inv=0.037\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f70350c69045d781f1c4bb0020aced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=157), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 2: loss_policy=0.004\n",
      "Ep 3: reward=-11.92\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19193ada9802472b92df2be9512a31d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=469), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 3: loss_inv=0.014\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d809d309a14dd3beb993889e7d8cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=157), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 3: loss_policy=0.003\n",
      "Ep 4: reward=-11.27\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53fee0abc6d344c08cdea2e7a369dd1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=625), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 4: loss_inv=0.008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed0e6c8da1f4c79b2ca084552150f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=157), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 4: loss_policy=0.003\n",
      "Ep 5: reward=-12.03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4cfc046857b4f63bed61c554cbd5797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 5: loss_inv=0.005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f50d00f6b1c49d99808cd8905df20ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=157), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 5: loss_policy=0.002\n",
      "Ep 6: reward=-12.42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556547070ccf4012b67fa986ba1776f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=938), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 6: loss_inv=0.004\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7dcf2f364094bf387b7fcb51520b51f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=157), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 6: loss_policy=0.002\n",
      "Ep 7: reward=-13.06\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7619ba7d82e7421691f0017ab115b9e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1094), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 7: loss_inv=0.003\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d32c9266bcc40bf9f83c48ef01e486c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=157), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 7: loss_policy=0.002\n",
      "Ep 8: reward=-12.58\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bc86dc51a3645589a1aa9b63650e6f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 8: loss_inv=0.003\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "296e8105476845b09fa6a057ebbc241c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=157), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 8: loss_policy=0.002\n",
      "Ep 9: reward=-12.28\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d4de68a6bd454f800e372d5c7cec38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1407), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-fbb8c6916199>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mls_bh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mls_bh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mTQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_inv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mls_bh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mls_ep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mls_bh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mbpo/lib/python3.6/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36mset_postfix\u001b[0;34m(self, ordered_dict, refresh, **kwargs)\u001b[0m\n\u001b[1;32m   1273\u001b[0m                                  for key in postfix.keys())\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_postfix_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mbpo/lib/python3.6/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36mrefresh\u001b[0;34m(self, nolock)\u001b[0m\n\u001b[1;32m   1198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnolock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnolock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mbpo/lib/python3.6/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(self, msg, pos)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mbpo/lib/python3.6/site-packages/tqdm/_tqdm_notebook.py\u001b[0m in \u001b[0;36mprint_status\u001b[0;34m(s, close, bar_style, desc)\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'||'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# remove inesthetical pipes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mescape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# html escape special characters (like '?')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                 \u001b[0mptext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;31m# Change bar style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/traitlets/traitlets.py\u001b[0m in \u001b[0;36m__set__\u001b[0;34m(self, obj, value)\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTraitError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The \"%s\" trait is read-only.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/traitlets/traitlets.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, obj, value)\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0;31m# we explicitly compare silent to True just in case the equality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# comparison above returns something other than True/False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_notify_trait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__set__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/traitlets/traitlets.py\u001b[0m in \u001b[0;36m_notify_trait\u001b[0;34m(self, name, old_value, new_value)\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0mnew\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m             \u001b[0mowner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m             \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'change'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m         ))\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mbpo/lib/python3.6/site-packages/ipywidgets/widgets/widget.py\u001b[0m in \u001b[0;36mnotify_change\u001b[0;34m(self, change)\u001b[0m\n\u001b[1;32m    604\u001b[0m                 \u001b[0;31m# Send new state to front-end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWidget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify_change\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/traitlets/traitlets.py\u001b[0m in \u001b[0;36mnotify_change\u001b[0;34m(self, change)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0mcallables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mcallables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trait_notifiers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0mcallables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trait_notifiers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0mcallables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trait_notifiers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trajs_inv = []\n",
    "\n",
    "for e in tqdm(range(EPOCHS)):\n",
    "    \n",
    "    # step1, generate inverse samples\n",
    "    cnt = 0\n",
    "    epn = 0\n",
    "    \n",
    "    rews = 0\n",
    "        \n",
    "    while True:\n",
    "        traj = []\n",
    "        rew = 0\n",
    "            \n",
    "        obs = env.reset()\n",
    "        while True:\n",
    "            inp = T.from_numpy(obs).view(((1, )+obs.shape)).float().cuda()\n",
    "            out = model.pred_act(inp).cpu().detach().numpy()\n",
    "                \n",
    "            if np.random.rand()>=EPS:\n",
    "                act = out[0]\n",
    "            else:\n",
    "                act = env.action_space.sample()\n",
    "                \n",
    "            new_obs, r, done, _ = env.step(act)\n",
    "                \n",
    "            traj.append([obs, act, new_obs])\n",
    "            obs = new_obs\n",
    "            rew += r\n",
    "            \n",
    "            cnt += 1\n",
    "                \n",
    "            if done==True:\n",
    "                rews += rew\n",
    "                trajs_inv.append(traj)\n",
    "                \n",
    "                epn += 1\n",
    "                \n",
    "                break\n",
    "        \n",
    "        if cnt >= M:\n",
    "            break\n",
    "        \n",
    "    rews /= epn\n",
    "    print('Ep %d: reward=%.2f' % (e+1, rews))\n",
    "        \n",
    "    # step2, update inverse model\n",
    "    ld_inv = DataLoader(DS_Inv(trajs_inv), batch_size=32, shuffle=True)\n",
    "    \n",
    "    with tqdm(ld_inv) as TQ:\n",
    "        ls_ep = 0\n",
    "        \n",
    "        for obs1, obs2, act in TQ:\n",
    "            out = model.pred_inv(obs1.float().cuda(), obs2.float().cuda())\n",
    "            ls_bh = loss_func(out, act.cuda())\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            ls_bh.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            ls_bh = ls_bh.cpu().detach().numpy()\n",
    "            TQ.set_postfix(loss_inv='%.3f' % (ls_bh))\n",
    "            ls_ep += ls_bh\n",
    "        \n",
    "        ls_ep /= len(TQ)\n",
    "        print('Ep %d: loss_inv=%.3f' % (e+1, ls_ep))\n",
    "    \n",
    "    # step3, predict inverse action for demo samples\n",
    "    traj_policy = []\n",
    "    \n",
    "    for obs1, obs2, _ in ld_demo:\n",
    "        out = model.pred_inv(obs1.float().cuda(), obs2.float().cuda())\n",
    "        \n",
    "        obs = obs1.cpu().detach().numpy()\n",
    "        out = out.cpu().detach().numpy()\n",
    "        \n",
    "        for i in range(100):\n",
    "            traj_policy.append([obs[i], out[i]])\n",
    "    \n",
    "    # step4, update policy via demo samples\n",
    "    ld_policy = DataLoader(DS_Policy(traj_policy), batch_size=32, shuffle=True)\n",
    "    \n",
    "    with tqdm(ld_policy) as TQ:\n",
    "        ls_ep = 0\n",
    "        \n",
    "        for obs, act in TQ:\n",
    "            out = model.pred_act(obs.float().cuda())\n",
    "            ls_bh = loss_func(out, act.cuda())\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            ls_bh.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            ls_bh = ls_bh.cpu().detach().numpy()\n",
    "            TQ.set_postfix(loss_policy='%.3f' % (ls_bh))\n",
    "            ls_ep += ls_bh\n",
    "        \n",
    "        ls_ep /= len(TQ)\n",
    "        print('Ep %d: loss_policy=%.3f' % (e+1, ls_ep))\n",
    "    \n",
    "    # step5, save model\n",
    "    T.save(model.state_dict(), 'Model/model_reacher_%d.pt' % (e+1))\n",
    "    \n",
    "    EPS *= DECAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
